{"0": {
    "doc": "Designing Your Schema",
    "title": "Designing Your Schema",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/",
    "relUrl": "/docs/part-2/designing-your-schema/"
  },"1": {
    "doc": "Designing Your Schema",
    "title": "Table of contents",
    "content": ". | General Recommendations for Schema Design and Data Layout | Series Cardinality . | How to estimate Cardinality | Dependent Tags | . | Runaway Cardinality . | Causes | . | Using Tags Intelligently . | Just Enough Flux | from() |&gt; range() |&gt; filter() | The Purpose of Tags . | Incorrectly casting tags as fields for the Air Quality dataset | Correctly using tags for the Air Quality Dataset | . | The Purpose of Measurements | . | Data Partitioning . | Single Bucket | Bucket per User | Org per Customer | . | Metrics vs. Events | Enforcing a Schema | . Figuring out the best data layout or schema for InfluxDB is important in optimizing the resources used by InfluxDB, as well as improving ingestion rates and the performance of Flux queries and tasks (scheduled Flux scripts). You also want to consider developer experience when designing your schema. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/#table-of-contents",
    "relUrl": "/docs/part-2/designing-your-schema/#table-of-contents"
  },"2": {
    "doc": "Designing Your Schema",
    "title": "General Recommendations for Schema Design and Data Layout",
    "content": "Generally, you should abide by the following recommendations when designing your schema: . | Keep bucket and measurement names short and simple. | Avoid encoding data in measurement names. | Encode meta data in tags. | Commonly queried metadata should be stored in tags for better query performance because tags are indexed while field values are not indexed. | Limit the number of series or try to reduce series cardinality and avoid runaway series cardinality. | Separate data into different buckets when you need to either: . | assign different retention policies to that data. | or need to scope authentication token to that bucket. | . | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/#general-recommendations-for-schema-design-and-data-layout",
    "relUrl": "/docs/part-2/designing-your-schema/#general-recommendations-for-schema-design-and-data-layout"
  },"3": {
    "doc": "Designing Your Schema",
    "title": "Series Cardinality",
    "content": "Series cardinality is the number series in your InfluxDB instance. Remember, a series is defined by the unique combinations of measurements, tag sets, and fields. Part of the reason why InfluxData adopted the “schema on write” approach for InfluxDB is because series creation and the resulting indexing generally offers efficient queries. However, this is only true up to a point. Similar to an over-indexed relational database, it can be possible for too much cardinality to slow down writes and queries because the indexes tracking the groups get too large. How much cardinality is too much? There is no single answer for that. However, each free tier user is currently limited to 10,000 total cardinality in their whole InfluxDB Cloud account. Pay as You Go (PaYG) customers are limited to 1,000,000 cardinality total. How to estimate Cardinality . To estimating your cardinality: . | Multiply the possible tag values for each possible tag for a worst case estimate. | Multiply the value in step 1 by the number of fields for a worse case estimate. | Repeat steps 1 and 2 for each measurement and each bucket, and then sum all of the values. | . In other words, the worst-case cardinality for a measurement = number of tag keys * number of tag values * number of field keys. The reason that this is “worst-case” is because it overestimates the total series in the presence of dependent tags. The worst-case cardinality equation assumes that every tag set is unique. In reality sometimes tags are redundant or dependent. Dependent Tags . A dependent tag is scoped by another tag. Dependent tags don’t influence the series cardinality, and should be removed from the cardinality calculation. Let’s take a look at the following line protocol to demonstrate how a dependent tag doesn’t influence the series cardinality. measurement1,tag1=\"tagvalue1\",tag2=\"tagvalue3\" field1=1i,field2=1,field3=\"a\" unixtime1 measurement1,tag1=\"tagvalue2\",tag2=\"tagvalue4\" field1=2i,field2=2,field3=\"b\" unixtime2 measurement1,tag1=\"tagvalue1\",tag2=\"tagvalue2\" field1=1i,field2=1,field3=\"a\" unixtime3 measurement1,tag1=\"tagvalue2\",tag2=\"tagvalue4\" field1=2i,field2=2,field3=\"b\" unixtime4 . In this instance the “tag2” tag key is a dependent tag because its tag value is dependent on the value of the “tag1” tag key. In other words, every time the value of the “tag1” tag key is “tagvalue1” the value of the “tag2” tag key is “tagvalue3”. Similarly, every time the “tag1” tag key is “tagvalue2” the value of the “tag2” tag key is “tagvalue4”. In this way “tag2” is scoped by “tag1”. If we were to calculate the series cardinality without removing the dependent tag, “tag2” we would get: . 1 measurement x ( 2 tag values for “tag1” x 2 tag values for “tag2”) x** 3** fields = 12 overestimated series cardinality . Again, each series is identified by their unique tag keys, tag values, and field key combinations. Because a series is defined in part by a unique set of tag values, in this case, the introduction of tag2 does not change the table or series count in the underlying data model. Therefore to correctly calculate the series cardinality, **remove **the dependent tag, “tag2” from the equation: . 1 measurement x 2 tag values for “tag1” x** 3** fields = 6 **actual **series cardinality . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/#series-cardinality",
    "relUrl": "/docs/part-2/designing-your-schema/#series-cardinality"
  },"4": {
    "doc": "Designing Your Schema",
    "title": "Runaway Cardinality",
    "content": "While calculating series cardinality can be challenging. Accidentally, overestimating your series cardinality shouldn’t be a real concern of yours–afterall InfluxDB has tools which calculate your cardinality for you (more on that later). However, you do need to make sure to avoid runaway cardinality. Runaway series cardinality occurs when you load your tags or measurements with data that is potentially unbounded. To illustrate a runaway series cardinality scenario, consider the following hypothetical situation in which we’re building an IoT application on top of InfluxDB. For this hypothetical IoT application example we’re gathering the following data: . | We have a set of customers. Each customer is identified with a single “customer_id” tag value under the “customer_id” tag key. | Each customer has a number of devices. Each one of the customer’s devices is identified with device id tag values under the “device_id” tag key. | Each device reports fields. | . For this example, cardinality = Cn _ * _Dn * F . Where… . _Cn = the number of “customer_id” tag values _ . Dn = the number of “device_id” tag values . _F = the number of field keys _ . Assuming your customers continue to grow, you can see how cardinality may “explode”. For example, if you start with 5 customers each with 5 devices, each with 1 field, total cardinality is 5 * 5 * 1, or a mere 25. However, if you end up with 100,000 customer, with 10 devices each, each with 5 fields, you end up with 100,000 * 10 * 5, or 5,000,000. Whoops, you blew your cardinality limit! . Causes . The most common schema design mistakes that can lead to runaway cardinality are: \\ . Mistake 1: Log messages as tags. Solution 1: We don’t advise that anyone store logs as tags due to the potential for unbounded cardinality (e.g. logs likely contain unique timestamps, UUIDs, etc). You can store attributes of a log as a tag, as long as the cardinality isn’t unbounded. For example, you could extract the log level (error, info, debug) or some key fields from the log message. Storing logs as a field is ok, but it is less efficient to search (essentially table scans), compared to other solutions. Mistake 2: Too many measurements. This typically happens when people are moving from —or think of InfluxDB as— a key-value store. So for example, if you’re writing system stats to an InfluxDB instance you might be inclined to write data like so: Cpu.server-5.us-west.usage_user value=20.0 . **Solution 2: **Instead encode that information as tags like so: cpu, host=server-5, region = us-west, usage_user=20.0 . Mistake 3: Making ids (such as eventid, orderid, or userid) a tag. This is another example that can cause unbounded cardinality if the tag values aren’t scoped. Solution 3: Instead, make these metrics a field. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/#runaway-cardinality",
    "relUrl": "/docs/part-2/designing-your-schema/#runaway-cardinality"
  },"5": {
    "doc": "Designing Your Schema",
    "title": "Using Tags Intelligently",
    "content": "Understanding how to properly use tags can not only help prevent runaway series cardinality but also increase your Flux query performance. In this section, we’ll refer to the Air sensor sample dataset to illustrate how to use tags intelligently. The Air sensor sample dataset represents an IoT use case. It contains the following schema: . | 1 measurement: airSensors | 3 fields: co, humidity, temperature, | 1 tag key: sensor_id | 8 tag values: TLM0100, TLM0101, TLM0102, TLM0103, TLM0200, TLM0101, TLM0202, TLM0203 | . Visualizing co, humidity, and temperature for th TLM0100 sensor from the Air sensor sample dataset after writing it to InfluxDB with the to() function as described in Write and Query Sample Data. Just Enough Flux . So far we have only discussed querying InfluxDB in an abstract manner. In order to understand the impact that tags have on Flux query performance, we need to take a moment to learn some Flux basics. This section is a crash course on Flux, the query, data transformation, and scripting language for InfluxDB 2.0. The aim of this section is to provide you with just enough basic understanding of Flux to be able to interpret the examples. We’ll deep dive into the Flux language in the following chapter. from() |&gt; range() |&gt; filter() . The following Flux query queries for the co, humidity, and temperature fields from the TLM0100 sensor from the Air sensor sample dataset bucket. from(bucket: \"Air sensor sample dataset\") |&gt; range(start: -1h, stop: now()) |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"airSensors\") |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"co\" or r[\"_field\"] == \"humidity\" or r[\"_field\"] == \"temperature\") |&gt; filter(fn: (r) =&gt; r[\"sensor_id\"] == \"TLM0100\") . The most basic Flux queries contain three functions: . | The from() function. A Flux query typically starts with a from() function. This function retrieves data from a specified bucket. | The range() function. The range() function must follow the use of a from() function. The range() function filters records based on time bounds provided to the start and stop parameters. | You can pass relative durations (see the example above), absolute time (2019-08-28T22:00:00Z) , or integers (1567029600) into the start and stop parameters. | . | The filter() function. The filter() function filters records based on conditions specified in the predicate function, fn. You can use the filter function() to filter for specific measurements, tags, and fields. You can also use the filter function to filter values based on thresholds and apply conditional query logic. The order of subsequent filter() functions doesn’t have an impact on performance. | . In Flux, functions are connected together through the |&gt; pipe-forward operator. The Purpose of Tags . As covered in the previous section, tag keys are grouped by the storage engine based on series. This allows the storage engine to quickly find points based on the tag keys and fields rather than scanning through all of your data to find the results of a query. To illustrate how indexing improves query performance, let’s consider the Air sensor sample dataset. Remember, the specific sensors that gather air quality data are tagged with a “sensor_id”. Now imagine that you want to query the dataset for all of the data from a specific sensor with a unique “sensor_id” value. When you apply a filter to query for a single tag value, the storage engine can use the indexed tag to quickly find the relevant table in the storage engine and return all the data associated with that tag value quickly. Filtering for a TLM0100 tag value from the “sensor_id” tag. Incorrectly casting tags as fields for the Air Quality dataset . However, if “sensor_id” was a field instead of a tag, then that sensor id isn’t indexed. Therefore, the storage engine would pull from one massive table for all of the sensor ids, using only the timestamps in the range() function as direction for which subset of data to pull from. In that instance, the storage engine would have to scan through all of the rows of that one massive table in order to pull out the rows that match the specific sensor id field value we’re filtering for. This scanning and row selection process is far less efficient than simply streaming out the relevant tables in their entirety. Casting the sensor id as a field presents additional query challenges and performance problems. Most likely you obtain senor id data because you want to know what the humidity or temperature is for a specific sensor. However, querying for the temperature from a specific sensor becomes extra challenging if your sensor id is a field. You would have to: . | Query for the sensor id field values. | Query for the temperature field values. | Join those two query results on timestamp–**assuming **that you only have one sensor reporting temperature values at each timestamp. | . In other words if the “sensor_id” was a field instead of a tag and you wanted to query for the temperature from that sensor you would have to query for the following 2 tables first: . | _measurement | _field | _value | _time | . | airSensors | sensor_id | TLM0100 | rfc3339time1 | . | airSensors | sensor_id | TLM0100 | rfc3339time2 | . | airSensors | sensor_id | TLM0101 | rfc3339time3 | . | airSensors | sensor_id | TLM0101 | rfc3339time4 | . | _measurement | _field | _value | _time | . | airSensors | temperature | 73.57230556533555 | rfc3339time1 | . | airSensors | temperature | 71.2194835668512 | rfc3339time2 | . | airSensors | temperature | 71.80350992863588 | rfc3339time3 | . | airSensors | temperature | 71.78232293801005 | rfc3339time4 | . To find the temperatures for the sensor TLM0100 the storage engine would need to read through the first table and pull out rows that have a _value of “TLM0100” then find matching timestamps in the second table and then merge those two tables and return them. When those tables become millions of rows long, you can see how this can slow down response time. Correctly using tags for the Air Quality Dataset . All of the problems in the section above are avoided when the sensor id is a tag. When you filter for the temperature for a specific “sensor_id” tag value, the storage engine simply reads out the first table as listed below because that series is indexed. The storage engine is able to return all the temperature results for a specific “sensor_id” even if the table has millions of rows (or the series has millions of points). | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0100 | temperature | 73.57230556533555 | rfc3339time1 | . | airSensors | TLM0100 | temperature | 71.2194835668512 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0101 | temperature | 71.80350992863588 | rfc3339time1 | . | airSensors | TLM0101 | temperature | 71.78232293801005 | rfc3339time2 | . The Purpose of Measurements . As covered in the previous section, measurements are grouped by the storage engine based on series. This allows the storage engine to quickly find points based on the measurements rather than scanning through all of your data to find the results of a query. The purpose of measurements is similar to the purpose of tags, but measurements offer a higher level of organization to your data. If your fields or tags are related, they should go into one measurement. Writing related data to the same measurement helps you avoid having to query for multiple measurements and perform joins. For example, imagine we’re developing a human health app and gathering blood oxygen levels, heart rate, and body temperature data. I would write the blood oxygen level, heart rate, and body temperature fields to one measurement because I foresee needing to simultaneously visualize and compare a combination of the metrics to assess human health. While you could separate out the fields into different measurements for a human health app, you’d most likely have to use joins() to perform math across measurements which are more computationally expensive. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/#using-tags-intelligently",
    "relUrl": "/docs/part-2/designing-your-schema/#using-tags-intelligently"
  },"6": {
    "doc": "Designing Your Schema",
    "title": "Data Partitioning",
    "content": "The final consideration for schema design in InfluxDB is finding the best approach to bucket and org data partitioning, especially when developing a customer facing IoT application on top of InfluxDB. There are 3 options: . | Single Bucket | Bucket per User | Org per Customer | . Remember authentication tokens can be restricted to read or write from individual buckets. Additionally each bucket is assigned one retention policy. Like tags and measurements, buckets are also indexed. Single Bucket . The single bucket design has the following advantages: . | Writing all of your data to a single bucket makes querying for data easier. You don’t have to keep track of which data exists within multiple buckets. You’ll likely need to use multiple measurements to organize your data effectively. However, you can still easily perform aggregations across multiple measurements by grouping the measurements together. | You can perform one downsampling task to downsample all of your data. Downsampling tasks are used to reduce the overall disk usage of your data by transforming high resolution data into lower resolution aggregates. | . The single bucket design has the following disadvantages: . | Each bucket is assigned one retention policy. With this design you can’t expire data subsets at different retention intervals. | If you need to query for a invicidual user’s data within a single bucket for your application, you should generate new read tokens for each individual customer. However, this design is less secure against malicious attacks than isolating different users’ data in separate buckets. | . The single bucket approach is good for use cases where: . | You’re developing a simple IoT application, like the Air sensor sample dataset. | You intend on treating all of your user’s data in the same way. Your data is being collected at similar intervals and a single retention policy is an effective solution for proper time series database management. | Your data is relatively non-sensitive and preventing a data breach is of little or no concern. | . Bucket per User . The bucket per user design has the following advantages: . | Your customer’s data is isolated and secure. | You can assign different retention policies to your different buckets depending on your customer’s needs. | You ca | . The bucket per user design has the following disadvantages: . | Performing math across your users’ data is hard. For example if you want to know which user is reporting a max value for their field, you must first join all of your data together across the different user buckets first. | You’ll likely need to write multiple downsampling tasks to downsample all of your user’s data. You can automate some of this downsampling task creation with the use of parameterized queries, but it’ll require a little more work. | . The bucket per user design is good for use cases where: . | You’re developing a more sophisticated IoT application and the data is sensitive, like a human health application. | You’re writing sensitive data to InfluxDB and isolating your users’ data is a priority. | Your customers have different data requirements and you need to provide them with different time series data management solutions (including different retention policies and downsampling tasks). | . Org per Customer . The org per customer design has the following advantages: . | You can meet the data requirements of users with advanced high throughput and cardinality use cases. | Your users’ data is isolated and secure. | . The org per customer design has the following disadvantages: . | You can’t easily query for data across multiple organizations. | . The org per customer design is good for use cases where: . | You’re developing an industrial IoT solution that can be used for multiple companies. | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/#data-partitioning",
    "relUrl": "/docs/part-2/designing-your-schema/#data-partitioning"
  },"7": {
    "doc": "Designing Your Schema",
    "title": "Metrics vs. Events",
    "content": "Understanding the difference between metrics and events is helpful for designing your schema. Metrics are time series data collected at regular intervals. Metrics are typically pulled from the data source. Metrics usually require regular downsampling tasks to convert the high resolution data to lower resolution aggregates for efficient disk usage management. Events are metrics collected at irregular intervals. They are typically pushed from the data source. A common design for industrial IoT application development is to write metrics from an IoT device to an OSS InfluxDB instance at the edge or in the fog. Then when those metrics meet certain conditions those events get pushed to the cloud. Metrics have an advantage over events, in that if metrics are written at precisely the same time intervals, the InfluxDB storage engine can apply aggressive compression when writing those metrics to disk. For Cloud, this will result in a small advantage in terms of read performance. For OSS users, this will also mean that you will be using less disk for the same metrics. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/#metrics-vs-events",
    "relUrl": "/docs/part-2/designing-your-schema/#metrics-vs-events"
  },"8": {
    "doc": "Designing Your Schema",
    "title": "Enforcing a Schema",
    "content": "Enforcing a schema with InfluxDB is important for maintaining consistent writes and for preventing data injections or bad writes. You can enforce a schema at the bucket level with explicit bucket schemas. You have to use the CLI to create a bucket with an explicit schema. First create a bucket with the influxd bucket create command and specify an explicit schema. influx bucket create \\ --name \"Air sensor sample dataset\"\\ --schema-type explicit . Next create a schema columns file for each measurement you want to add. We’ll enforce a schema for our airSensors measurement for our Air sensor sample dataset. Our schema columns file will be a CSV, but you can use JSON or NDJSON. Name the schema columns file (“airSensors_schema.csv”) and save it to your desired path. name,type,data_type time,timestamp, sensor_id,tag, co,field,float humidity,field,float temperature,field,float . Now we can add the schema to our airSensor measurement with the influx bucket-schema create command: . influx bucket-schema create \\ --bucket \"Air sensor sample dataset\" \\ --name airSensors \\ --columns-file airSensors_schema.csv . Specify the bucket that you want to create the schema for, the measurement to which your schema columns file should be applied to, and the path to your schema columns file. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/#enforcing-a-schema",
    "relUrl": "/docs/part-2/designing-your-schema/#enforcing-a-schema"
  },"9": {
    "doc": "Home",
    "title": "Time to Awesome",
    "content": "A book on InfluxDB . ",
    "url": "http://localhost:4000/time-to-awesome/#time-to-awesome",
    "relUrl": "/#time-to-awesome"
  },"10": {
    "doc": "Home",
    "title": "Table of Contents",
    "content": "Contributing . ",
    "url": "http://localhost:4000/time-to-awesome/#table-of-contents",
    "relUrl": "/#table-of-contents"
  },"11": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/",
    "relUrl": "/"
  },"12": {
    "doc": "InfluxDB Data Model",
    "title": "InfluxDB Data Model",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/influxdb-data-model/",
    "relUrl": "/docs/part-2/influxdb-data-model/"
  },"13": {
    "doc": "InfluxDB Data Model",
    "title": "Table of contents",
    "content": ". | InfluxDB Data Elements . | Buckets | Measurements | Tag Sets | Field Sets | . | Series and Points | Assumptions and Conventions . | Conventions Used In This Book | Meta Syntax for Examples | . | . The goal of this section is to provide the reader with a firm foundation in the InfluxDB data model, specifically: . | Understanding the InfluxDB input format (line protocol). | Understanding the InfluxDB output format (annotated CSV). | The relationships between these two formats. | Understanding how the Influxdb storage engine persists data in a table on disk | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/influxdb-data-model/#table-of-contents",
    "relUrl": "/docs/part-2/influxdb-data-model/#table-of-contents"
  },"14": {
    "doc": "InfluxDB Data Model",
    "title": "InfluxDB Data Elements",
    "content": "Buckets . All data in InfluxDB gets written to a bucket. A bucket is a container that can hold points for as many measurements as you like. Buckets have some important properties: . | They can be named whatever you want (within reason). | You can create tokens that can control read and write permissions for a bucket, scoped only to a specific bucket. | You must set a retention period on a bucket, upon creation. A retention period determines how long InfluxDB will store your time series data within that bucket. Retention periods are critical for time series database management. They provide users with a convenient solution for automatic expiration of old, useless data which enables them to focus on the recent, valuable data instead while reducing their storage bills. | . These topics will be covered in detail in a later section, for now, it is enough to know that measurements are stored in and read from a bucket. Measurements . A measurement is the highest level of data structure within a bucket. InfluxDB accepts one measurement per point. Use a measurement to organize similar data. In some ways, you can think of it as analogous to a table in a traditional database. Measurements are also indexed, which enables you to query data within a measurement more quickly when you filter for a specific measurement. Measurements must be a string type. Measurement names cannot begin with an underscore. To further understand measurements, let’s imagine you’re building a weather app and writing temperature data across multiple cities to InfluxDB. For this time series use case, you might create a measurement named “air_temperature”. Tag Sets . A tag set consists of key-value pairs, where the values are always strings. Tags are essentially metadata, typically encoding information about the source of the data. Imagine you’re building a weather app and writing temperature data across multiple cities to InfluxDB. For this time series use case, you might add a tag key to your data called “location” where the “location” tag key contains tag values for the cities weather you’re monitoring. This way you can easily query for all of the temperature data for “Los Angeles”, for example, by filtering for the “location” tag key with the “Los Angeles” tag value. Critically, tags are indexed, so they are an important element of designing for query performance. However, tags are also optional. Tag keys cannot begin with an underscore, because InfluxDB reserves leading underscores for its own purposes. Field Sets . A field set consists of key-value pairs, where the values can be strings, integers, or floats. Fields are the actual data to store, visualize, use for computations, etc… . Building on the weather app example, the temperature readings would be an example of a field. Field sets are required for InfluxDB. This is where you store the value of your time series data. Field values can be of either an integer, float, or string type. Since field values are almost always different, field keys are frequently referred to as just fields. Fields are not indexed. Field also cannot begin with an underscore. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/influxdb-data-model/#influxdb-data-elements",
    "relUrl": "/docs/part-2/influxdb-data-model/#influxdb-data-elements"
  },"15": {
    "doc": "InfluxDB Data Model",
    "title": "Series and Points",
    "content": "A series is defined by the unique combination of measurement, tag set(s), and fields. A point is a datapoint from a series at a specific timestamp. If you rewrite a duplicate point with identical measurement, tag set, field, and timestamp values, this write will overwrite your previous point. If you try to write a new point to the same series but you change the field type, this write will fail. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/influxdb-data-model/#series-and-points",
    "relUrl": "/docs/part-2/influxdb-data-model/#series-and-points"
  },"16": {
    "doc": "InfluxDB Data Model",
    "title": "Assumptions and Conventions",
    "content": "Before diving into more nuanced and technical topics around the InfluxDB Data Model, let’s take a moment to establish some baseline assumptions and conventions. Conventions Used In This Book . Chapters in this book will generally introduce concepts using abstract and simplified examples, and then follow with detailed examples using real world data. Meta Syntax for Examples . For the abstract and simplified examples, we will use names in the form of: . attributeorvaluen . Where “attributeorvalue” refers a column name of a table or a value in a point, and “n” is a number simply differentiating multiple identifiers of the same role. Roles can comprise any of the following: . | Measurement | Tag | Tag Value | Field | . Samples will also generally include: . | Field Values | Timestamps | . Field values will be represented by actual values. Timestamps are in the following timestamp formats: . | Unix: The unix timestamp is a way to track time as a running total of seconds–i.e. 1465839830100400200 | RFC3339: A standard for date and time representation from a Request For Comment document by the Internet Engineering Task Force (IETF)–​​i.e. 2019-08-28T22:00:000000000Z | Relative Duration: -1h | Duration: 1h | . Timestamps will be represented by actual values or by “unixtime1” or “rfc3339time1” . In case we want to refer to another timestamp in the same example, we will use “unixtime2” or “rfc3339time2” . To refer to a measurement in an example, we will use: “measurement1”. In case we want to refer to another measurement in the same example, we will use “measurement2”, and so forth. An example of line protocol (explained in depth later) then, may look like: . measurement1,tag1=tagvalule1,tag2=tagvalue2 field1=1i,field2=2 1628858104 . From time to time an example may be focused on understanding a type. In such cases, we will use the form “atype” where “type” is the data type under focus. For example, if we are discussing that field names are always strings. we may say: . r._field == \"astring\" . Or if we are discussing type conflicts, we may say: . aint == afloat . Instead of an example with specific values such as: . ``` 1i == 1.0 . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/influxdb-data-model/#assumptions-and-conventions",
    "relUrl": "/docs/part-2/influxdb-data-model/#assumptions-and-conventions"
  },"17": {
    "doc": "Input Format vs Output Format",
    "title": "Input Format vs Output Format",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/input-format-vs-output-format/",
    "relUrl": "/docs/part-2/input-format-vs-output-format/"
  },"18": {
    "doc": "Input Format vs Output Format",
    "title": "Table of contents",
    "content": ". | Line Protocol . | Adding multiple fields | Types explained | Type conflicts | Adding tags | Adding timestamps | Note on Timestamp Precision | Overwriting points | . | Annotated CSV . | Raw CSV vs Annotated CSV | Header Row | Record Rows | Records vs Points | Annotation Rows | . | From Series to Tables on Disk . | Adding Fields | Adding Tags | . | Real World Data . | Exploring the Real Word Data Schema with Flux | Exercises with Real World Data | . | . The InfluxDB input format is line protocol. The InfluxDB output format is Annotated CSV. The input format is different from the InfluxDB persistence. The Annotated CSV output can match the InfluxDB persistence format with simple Flux queries. However, can add Flux transformations to your query such that the Annotated CSV output doesn’t reflect the InfluxDB persistence format. Understanding these subtle differences is critical for good schema design and for using InfluxDB optimally. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/input-format-vs-output-format/#table-of-contents",
    "relUrl": "/docs/part-2/input-format-vs-output-format/#table-of-contents"
  },"19": {
    "doc": "Input Format vs Output Format",
    "title": "Line Protocol",
    "content": "The smallest allowed line of line protocol includes a measurement, a single field, and a value for that field. A measurement is the highest grouping of data inside a bucket. A field is a part of that measurement and defines a single point. Fields are separated by measurements by a space. So, the smallest readable line looks like: . measurement1 field1=1i . Adding multiple fields . You need to supply at least one field, but you can supply as many as you need in a single line, separated by a comma: . measurement1 field1=1i,field2=1,field3=\"a\" . Line protocol is very compact, but each of the fields will be in their own series when stored in the database. More on that in From Series to Tables on Disk. Types explained . The types of a field value can be an integer, a float, or a string. By default a number will be interpreted as a float. The addition of the “i” after each number for my_field tells InfluxDB that I wanted those to be integers. Using quotes ensures that InfluxDB knows I want a string type. Type conflicts . Once you have created a series, you cannot change the type field type of the series. InfluxDB will reject the the following write: . measurement1 field1=1,field2=\"1\",field3=\"a\" . After having written this line: measurement1 field1=1i,field2=1,field3=\"a\" . Notice how the field2 field value has been changed from a string to a float. Adding tags . Tags are another kind of data that you can add to a line in line protocol. Tags are useful because they are automatically indexed by InfluxDB. Using and querying for tags allows you to significantly improve your query performance. Additionally, tags are useful for categorizing your queries. Remember, a series is defined by a measurement(s), tag sets(s), and field key(s). Tags are defined after the measurement name, and separated from it by a comma. I can add a tag to the previous line protocol as such: . measurement1,tag1=tagvalue1 field1=1i,field2=1,field3=\"a\" 1626118680000000000 measurement1,tag1=tagvalue2 field1=2i,field2=2,field3=\"b\" 1626118740000000000 . The introduction of the tag key, “tag1”, with 2 different tag values, “tagvalue1” and “tagvalue2” produces 6 series–3 series come from the different fields for each of the 2 tag values. Adding timestamps . In cases where a timestamp is omitted from line protocol, InfluxDB will add a timestamp based on the current server time at the time of the write. Letting InfluxDB automatically supply a timestamp is very convenient, but is likely too imprecise for your application, so you will typically supply a timestamp as well. InfluxDB expects timestamps to be unix timestamps. InfluxDB also expects the timestamps to be in nanosecond resolution, but the write API allows you to define lower resolution precision if needed. The timestamp comes at the end of a line of line protocol and is separated from the last field value by a space. Supplying a timestamp to the above line protocol would look like this: . my_measurement my_field=1i,my_other_field=1,yet_another_field=\"a\" 1626118680000000000 my_measurement my_field=2i,my_other_field=2,yet_another_field=\"b\" 1626118740000000000 . Note on Timestamp Precision . The native resolution of time stamps in InfluxDB is nanoseconds. In InfluxDB, the unit of resolution (for example nanosecond, microsecond, millisecond) is called the “precision” of the time stamp. For context: . | There are 1,000 nanoseconds in a microsecond | There are 1,000,000 nanoseconds in a millisecond | There are 1,000,000,000 nanoseconds in a second | . This is important to keep in mind while constructing your application, because many systems do not handle nanosecond resolution, so it is necessary to convert between them. Note that InfluxDB tools do allow you to define the precision of your timestamps, so when writing, you can allow InfluxDB to handle the conversion for you. This will be covered in Part 3. Overwriting points . You can overwrite points in InfluxDB when you write data with the same series and same timestamp. For example, if you wrote this line of line protocol to InfluxDB: . my_measurement my_field=2i,my_other_field=2,yet_another_field=\"b\" 1626118740000000000 . You would then overwrite the 3 points by writing this line next. my_measurement my_field=10i,my_other_field=10,yet_another_field=\"overwritten\" 1626118740000000000 &lt;more examples that include partial overwrites, subsets of tags&gt; . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/input-format-vs-output-format/#line-protocol",
    "relUrl": "/docs/part-2/input-format-vs-output-format/#line-protocol"
  },"20": {
    "doc": "Input Format vs Output Format",
    "title": "Annotated CSV",
    "content": "Annotated CSV is the output format for InfluxDB. The annotated CSV output matches the InfluxDB persistence format with simple queries or when you don’t apply additional transformations to your data. Annotated CSV result is a stream of tables returned by Flux where each table represents a series for simple queries only. For example if you wrote this line protocol line to InfluxDB: . Measurement1,tag1=tagvalue1 field1=1i . You would return the following full annotated CSV output when querying for all fields and tags (of which there are none) within the measurement: . #group,false,false,true,true,false,false,true,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,long,string,string,string #default,_result,,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 ,,0,rfc3339time1,rfc3339time2,2021-08-17T21:23:39.000000000Z,1,field1,Measurement1,tagvalue1 . Remember, that line of line protocol produces 1 series which is why one table is included in the annotated CSV output. Raw CSV vs Annotated CSV . The first thing to notice about the output format of annotated CSV is that it resembles the CSV format that you’re familiar with. To easily distinguish between the two we’ll refer to CSV as Raw CSV. Unlike raw CSV, annotated CSV contains the following rows: . | Header Row | Records Rows | Annotation Rows | . Header Row . The header row is similar to any header row in a CSV. It describes the column names for your time series data.The header row is found below the 3 Annotation rows. Our header row is: . ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 . Some of these headers are an intuitive translation from line protocol while others are not. The headers are: . | result. This column includes the name of the result as specified by the query. If no name is provided it will be empty. | table. This column contains a unique ID for each table in the annotated CSV output result. stream. In the example above we are only writing and querying for 1 series so the value of that column is set to 0 for that table. | _start.This column includes the start time for the time range over which you queried your data for. Specifying a range is necessary for any Flux query. | _stop.This column includes the stop time for the time range over which you queried your data for. Specifying a range is necessary for any Flux query. | _time.This column is the timestamp for your time series. The value of this column is either added upon write or included in the line protocol explicitly. | _value. This column contains the field values for the corresponding field keys in the same row under the _field column. | _field. This column contains the field keys. | _measurement. This column contains the name of the measurement. | tag1. This column contains the tag value for our tag1 tag key. | . For the rest of this section you can ignore the values for the _start, _stop, and result columns. Those values are assigned by the user during query execution. For now, just focus on understanding the similarities between your line protocol input format and the resulting annotated CSV. Record Rows . The records row(s) is directly below the header row. These rows contain our time series data. Each row contains one record. A record is a tuple of named values. Each table contains at least one record. When querying for your time series without adding any additional Flux transformations, a table . Records vs Points . Remember, a point is a datapoint from a series at a specific timestamp. A record can be a point but not always. For example, you could use Flux to drop all tag, field, and measurement columns. At this point the records in the annotated CSV wouldn’t reflect a point but rather time series data instead. Annotation Rows . The annotation rows are found in the first three lines of the full annotated CSV output. Annotations include metadata about your annotated CSV. When querying InfluxDB with the API, you can specify which headers you want to include in your annotated CSV output. All 3 headers are included by default when you export the results of your query through the InfluxDB UI. The 3 annotations are: . | ​​#group: A boolean that indicates the column is part of the group key. A group key is a list of columns for which every row in the table has the same value. A column is part of the group key if its ​​#group annotation is set to ​​true. Important Note: The exception is for the table column. The group key for the table is set to false because users can’t directly change the table number. The table record will always be the same across rows even though the group key is set to false. | #datatype: Describes the type of data or which line protocol element the column represents. | #default: The value to use for rows with an empty value. | . If annotations confuse you, don’t worry. The importance of annotations will become apparent in subsequent sections of this book, specifically around understanding Flux. Additionally it’s worth mentioning that the​​ #group annotation is the most important annotation for using Flux successfully. For now, be aware that InfluxDB applies a default group key to your data so that the tables in your annotated CSV output will each represent a single series for simple queries. This default application of group keys is the result of the way that series are stored on disk, described in the following section. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/input-format-vs-output-format/#annotated-csv",
    "relUrl": "/docs/part-2/input-format-vs-output-format/#annotated-csv"
  },"21": {
    "doc": "Input Format vs Output Format",
    "title": "From Series to Tables on Disk",
    "content": "Each series is stored as a table on disk. Data gets written to InfluxDB into buckets. When data is written to a bucket it is added to an appropriate table, or a new table is created if needed. Each table has exactly one measurement, one field, and a unique set of tag values. Indexes are created or amended on write to enable quickly finding these tables when queried. Additionally, all rows in all tables are indexed by time. Some people consider InfluxDB to be a “schemaless” database. However, that is not really accurate. More accurately, InfluxDB is a “schema on write.” That is to say, InfluxDB does not require a schema to be defined beforehand nor enforce a schema on writes. Instead, Influxdb builds a schema implicitly based on the writes you do make. While schema on write is a true convenience for developers, you should be cognizant of the schema that you are implicitly creating as you write data. A poorly designed schema can have a negative impact on things like ease of querying, performance, and cardinality. In this section we’ll learn about how line protocol produces series which get stored as tables on disk. Adding Fields . In this section we’ll break down how the line protocol is converted to series and written as tables in InfluxDB. Let’s review the line protocol examples above: . measurement1 field1=1i,field2=1,field3=\"a\" measurement1 field1=1i,field2=2,field3=\"b\" . When written, this will form 3 different time series with 2 points in each series, assuming each line was written one minute apart. The data will then be persisted in three separate tables: . | _measurement | _field | _value | _time | . | measurement1 | field1 | 1i | 2021-07-12T19:38:00.000000000Z | . | measurement1 | field1 | 1i | 2021-07-12T19:39:00.000000000Z | . | _measurement | _field | _value | _time | . | measurement1 | field2 | 1 | 2021-07-12T19:38:00.000000000Z | . | measurement1 | field2 | 2 | 2021-07-12T19:39:00.000000000Z | . | _measurement | _field | _value | _time | . | measurement1 | field3 | a | 2021-07-12T19:38:00.000000000Z | . | measurement1 | field3 | b | 2021-07-12T19:39:00.000000000Z | . This example provides the first insights into how the input format and the persistence/output format differ. Each series in InfluxDB is persisted with exactly one field, though line protocol allows writing with multiple fields. This is a critically important concept to understand, so it is worth repeating. The InfluxDB input format is different from the InfluxDB persistence and output format. These tables represent the minimum data roles that can exist in a series. A series must have at a minimum: . | A measurement name | A field key | A value for the field | A time | . These are represented in the database with a leading “_”. The leading underscore conveys that these are “slots” in the series that are enforced by the storage engine, as well as helps to avoid naming conflicts with potential tag names. When simply querying for this data, without adding any additional Flux transformations, the annotated CSV output looks like: . #group,false,false,true,true,false,false,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,long,string,string #default,_result,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement ,,0,rfc3339time1,rfc3339time2,2021-07-12T19:39:00.000000000Z,1,field1,measurement1 ,,0,rfc3339time1,rfc3339time2,2021-07-12T19:38:00.00000000Z,1,field1,measurement1 #group,false,false,true,true,false,false,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,string,string,string #default,_result,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement ,,1,rfc3339time1,rfc3339time2,2021-07-12T19:39:00.000000000Z,a,field3,measurement1 ,,1,rfc3339time1,rfc3339time2,2021-07-12T19:38:00.00000000Z,b,field3,measurement1 #group,false,false,true,true,false,false,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,double,string,string #default,_result,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement ,,2,rfc3339time1,rfc3339time2,2021-07-12T19:39:00.000000000Z,1,field2,measurement1 ,,2,rfc3339time1,rfc3339time2,2021-07-12T19:38:00.000000000Z,2,field2,measurement1 . Notice how the resulting annotated CSV contains 3 tables in the output. This is evident by the row separation and also by the value of the table column in the last stream of the table which is equal to 2 (remember annotated CSV counts the table results from 0). Group keys have been added to the data to produce these tables so that each table represents a series by default. Remember a column is part of a group key if all of the values in that column are identical within a single table. For example, the time and value columns are assigned a #group annotation of ​​false. Setting the #group annotation to ​​false allows the different timestamps and field values of points across a single series to be included in the same table. Adding Tags . Let’s review the line protocol example above with an added tag: . measurement1,tag1=\"tagvalue1\" field1=1i,field2=1,field3=\"a\" 1626118680000000000 measurement1,tag1=\"tagvalue2\" field1=2i,field2=2,field3=\"b\" 1626118740000000000 . The introduction of tag1 produces the following 6 series: . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue1 | field1 | 1i | 2021-07-12T19:38:00.000000000Z | . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue1 | field2 | 1 | 2021-07-12T19:38:00.000000000Z | . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue1 | field3 | a | 2021-07-12T19:38:00.000000000Z | . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue2 | field1 | 2i | 2021-07-12T19:39:00.000000000Z | . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue2 | field2 | 2 | 2021-07-12T19:39:00.000000000Z | . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue2 | field3 | b | 2021-07-12T19:39:00.000000000Z | . Ensure that you take the time to study and understand the relationship between the line protocol and the resulting tables as they are represented on disk by the storage engine. This relationship is critically important to achieving the most effective schema and querying for your application. When simply querying for this data, without adding any additional Flux transformations the annotated CSV output looks like: . #group,false,false,true,true,false,false,true,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,string,string,string,string #default,_result,,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 ,,0,rfc3339time1,rfc3339time2,2021-07-12T19:39:000000000Z,b,field3,measurement1,tagvalue2 #group,false,false,true,true,false,false,true,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,double,string,string,string #default,_result,,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 ,,1,rfc3339time1,rfc3339time2,2021-07-12T19:38:000000000Z,1,field2,measurement1,tagvalue1 #group,false,false,true,true,false,false,true,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,long,string,string,string #default,_result,,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 ,,2,2021-01-18T20:59:37Z,2021-08-17T19:59:37.097Z,2021-07-12T19:38:000000000Z,1,field1,measurement1,tagvalue1 #group,false,false,true,true,false,false,true,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,string,string,string,string #default,_result,,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 ,,3,rfc3339time1,rfc3339time2,2021-07-12T19:38:000000000Z,a,field3,measurement1,tagvalue1 #group,false,false,true,true,false,false,true,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,long,string,string,string #default,_result,,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 ,,4,rfc3339time1,rfc3339time2,2021-07-12T19:39:000000000Z,2,field1,measurement1,tagvalue2 #group,false,false,true,true,false,false,true,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,double,string,string,string #default,_result,,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 ,,5,rfc3339time1,rfc3339time2,2021-07-12T19:39:000000000Z,2,field2,measurement1,tagvalue2 . Notice how the resulting annotated CSV contains 6 tables in the output. This is evident by the row separation and also by the value of the table column in the last stream of the table which is equal to 5 (remember annotated CSV counts the table results from 0). Group keys have been added to the data to produce these tables so that each table represents a series by default. Remember a column is part of a group key if all of the values in that column are identical within a single table. For example, the time column is assigned a #group annotation of ​​false. Setting the #group annotation to ​​false allows the different timestamps of points across a single series to be included in the same table. Conversely, the _measurement column is assigned a #group annotation of ​​true. The _measurement column is assigned a #group annotation of ​​true. Setting the #group annotation to ​​true enforces that all of the records in that table have the same measurement value. Remember, a series is identified by a unique combination of measurements, tag sets, and fields. If a table is to represent a single series, the table must contain records with the same measurement, tag sets, and fields across all of the rows. **Important note: You can use Flux to manipulate the group keys and the resulting number of tables in the the output Annotated CSV table stream. We’ll learn about how to do this in later chapters. ** . For now, let’s focus on understanding how line protocol results in different series. We can extend the example by adding an additional tag, but in this case, note that there is only a single tag value for tag2: . measurement1,tag1=\"tagvalue1\",tag2=\"tagvalue3\" field1=1i,field2=1,field3=\"a\" 1626118680000000000 measurement1,tag1=\"tagvalue2\",tag2=\"tagvalue3\" field1=2i,field2=2,field3=\"b\" 1626118740000000000 . Again, each series is identified by their unique tag keys, tag values, and field key combinations. Because a series is defined in part by a unique set of tag values, in this case, the introduction of tag2 does not change the table count in the underlying data model. When the introduction of a tag does not change the table count in the underlying data model, the tag is referred to as a dependent tag: . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue1 | tagvalue2 | field1 | 1i | 2021-07-12T19:38:00.000Z | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue1 | tagvalue2 | field2 | 1 | 2021-07-12T19:38:00.000Z | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue1 | tagvalue2 | field3 | a | 2021-07-12T19:38:00.000Z | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue2 | tagvalue2 | field1 | 2i | 2021-07-12T19:39:00.000Z | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue2 | tagvalue2 | field2 | 2 | 2021-07-12T19:39:00.000Z | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue2 | tagvalue2 | field3 | b | 2021-07-12T19:39:00.000Z | . To demonstrate the impact of combinations of tag values on the creation of time series, here are three lines of line protocol, but with only one field: . measurement1,tag1=\"tagvalue1\",tag2=\"tagvalue4\" field1=1i 1626118620000000000 measurement1,tag1=\"tagvalue2\",tag2=\"tagvalue5\" field1=2i 1626118680000000000 measurement1,tag1=\"tagvalue3\",tag2=\"tagvalue6\" field1=3i 1626118740000000000 . In those 3 lines there are 3 unique combinations of tag values and the single field, so, despite the presence of six total tag values, there are only 3 series created: . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue1 | tagvalue4 | field1 | 1i | 2021-07-12T19:37:00.000Z | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue2 | tagvalue5 | field1 | 2i | 2021-07-12T19:38:00.000Z | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue3 | tagvalue6 | field1 | 3i | 2021-07-12T19:39:00.000Z | . In this example, there are only 2 unique combinations of tag values: . measurement1,tag1=\"tagvalue1\",tag2=\"tagvalue4\" field1=1i 1626118620000000000 measurement1,tag1=\"tagvalue1\",tag2=\"tagvalue4\" field1=2i 1626118680000000000 measurement1,tag1=\"tagvalue2\",tag2=\"tagvalue4\" field1=3i 1626118740000000000 . As a result, the first series contains two points because those two points have the same combination of field name and tag values, whereas the third point has a different set of tag values. | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue1 | tagvalue2 | field1 | 1i | 2021-07-12T19:37:00.000Z | . | measurement1 | tagvalue1 | tagvalue2 | field1 | 2i | 2021-07-12T19:38:00.000Z | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue3 | tagvalue4 | field1 | 3i | 2021-07-12T19:39:00.000Z | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/input-format-vs-output-format/#from-series-to-tables-on-disk",
    "relUrl": "/docs/part-2/input-format-vs-output-format/#from-series-to-tables-on-disk"
  },"22": {
    "doc": "Input Format vs Output Format",
    "title": "Real World Data",
    "content": "There is an excellent repository of semi-live line protocol data maintained by InfluxData. This is generally intended as sample data to help you get started exploring InfluxDB. Currently, 4 datasets are kept up to date there: . | Air Sensor Data: This dataset includes a single tag, which is an id for the specific air quality sensor that is reporting 3 fields, temperature, humidity, and carbon monoxide levels. | Bird Migration Data: This is geo-spatial dataset represents migratory movements of birds. It is tagged to aid geo-spacial querying. | NOAA National Buoy Center Data: This dataset provides the latest observations from the NOAA NDBC network of buoys. It contains a large number of tags and fields. | USGS Earthquake Data. The United States Geological Survey (USGS) earthquake dataset contains seismic activity data. This is a very large dataset, and contains even more tags and fields. | . While you can simply copy the Flux from any of the real world sample datasets into the Script Editor **in the **Data Explorer and visualize the data. I recommend creating a bucket and using the to() function to write the data to that bucket, as described in Write and Query Sample Data in Part 1. Writing the data to a bucket in InfluxDB allows you to use Flux to explore the schema of your dataset. **Important Note: **You might hit your series cardinality limit for Free Tier accounts if you write the larger datasets to InfluxDB. I recommend just writing the Air Sensor Data if you’re using the Free Tier account. **Important Note: **This section recommends that you use the InfluxDB UI to write data to InfluxDB only because writing data with other tools hasn’t been covered yet. We recommend writing data with the CLI or VS Code. If you prefer developing in with those tools look at the . Exploring the Real Word Data Schema with Flux . Let us turn our attention to 2 real world data sets: Air Sensor Data and the NOAA National Buoy Center Data. We’ll use Flux to get an understanding of our schema. Then we’ll run through some exercises to ensure our understanding of the relationship between the line protocol input format and the InfluxDB data model as persisted on disk by the storage engine. We’ll start by focusing on the Air sensor dataset, as it is the simplest dataset. The a Air sensor dataset contains: . | 1 measurement: airSensors | 3 fields: . | co | humidity | temperature | . | 1 tag: sensor_id . | 8 sensor_id tag values | . | . As you can see, the fields are the actual data, in this case all of type float, where the tag is metadata, defining which sensor produced the data. Explore your data schema with Flux to obtain the number of measurements, tag keys, tag values, and field keys in your data by using the schema package. To get the number of fields in the airSensors measurement from the Air sensor sample dataset, run the following Flux query in your preferred tool (CLI, VS Code, InfluxDB UI): . import \"influxdata/influxdb/schema\" schema.measurementFieldKeys( bucket: \"Air sensor sample dataset\", measurement: \"airSensors\" ) |&gt; count() . To get the number of tag keys in the airSensors measurement from the Air sensor sample dataset, run the following Flux query in your preferred tool (CLI, VS Code, InfluxDB UI): . import \"influxdata/influxdb/schema\" schema.measurementTagKeys( bucket: \"Air sensor sample dataset\", measurement: \"airSensors\" ) |&gt; count() . To get the number of tag values for the sensor_id tag key in the airSensors measurement from the Air sensor sample dataset, use the following Flux query (CLI, VS Code, InfluxDB UI): . import \"influxdata/influxdb/schema\" schema.measurementTagValues( bucket: \"Air sensor sample dataset\", tag: \"sensor_id\", measurement: \"example-measurement\" ) |&gt; count() . We can repeat the same approach for the NOAA National Buoy Center Data. We find that the NOAA National Buoy Center Data has the following schema: . | 1 measurement: ndbc | 21 fields: . | air_temp_degc | avg _wave_period_sec | dewpoint_temp_degc | dominate_wave _period_sec | gust_speed_mps | lat | lon | pressure_temdancy_hpa | sea_level_pressure_hpa | sea_surface_temp_degc | significant_Wave_height_m | station_currents | station_dart | station_elev | sation_met | station_visibility_mei | station_waterquality | water_level_ft | wave_dir_degt | wind_dir_degt | wind_spead_mps | . | 5 tag keys: . | station_id . | 113 station_id tag values | . | station_name . | 828 | . | station_owner . | 57 station_owner tag values | . | station_pgm . | 6 station_pgm tag values | . | station_type . | 4 station_type tag values | . | . | . Again, note that the 21 fields are all the different kinds of data that might be collected by a weather station, whereas the 5 tags contain metadata about which stations collected it. Exercises with Real World Data . Now that we understand the schema of the 2 real world data sets, Air Sensor Data and the NOAA National Buoy Center Data, try to answer the following questions to test your understanding of schema design, line protocol, and . Question 1: How many series will be created by the Air Sensor Data given the schema above? . Answer 1: (1 sensor_id tag x 8 unique tag values) x (3 fields) = 4 x 3 = 24 . Question 2: How many series will be created by the NOAA National Buoy Center Data given the schema above (assuming no tags are dependent tags)? . Answer 2: (1 station_id tag x 113 unique tag values) x (1 station_name tag x 828 unique tag values) x (1 station_name tag x 828 unique tag values) x (1 station_owner tag x 57 unique tag values) x (1 station_pgm tag x 6 unique tag values) x (1 station_type tag x 47 unique tag values) x (21 fields) = 113 x 828 x 56 x 6 x 47 x 21 = 31028816448 . Question 3: How would the following line protocol form the Air sensor sample dataset be organized into tables on disk? And how many points are in each series? . airSensors,sensor_id=TLM0100 temperature=71.17615703642676,humidity=35.12940716174776,co=0.5024058630839136 1626537623000000000 airSensors,sensor_id=TLM0101 temperature=71.80350992863588,humidity=34.864121891949736,co=0.4925449578765155 1626537623000000000 airSensors,sensor_id=TLM0102 temperature=72.02673296407973,humidity=34.91147650009415,co=0.4941631223400505 1626537623000000000 airSensors,sensor_id=TLM0103 temperature=71.34822444566278,humidity=35.19576623496297,co=0.4046734235304059 1626537623000000000 airSensors,sensor_id=TLM0200 temperature=73.57230556533555,humidity=35.77102288427073,co=0.5317633226995193 1626537623000000000 airSensors,sensor_id=TLM0201 temperature=c,humidity=35.17327249047271,co=0.5000439017037601 1626537623000000000 airSensors,sensor_id=TLM0202 temperature=75.28582430811852,humidity=35.668729783597556,co=0.48071553398947864 1626537623000000000 airSensors,sensor_id=TLM0203 temperature=74.75927935923579,humidity=35.89268792033798,co=0.4089308476612381 1626537623000000000 airSensors,sensor_id=TLM0100 temperature=71.2194835668512,humidity=35.12891266051405,co=0.4958773037139102 1626537633000000000 airSensors,sensor_id=TLM0101 temperature=71.78232293801005,humidity=34.88621453634278,co=0.5074032895942003 1626537633000000000 airSensors,sensor_id=TLM0102 temperature=72.07101160147653,humidity=34.938529830668536,co=0.5102716855442547 1626537633000000000 airSensors,sensor_id=TLM0103 temperature=71.32889101333731,humidity=35.21581883021604,co=0.4245915521103036 1626537633000000000 airSensors,sensor_id=TLM0200 temperature=73.55081075397399,humidity=35.74330537831752,co=0.5435288991742965 1626537633000000000 airSensors,sensor_id=TLM0201 temperature=74.06284877512215,humidity=35.17611147751894,co=0.4813785832360323 1626537633000000000 airSensors,sensor_id=TLM0202 temperature=75.29425020175684,humidity=35.64366062740866,co=0.4911462705616819 1626537633000000000 airSensors,sensor_id=TLM0203 temperature=74.77142594525142,humidity=35.941017361190255,co=0.42797647488504065 1626537633000000000 . Answer 3: . The line protocol would result in the following series and tables on disk. Each series contains two points from the line protocol above. | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0100 | temperature | 73.57230556533555 | rfc3339time1 | . | airSensors | TLM0100 | temperature | 71.2194835668512 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0101 | temperature | 72.02673296407973 | rfc3339time1 | . | airSensors | TLM0101 | temperature | 71.78232293801005 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0102 | temperature | 73.57230556533555 | rfc3339time1 | . | airSensors | TLM0102 | temperature | 72.07101160147653 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0103 | temperature | 71.34822444566278 | rfc3339time1 | . | airSensors | TLM0103 | temperature | 71.32889101333731 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0200 | temperature | 73.57230556533555 | rfc3339time1 | . | airSensors | TLM0200 | temperature | 73.55081075397399 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0201 | temperature | 73.57230556521233 | rfc3339time1 | . | airSensors | TLM0201 | temperature | 74.06284877512215 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0202 | temperature | 75.28582430811852 | rfc3339time1 | . | airSensors | TLM0202 | temperature | 75.29425020175684 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0203 | temperature | 74.75927935923579 | rfc3339time1 | . | airSensors | TLM0203 | temperature | 74.77142594525142 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0100 | humidity | 35.12940716174776 | rfc3339time1 | . | airSensors | TLM0100 | humidity | 35.12891266051405 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0101 | humidity | 34.864121891949736 | rfc3339time1 | . | airSensors | TLM0101 | humidity | 34.88621453634278 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0102 | humidity | 34.91147650009415 | rfc3339time1 | . | airSensors | TLM0102 | humidity | 34.938529830668536 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0103 | humidity | 35.19576623496297 | rfc3339time1 | . | airSensors | TLM0103 | humidity | 35.21581883021604 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0200 | humidity | 35.77102288427073 | rfc3339time1 | . | airSensors | TLM0200 | humidity | 35.74330537831752 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0201 | humidity | 35.17327249047271 | rfc3339time1 | . | airSensors | TLM0201 | humidity | 35.17611147751894 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0202 | humidity | 35.668729783597556 | rfc3339time1 | . | airSensors | TLM0202 | humidity | 35.64366062740866 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0203 | humidity | 35.89268792033798 | rfc3339time1 | . | airSensors | TLM0203 | humidity | 35.941017361190255 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0100 | co | 0.4925449578765155 | rfc3339time1 | . | airSensors | TLM0100 | co | 0.495877303713910 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0101 | co | 0.4925449578765155 | rfc3339time1 | . | airSensors | TLM0101 | co | 0.5074032895942003 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0102 | co | 0.4941631223400505 | rfc3339time1 | . | airSensors | TLM0102 | co | 0.5102716855442547 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0103 | co | 0.4046734235304059 | rfc3339time1 | . | airSensors | TLM0103 | co | 0.4245915521103036 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0200 | co | 0.5317633226995193 | rfc3339time1 | . | airSensors | TLM0200 | co | 0.5435288991742965 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0201 | co | 0.5000439017037601 | rfc3339time1 | . | airSensors | TLM0201 | co | 0.4813785832360323 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0202 | co | 0.48071553398947864 | rfc3339time1 | . | airSensors | TLM0202 | co | 0.4911462705616819 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0203 | co | 0.4089308476612381 | rfc3339time1 | . | airSensors | TLM0203 | co | 0.42797647488504065 | rfc3339time2 | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/input-format-vs-output-format/#real-world-data",
    "relUrl": "/docs/part-2/input-format-vs-output-format/#real-world-data"
  },"23": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "Introduction to InfluxDB Tools",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/"
  },"24": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "Table of contents",
    "content": ". | Flux and the Task Engine | InfluxDB User Interface | Telegraf | Awesome CLI | Rest API | Client Libraries | VSCode Plugin | Stacks and Templates | . To honor our commitment to developer happiness, InfluxData offers a wide range of tools working with time series data easily for all types of developers. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#table-of-contents",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#table-of-contents"
  },"25": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "Flux and the Task Engine",
    "content": "Flux is a functional query and scripting language. Flux enables you to: . | Transform and analyze data. | Write Tasks | . Flux has a Javascript inspired syntax that makes it easily composable and readable. Pipe-forward operators separate out functions and make data transformations flow smoothly. Flux is also testable, shareable, and contributable. The Task Engine executes Flux scripts on a schedule. It allows you to: . | Process data to make visualizations screaming fast | Get alerted if your data stops writing or breaches certain thresholds | Periodically call an external service with data from InfluxDB | . The Task engine can tackle all the points above with no additional code or operations on your part. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#flux-and-the-task-engine",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#flux-and-the-task-engine"
  },"26": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "InfluxDB User Interface",
    "content": "The InfluxDB UI provides a complete user interface for working with time series data and InfluxDB. The InfluxDB UI enables you to: . | Build queries to visualize your time series data. You can select from a wide variety of visualization types. The InfluxDB UI also supports geotemporal map visualizations. | Edit Flux code in the Flux Script Editor. | Build dashboards and notebooks. | Manage multiple users in your Organization. | Build and manage tasks. Tasks are Flux scripts that run on a schedule. | Build checks and notifications. Checks and notifications are specialized types of tasks which enable alert creation. | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#influxdb-user-interface",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#influxdb-user-interface"
  },"27": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "Telegraf",
    "content": "Have more stringent requirements for your writes? Need batching, retries, and other features? Don’t write this code yourself, just use Telegraf. Telegraf is InfluxData’s plugin driven collection agent for metric and events. With over 200 input plugins, Telegraf probably has an input plugin that fits your needs. But even if there isn’t a plugin for your exact use case, you can use telegraf to easily reformat any data type into your preferred output format, be it line protocol, json, csv, etc…. Telegraf isn’t just a tool for writing data to a destination data store. Telegraf processor plugins and aggregator plugins enable you to do so much more with your data than sophisticated collection and writes. For example, you can easily add data transformations with Starlark, and you can even use the execd processor plugin which makes Telegraf extensible in any language. It’s no wonder that Telegraf has close to 11,000 stars on github. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#telegraf",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#telegraf"
  },"28": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "Awesome CLI",
    "content": "The InfluxDB CLI allows users to interact with InfluxDB with ease. Features like configuration profiles and environment variable support make scripting and controlling InfluxDB a breeze. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#awesome-cli",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#awesome-cli"
  },"29": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "Rest API",
    "content": "All of InfluxDB is wrapped in a REST API. This API is highly compatible between OSS and Cloud. The API allows you to write data and query data, of course, but also has everything you need to manage the database, including creating resources like authentication tokens, buckets, users, etc… . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#rest-api",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#rest-api"
  },"30": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "Client Libraries",
    "content": "You aren’t limited to making REST calls on your own. If you prefer, InfluxDB has client libraries written in 13 languages. These Client Libraries enable you to drop InfluxDB functionality into an existing code base with ease or create a new code base. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#client-libraries",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#client-libraries"
  },"31": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "VSCode Plugin",
    "content": "If you are a Visual Studio (VS) Code user, then it’s easy to add InfluxDB to your workflow using Flux VS Code Extension. Using the Flux VS Code Extension to write Flux and run the query. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#vscode-plugin",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#vscode-plugin"
  },"32": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "Stacks and Templates",
    "content": "Want to back up or move your whole InfluxDB configuration? Add it to github so you can restore from a backup? Even use a gitops workflow to integrate with your existing CD (continuous deployment) process? InfluxDB supports all of this with our Stacks and Templates features. A Template is a prepackaged InfluxDB configuration that contains multiple InfluxDB resources. Templates include everything from Telegraf configuration, to Dashboards, to Alerts, and more. Use Templates to: . | get up and running with InfluxDB for a variety of common use cases with a Community Template,a community-contributed Templates. | quickly get setup with a new InfluxDB instance | backup your Dashboard, Alert, and Task configurations. | . Applying an existing Community Template is as easy as copy and pasting a URL for the Template yaml in the UI: . A Stack is a stateful InfluxDB template that lets you add, update, and remove template resources. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#stacks-and-templates",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#stacks-and-templates"
  },"33": {
    "doc": "Introduction to InfluxDB",
    "title": "Introduction to InfluxDB",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb/",
    "relUrl": "/docs/part-1/introduction-to-influxdb/"
  },"34": {
    "doc": "Introduction to InfluxDB",
    "title": "Table of contents",
    "content": ". | What is InfluxDB | The InfluxDB Advantage | Time to Awesome | Write and Query sample data | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb/#table-of-contents",
    "relUrl": "/docs/part-1/introduction-to-influxdb/#table-of-contents"
  },"35": {
    "doc": "Introduction to InfluxDB",
    "title": "What is InfluxDB",
    "content": "InfluxData is the company behind InfluxDB and Telegraf. InfluxDB, released in 2013, is the best time series database available for storing metrics and sensing data. It has since evolved into a full blown time series application development platform used by thousands of developers to create customer facing IoT, server monitoring, financial applications, bespoke monitoring applications for thousands of servers and devices, and many many other applications. Take a look at various case studies and customer testimonials from IBM, Adobe, Hulu, Cisco and more. InfluxDB is more than the leading time series database. InfluxDB also includes the InfluxDB User Interface (InfluxDB UI) and Flux. The InfluxDB UI is a time series management, visualization, and dashboarding tool. It also offers a script editor for Flux. Flux is a functional scripting and query language that enables data processing tasks like sophisticated data transformation and alerting. Telegraf is the open source server agent for collecting metrics and events. Telegraf is plugin driven and compiles into a single binary. There is a huge collection of input, output, aggregator, and parser plugins that enable developers to collect data, apply transformations to it, and write it to the destination datastore of their choice. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb/#what-is-influxdb",
    "relUrl": "/docs/part-1/introduction-to-influxdb/#what-is-influxdb"
  },"36": {
    "doc": "Introduction to InfluxDB",
    "title": "The InfluxDB Advantage",
    "content": "Paul Dix, founder and CTO of InfluxData, frames decision making around the concept of minimizing “time to awesome” or the hurdle to adoption and value. The developer experience is our priority at InfluxData. If you are thinking about creating an application related to time stamped data (IoT, Sensor Monitoring, Server Monitoring, Finance, etc…), InfluxDB is the easiest and most powerful development platform for you. This book will show you why that’s true. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb/#the-influxdb-advantage",
    "relUrl": "/docs/part-1/introduction-to-influxdb/#the-influxdb-advantage"
  },"37": {
    "doc": "Introduction to InfluxDB",
    "title": "Time to Awesome",
    "content": "You can choose between downloading and running the Open Source (OSS) version or creating a free account in InfluxDB Cloud and letting us run it for you. The OSS version comes as a single binary, so you can just download it and run it. There are also packages for popular Linux distributions. You can also sign up for a free cloud account which is just as easy as “click, click, authenticate your email, and get going”. Either way, getting up and running with InfluxDB literally takes a couple of minutes. Finally, when it comes to choosing between the OSS version of InfluxDB cloud, either choice is the right choice because we put a lot of effort into keeping the API’s consistent and compatible across our two offerings. So if you need, you can always switch over to the other later. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb/#time-to-awesome",
    "relUrl": "/docs/part-1/introduction-to-influxdb/#time-to-awesome"
  },"38": {
    "doc": "Introduction to InfluxDB",
    "title": "Write and Query sample data",
    "content": "The fastest way to write data into InfluxDB is to write some sample data with the script editor in the InfluxDB UI. Writing a sample dataset is a great way to get some meaningful data into the platform to get a feel for InfluxDB. You can pick whichever sample dataset you want to use, but we’ll use the NOAA ​​water sample data in this section. After setting up InfluxDB, navigate the Explorer page and click the + Create Bucket button. Name your bucket “noaa”. A bucket is a named location where you store your data in InfluxDB. Now navigate to the Script Editor and copy and paste the following Flux code from the documentation. You don’t have to understand this code right now, the only part to pay attention to is the to() function in the last line. import \"experimental/csv\" relativeToNow = (tables=&lt;-) =&gt; tables |&gt; elapsed() |&gt; sort(columns: [\"_time\"], desc: true) |&gt; cumulativeSum(columns: [\"elapsed\"]) |&gt; map(fn: (r) =&gt; ({ r with _time: time(v: int(v: now()) - (r.elapsed * 1000000000))})) csv.from(url: \"https://influx-testdata.s3.amazonaws.com/noaa.csv\") |&gt; relativeToNow() |&gt; to(bucket: \"noaa\", org: \"example-org\") . Make sure to change the following parameters in the to() function: . | The bucket you want to write the NOAA sample dataset to (if you created a “noaa” bucket already, then ignore this step). | The org to the email you used to register for a Cloud account or set up your OSS instance. | . Finally, hit** Submit**. Easily query your data through the UI by using the Query Builder. Simply select for the data you want to visualize and hit Submit. Next Section . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb/#write-and-query-sample-data",
    "relUrl": "/docs/part-1/introduction-to-influxdb/#write-and-query-sample-data"
  },"39": {
    "doc": "Part 1",
    "title": "Part 1",
    "content": "In this part, you’ll be introduced to InfluxDB and various tools. We’ll also learn how to set up InfluxDB so you can start expiriencing Awesome. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1",
    "relUrl": "/docs/part-1"
  },"40": {
    "doc": "Part 2",
    "title": "Part 2",
    "content": "In this part, you’ll gain an understanding for the InfluxDB data model, schema design bests practices and how to use Flux, the query and scripting language for InfluxDB. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2",
    "relUrl": "/docs/part-2"
  },"41": {
    "doc": "Setting Up InfluxDB",
    "title": "Introduction to InfluxDB",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/setting-up-influxdb/#introduction-to-influxdb",
    "relUrl": "/docs/part-1/setting-up-influxdb/#introduction-to-influxdb"
  },"42": {
    "doc": "Setting Up InfluxDB",
    "title": "Table of contents",
    "content": ". | Setting up InfluxDB Open Source | An introductory tour of the InfluxDB UI . | User and Account Information . | About | . | Data . | Buckets | Tokens | . | Explorer . | Query Builder | Flux Editor | . | . | Setting Up the CLI . | Find Your Host URL | Finding Your Org ID | Generating Your All-Access Token | Create and Test the CLI Config | . | Setting Up VSCode | . In general, I will assume that you are using the SaaS version of InfluxDB, InfluxDB Cloud. However, most of the content here should be applicable to the OSS standalone version, and when there are important differences between the two, I will point them out. Start by visiting the InfluxDB Cloud sign up page and fill in the sign up form: . The InfluxDB Cloud sign up page. Image 6 . After acknowledging the email verification, choose a Cloud Provider and a region. This is extremely useful if you are already running cloud-based services. InfluxDB Cloud account because InfluxDB Cloud runs in multiple regions across Google, Azure, and AWS clouds. If you don’t have a preference, just look for a region close to your locality. It’s possible to migrate data and the rest of the account later, but the process is not particularly streamlined, so if you do have a preference, keep that in mind. Note that you don’t need your own account with any of these providers to create an InfluxDB Cloud account. Next, choose a plan. A free tier plan is perfect for light workloads and getting started with your application. Converting a free tier plan to a paid plan is easy if you need more resources or if you are going into production later. A free tier plan does not require a credit card–just create a free InfluxDB Cloud account and get started. Now your account is ready to use. You’ll explore some options for uploading sample data or your real data in the next section. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/setting-up-influxdb/#table-of-contents",
    "relUrl": "/docs/part-1/setting-up-influxdb/#table-of-contents"
  },"43": {
    "doc": "Setting Up InfluxDB",
    "title": "Setting up InfluxDB Open Source",
    "content": "Running InfluxDB OSS is not much more involved than creating an InfluxDB Cloud account. Simply download the InfluxDB OSS single binary, run InfluxDB, and then access the InfluxDB UI in your browser. There are a few options for installing InfluxDB on the install page of the docs depending on your OS A few things to note: . | The Linux Arm versions are limited to 64 bit architectures, so they require a later version of Raspberry Pi, for example, if you are seeking to run it there. | InfluxDB is packaged in brew for the Mac, and many Linux distributions have native package manager support. However, some repositories still have an older 1.x version of InfluxDB in their repositories. | . Because InfluxDB is available as one simple binary, you can also download the latest release from the downloads portal. Depending on your installation method, influxdb may already be running. If not, run InfluxDB with with the influxd command: . $ influxd . And then access it at http://localhost:8086. You’ll be prompted to create an initial account to get started, and then the set up experience is pretty much identical to InfluxDB Cloud. The **Getting Started **page in the InfluxDB UI . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/setting-up-influxdb/#setting-up-influxdb-open-source",
    "relUrl": "/docs/part-1/setting-up-influxdb/#setting-up-influxdb-open-source"
  },"44": {
    "doc": "Setting Up InfluxDB",
    "title": "An introductory tour of the InfluxDB UI",
    "content": "Now that you’ve set up your InfluxDB account, take a moment to familiarize yourself with the InfluxDB UI. When you first log into your InfluxDB Account you should see the Getting Started home page. Navigate the User and Account Information icon and the Explorer page from the navigation bar to the right. The Getting Started home page highlights some of the key functionality of InfluxDB including the ability to: . | Load data | Build a dashboard | Set up alerting | . These panels will redirect you to corresponding pages available through the navigation bar to the right. In this section we’ll focus on the most critical areas of the InfluxDB UI for getting started with InfluxDB: . | User and Account Information icon | Explorer | . We’ll dive into the other areas later on. User and Account Information . The User and Account Information icon redirects you to corresponding Usage, Billing, and Organization (from the Users and Accounts options) pages: . About . The About tab in the Organization page provides Common ID’s that you’ll need for Authentication and Authorization within InfluxDB. These ID’s include your User ID and Organization ID (org ID). You’ll need these ID’s to configure the CLI, use a Client, and more. Data . The Data page provides you with multiple solutions for loading your data into InfluxDB including file upload, Client Libraries, and Telegraf. It also contains tabs for all the necessary resource management to successfully write data through those solutions. Specifically, the Data page allows you to: . | Load data from a variety of sources including file upload, client libraries, and Telegraf. | Create and manage your buckets. | Create and manage your Telegraf configurations. | Create and manage your Authentication tokens. Authentication tokens are required for a variety of InfluxDB tools. You’ll need an Authentication token to configure the CLI, use a Client, and more. | . Buckets . The Buckets tab allows you to create and delete buckets. Buckets are a named location to store data within InfluxDB. InfluxDB Data Model will cover buckets in detail. Click the + Create Bucket button to name generate a new bucket. Tokens . The Tokens tab allows you to create and delete tokens. InfluxDB Authentication tokens ensure secure interaction between users and data. You’ll need an Authentication token to configure the CLI, use a Client, and more. Important Note: The only way to create an All-Access token is through the InfluxDB UI. An All-Access token grants full access permissions to all resources in an organization. To create an All-Access token click the + Generate Token dropdown button and select All Access Token from the list. Name the token something meaningful and click Save. Click on the token name in the list to view and copy the token string out of the InfluxDB UI. It’s a good practice to save the token string somewhere safe outside of InfluxDB, like a password manager. Select **Copy to Clipboard **to copy your token. Explorer . The Data Explorer in the InfluxDB UI enables you to build, execute, and visualize your Flux Queries. The Query Builder offers you a no-code solution to building Flux queries, and the Script Editor is a Flux code editor where you can manually edit the query. These two Flux building and editing solutions enable both beginners and advanced Flux users alike. When you navigate to the Data Explorer page you’ll be placed in the Query Builder by default. Query Builder . Build a Flux query with the Query Builder by clicking on the data you want to visualize, selecting a time range, and applying an aggregation to your data. Hit Submit to run the query. Flux Editor . Click the Script Editor button to see the corresponding Flux that you generated with the Query Builder. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/setting-up-influxdb/#an-introductory-tour-of-the-influxdb-ui",
    "relUrl": "/docs/part-1/setting-up-influxdb/#an-introductory-tour-of-the-influxdb-ui"
  },"45": {
    "doc": "Setting Up InfluxDB",
    "title": "Setting Up the CLI",
    "content": "The InfluxDB CLI works equally well for both Cloud and OSS. This section will cover setting up the CLI and setting up a configuration to make the CLI easier to use. If you downloaded and installed InfluxDB OSS, you most likely already have the CLI installed. Otherwise, you can pick it up from the downloads page. Though optional, using the CLI is much easier if you set up a config. Instructions for creating a config are easily available at the command prompt. % influx config create -h The influx config create command creates a new InfluxDB connection configuration and stores it in the configs file (by default, stored at ~/.influxdbv2/configs). Examples: # create a config and set it active influx config create -a -n $CFG_NAME -u $HOST_URL -t $TOKEN -o $ORG_NAME # create a config and without setting it active influx config create -n $CFG_NAME -u $HOST_URL -t $TOKEN -o $ORG_NAME For information about the config command, see https://docs.influxdata.com/influxdb/latest/reference/cli/influx/config/ and https://docs.influxdata.com/influxdb/latest/reference/cli/influx/config/create/ Usage: influx config create [flags] Flags: -a, --active Set as active config -c, --active-config string Config name to use for command; Maps to env var $INFLUX_ACTIVE_CONFIG -n, --config-name string The config name (required) --configs-path string Path to the influx CLI configurations; Maps to env var $INFLUX_CONFIGS_PATH (default \"/Users/rickspencer/.influxdbv2/configs\") -h, --help Help for the create command --hide-headers Hide the table headers; defaults false; Maps to env var $INFLUX_HIDE_HEADERS -u, --host-url string The host url (required) --json Output data as json; defaults false; Maps to env var $INFLUX_OUTPUT_JSON -o, --org string The optional organization name -t, --token string The token for host (required) . So you will need: . | The URL pointing to your Influxdb account or host URL . | The Org ID for your account. | An All-Access token. | . Find Your Host URL . For a cloud account, the base url is easiest to find in the web browser. For the rest of the book we’ll use the following host URL: . https://eastus-1.azure.cloud2.influxdata.com . Additionally if you know the region and cloud provider you selected during account creation, you can lookup your host URL in this complete list of host URLs. If you are using Open Source, it is most likely: . http://localhost:8086 . Finding Your Org ID . You can find your org ID in the UI by navigating to the about page, as described above. For the rest of the book we’ll use the following Org ID: 0261487478164c85. Generating Your All-Access Token . You can generate an All-Access token through the InfluxDB UI, as described above. For the rest of the book we’ll use the following All-Access token: . w4NgOD1k941sMMBlw4L9KxEUsX5gC5Ix5_9u2r-Ac3Z8u6xAlIhHqT5Gu2t2XKsdxU6_tDyc4tOg_qBnpwXRyQ== . Create and Test the CLI Config . Now I you can issue the command to create the config: . % influx config create -u https://eastus-1.azure.cloud2.influxdata.com -o 0261487478164c85 -t w4NgOD1k941sMMBlw4L9KxEUsX5gC5Ix5_9u2r-Ac3Z8u6xAlIhHqT5Gu2t2XKsdxU6_tDyc4tOg_qBnpwXRyQ== --config-name cloud Active Name URL Org cloud https://eastus-1.azure.cloud2.influxdata.com 0261487478164c85 . We can make sure the CLI is working by issuing a simple influx bucket list command: . % influx bucket list ID Name Retention Shard group duration Organization ID 964221964e8fa8ab _monitoring 168h0m0s n/a 0261487478164c85 31ad8b129cd7db7a _tasks 72h0m0s n/a 0261487478164c85= . You should expect a list of your buckets with additional detail about them. Every InfluxDB instance contains two default system buckets: . | _monitoring | _tasks | . We’ll dive into more detail about these buckets later on, but you should always expect two buckets in the output from your influx bucket list command at a minimum. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/setting-up-influxdb/#setting-up-the-cli",
    "relUrl": "/docs/part-1/setting-up-influxdb/#setting-up-the-cli"
  },"46": {
    "doc": "Setting Up InfluxDB",
    "title": "Setting Up VSCode",
    "content": "The Visual Studio Code Flux Plugin is designed to allow you to integrate InfluxDB as a backend into your existing application development process, and it is relatively easy to set up the InfluxDB VSCode Extension. The Flux extension is easy to find by searching for “Influx” in the extension manager. After installing the extension, you can see an InfluxDB window added to the bottom left. Begin setting up a connection with my Cloud account by giving focus to the InfluxDB window and clicking the + button, and complete the form. After saving, the InfluxDB window should be populated. Part 2 . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/setting-up-influxdb/#setting-up-vscode",
    "relUrl": "/docs/part-1/setting-up-influxdb/#setting-up-vscode"
  },"47": {
    "doc": "Setting Up InfluxDB",
    "title": "Setting Up InfluxDB",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/setting-up-influxdb/",
    "relUrl": "/docs/part-1/setting-up-influxdb/"
  }
}
