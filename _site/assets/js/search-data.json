{"0": {
    "doc": "Deletes",
    "title": "Deletes",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/deletes/",
    "relUrl": "/docs/part-2/deletes/"
  },"1": {
    "doc": "Deletes",
    "title": "Table of contents",
    "content": ". | Deletes | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/deletes/#table-of-contents",
    "relUrl": "/docs/part-2/deletes/#table-of-contents"
  },"2": {
    "doc": "Deletes",
    "title": "Deletes",
    "content": "You can delete time series data with either: . | The InfluxDB CLI command influx delete. | The InfluxDB v2 API delete endpoint. | . Use the InfluxDB CLI to delete data from a bucket within a specific time range, measurement, and tags. For example, if we wanted to delete some sensor data from the Air Sensor sample dataset we could use the following command: . influx delete --bucket 'Air sensor sample dataset' \\ --start 'rfc3339time1' \\ --stop 'rfc3339time2' \\ --predicate '_measurement=\"airSensors\" AND sensor_id=\"TM0100\"' . Remember the rfc3339time timestamps are in the form of %Y-%m-%dT%H:%M:%SZ, with an example being: 2020-03-01T00:00:00Z. After making the delete, the following query returns no results: \\ . from(bucket: \"Air sensor sample dataset\") |&gt; range(start: rfc33391, stop: rfc33392) |&gt; filter(fn: (r) =&gt; r._measurement == \"airSensors\") |&gt; filter(fn: (r) =&gt; r.sensor_id == \"TM0100\") . Similarly, you can use the InfluxDB v2 API to delete the same data with: . curl -i -XPOST '&lt;yourInflxDBURL&gt;api/v2/delete?orgID=&lt;yourOrgID&gt;bucket=&lt;yourBucketID&gt;\\ --header 'Authorization: Token &lt;yourToken&gt;' \\ --data-raw '{ \"start\": \"rfc3339time1\", \"stop\": \"rfc3339time2\", \"predicate\": '_measurement=\\\"airSensors\\\" and sensor_id=\\\"TM0100\\\"' }' . Remember you’ll want to replace &lt;yourInflxDBURL&gt; with either: . | The InfluxDB OSS UR . | [http://localhost:8086/](http://localhost:8086/) by default | . | The InfluxDB Cloud URL from your correct region and provider . | https://us-west-2-1.aws.cloud2.influxdata.com/ for example | . | . Please review Part 1, to learn about how to gather your &lt;/code&gt; and &lt;/code&gt;. Next Section . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/deletes/",
    "relUrl": "/docs/part-2/deletes/"
  },"3": {
    "doc": "Designing Your Schema",
    "title": "Designing Your Schema",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/",
    "relUrl": "/docs/part-2/designing-your-schema/"
  },"4": {
    "doc": "Designing Your Schema",
    "title": "Table of contents",
    "content": ". | General Recommendations for Schema Design and Data Layout | Series Cardinality . | How to estimate Cardinality | Dependent Tags | . | Runaway Cardinality . | Causes | . | Using Tags Intelligently . | Just Enough Flux | from() |&gt; range() |&gt; filter() | The Purpose of Tags . | Incorrectly casting tags as fields for the Air Quality dataset | Correctly using tags for the Air Quality Dataset | . | The Purpose of Measurements | . | Data Partitioning . | Single Bucket | Bucket per User | Org per Customer | . | Metrics vs. Events | Enforcing a Schema | . Figuring out the best data layout or schema for InfluxDB is important in optimizing the resources used by InfluxDB, as well as improving ingestion rates and the performance of Flux queries and tasks (scheduled Flux scripts). You also want to consider developer experience when designing your schema. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/#table-of-contents",
    "relUrl": "/docs/part-2/designing-your-schema/#table-of-contents"
  },"5": {
    "doc": "Designing Your Schema",
    "title": "General Recommendations for Schema Design and Data Layout",
    "content": "Generally, you should abide by the following recommendations when designing your schema: . | Keep bucket and measurement names short and simple. | Avoid encoding data in measurement names. | Encode meta data in tags. | Commonly queried metadata should be stored in tags for better query performance because tags are indexed while field values are not indexed. | Limit the number of series or try to reduce series cardinality and avoid runaway series cardinality. | Separate data into different buckets when you need to either: . | assign different retention policies to that data. | or need to scope authentication token to that bucket. | . | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/#general-recommendations-for-schema-design-and-data-layout",
    "relUrl": "/docs/part-2/designing-your-schema/#general-recommendations-for-schema-design-and-data-layout"
  },"6": {
    "doc": "Designing Your Schema",
    "title": "Series Cardinality",
    "content": "Series cardinality is the number series in your InfluxDB instance. Remember, a series is defined by the unique combinations of measurements, tag sets, and fields. Part of the reason why InfluxData adopted the “schema on write” approach for InfluxDB is because series creation and the resulting indexing generally offers efficient queries. However, this is only true up to a point. Similar to an over-indexed relational database, it can be possible for too much cardinality to slow down writes and queries because the indexes tracking the groups get too large. How much cardinality is too much? There is no single answer for that. However, each free tier user is currently limited to 10,000 total cardinality in their whole InfluxDB Cloud account. Pay as You Go (PaYG) customers are limited to 1,000,000 cardinality total. How to estimate Cardinality . To estimating your cardinality: . | Multiply the possible tag values for each possible tag for a worst case estimate. | Multiply the value in step 1 by the number of fields for a worse case estimate. | Repeat steps 1 and 2 for each measurement and each bucket, and then sum all of the values. | . In other words, the worst-case cardinality for a measurement = number of tag keys * number of tag values * number of field keys. The reason that this is “worst-case” is because it overestimates the total series in the presence of dependent tags. The worst-case cardinality equation assumes that every tag set is unique. In reality sometimes tags are redundant or dependent. Dependent Tags . A dependent tag is scoped by another tag. Dependent tags don’t influence the series cardinality, and should be removed from the cardinality calculation. Let’s take a look at the following line protocol to demonstrate how a dependent tag doesn’t influence the series cardinality. measurement1,tag1=\"tagvalue1\",tag2=\"tagvalue3\" field1=1i,field2=1,field3=\"a\" unixtime1 measurement1,tag1=\"tagvalue2\",tag2=\"tagvalue4\" field1=2i,field2=2,field3=\"b\" unixtime2 measurement1,tag1=\"tagvalue1\",tag2=\"tagvalue2\" field1=1i,field2=1,field3=\"a\" unixtime3 measurement1,tag1=\"tagvalue2\",tag2=\"tagvalue4\" field1=2i,field2=2,field3=\"b\" unixtime4 . In this instance the “tag2” tag key is a dependent tag because its tag value is dependent on the value of the “tag1” tag key. In other words, every time the value of the “tag1” tag key is “tagvalue1” the value of the “tag2” tag key is “tagvalue3”. Similarly, every time the “tag1” tag key is “tagvalue2” the value of the “tag2” tag key is “tagvalue4”. In this way “tag2” is scoped by “tag1”. If we were to calculate the series cardinality without removing the dependent tag, “tag2” we would get: . 1 measurement x ( 2 tag values for “tag1” x 2 tag values for “tag2”) x** 3** fields = 12 overestimated series cardinality . Again, each series is identified by their unique tag keys, tag values, and field key combinations. Because a series is defined in part by a unique set of tag values, in this case, the introduction of tag2 does not change the table or series count in the underlying data model. Therefore to correctly calculate the series cardinality, **remove **the dependent tag, “tag2” from the equation: . 1 measurement x 2 tag values for “tag1” x** 3** fields = 6 **actual **series cardinality . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/#series-cardinality",
    "relUrl": "/docs/part-2/designing-your-schema/#series-cardinality"
  },"7": {
    "doc": "Designing Your Schema",
    "title": "Runaway Cardinality",
    "content": "While calculating series cardinality can be challenging. Accidentally, overestimating your series cardinality shouldn’t be a real concern of yours–afterall InfluxDB has tools which calculate your cardinality for you (more on that later). However, you do need to make sure to avoid runaway cardinality. Runaway series cardinality occurs when you load your tags or measurements with data that is potentially unbounded. To illustrate a runaway series cardinality scenario, consider the following hypothetical situation in which we’re building an IoT application on top of InfluxDB. For this hypothetical IoT application example we’re gathering the following data: . | We have a set of customers. Each customer is identified with a single “customer_id” tag value under the “customer_id” tag key. | Each customer has a number of devices. Each one of the customer’s devices is identified with device id tag values under the “device_id” tag key. | Each device reports fields. | . For this example, cardinality = Cn _ * _Dn * F . Where… . _Cn = the number of “customer_id” tag values _ . Dn = the number of “device_id” tag values . _F = the number of field keys _ . Assuming your customers continue to grow, you can see how cardinality may “explode”. For example, if you start with 5 customers each with 5 devices, each with 1 field, total cardinality is 5 * 5 * 1, or a mere 25. However, if you end up with 100,000 customer, with 10 devices each, each with 5 fields, you end up with 100,000 * 10 * 5, or 5,000,000. Whoops, you blew your cardinality limit! . Causes . The most common schema design mistakes that can lead to runaway cardinality are: \\ . Mistake 1: Log messages as tags. Solution 1: We don’t advise that anyone store logs as tags due to the potential for unbounded cardinality (e.g. logs likely contain unique timestamps, UUIDs, etc). You can store attributes of a log as a tag, as long as the cardinality isn’t unbounded. For example, you could extract the log level (error, info, debug) or some key fields from the log message. Storing logs as a field is ok, but it is less efficient to search (essentially table scans), compared to other solutions. Mistake 2: Too many measurements. This typically happens when people are moving from —or think of InfluxDB as— a key-value store. So for example, if you’re writing system stats to an InfluxDB instance you might be inclined to write data like so: Cpu.server-5.us-west.usage_user value=20.0 . **Solution 2: **Instead encode that information as tags like so: cpu, host=server-5, region = us-west, usage_user=20.0 . Mistake 3: Making ids (such as eventid, orderid, or userid) a tag. This is another example that can cause unbounded cardinality if the tag values aren’t scoped. Solution 3: Instead, make these metrics a field. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/#runaway-cardinality",
    "relUrl": "/docs/part-2/designing-your-schema/#runaway-cardinality"
  },"8": {
    "doc": "Designing Your Schema",
    "title": "Using Tags Intelligently",
    "content": "Understanding how to properly use tags can not only help prevent runaway series cardinality but also increase your Flux query performance. In this section, we’ll refer to the Air sensor sample dataset to illustrate how to use tags intelligently. The Air sensor sample dataset represents an IoT use case. It contains the following schema: . | 1 measurement: airSensors | 3 fields: co, humidity, temperature, | 1 tag key: sensor_id | 8 tag values: TLM0100, TLM0101, TLM0102, TLM0103, TLM0200, TLM0101, TLM0202, TLM0203 | . Visualizing co, humidity, and temperature for th TLM0100 sensor from the Air sensor sample dataset after writing it to InfluxDB with the to() function as described in Write and Query Sample Data. Just Enough Flux . So far we have only discussed querying InfluxDB in an abstract manner. In order to understand the impact that tags have on Flux query performance, we need to take a moment to learn some Flux basics. This section is a crash course on Flux, the query, data transformation, and scripting language for InfluxDB 2.0. The aim of this section is to provide you with just enough basic understanding of Flux to be able to interpret the examples. We’ll deep dive into the Flux language in the following chapter. from() |&gt; range() |&gt; filter() . The following Flux query queries for the co, humidity, and temperature fields from the TLM0100 sensor from the Air sensor sample dataset bucket. from(bucket: \"Air sensor sample dataset\") |&gt; range(start: -1h, stop: now()) |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"airSensors\") |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"co\" or r[\"_field\"] == \"humidity\" or r[\"_field\"] == \"temperature\") |&gt; filter(fn: (r) =&gt; r[\"sensor_id\"] == \"TLM0100\") . The most basic Flux queries contain three functions: . | The from() function. A Flux query typically starts with a from() function. This function retrieves data from a specified bucket. | The range() function. The range() function must follow the use of a from() function. The range() function filters records based on time bounds provided to the start and stop parameters. | You can pass relative durations (see the example above), absolute time (2019-08-28T22:00:00Z) , or integers (1567029600) into the start and stop parameters. | . | The filter() function. The filter() function filters records based on conditions specified in the predicate function, fn. You can use the filter function() to filter for specific measurements, tags, and fields. You can also use the filter function to filter values based on thresholds and apply conditional query logic. The order of subsequent filter() functions doesn’t have an impact on performance. | . In Flux, functions are connected together through the |&gt; pipe-forward operator. The Purpose of Tags . As covered in the previous section, tag keys are grouped by the storage engine based on series. This allows the storage engine to quickly find points based on the tag keys and fields rather than scanning through all of your data to find the results of a query. To illustrate how indexing improves query performance, let’s consider the Air sensor sample dataset. Remember, the specific sensors that gather air quality data are tagged with a “sensor_id”. Now imagine that you want to query the dataset for all of the data from a specific sensor with a unique “sensor_id” value. When you apply a filter to query for a single tag value, the storage engine can use the indexed tag to quickly find the relevant table in the storage engine and return all the data associated with that tag value quickly. Filtering for a TLM0100 tag value from the “sensor_id” tag. Incorrectly casting tags as fields for the Air Quality dataset . However, if “sensor_id” was a field instead of a tag, then that sensor id isn’t indexed. Therefore, the storage engine would pull from one massive table for all of the sensor ids, using only the timestamps in the range() function as direction for which subset of data to pull from. In that instance, the storage engine would have to scan through all of the rows of that one massive table in order to pull out the rows that match the specific sensor id field value we’re filtering for. This scanning and row selection process is far less efficient than simply streaming out the relevant tables in their entirety. Casting the sensor id as a field presents additional query challenges and performance problems. Most likely you obtain senor id data because you want to know what the humidity or temperature is for a specific sensor. However, querying for the temperature from a specific sensor becomes extra challenging if your sensor id is a field. You would have to: . | Query for the sensor id field values. | Query for the temperature field values. | Join those two query results on timestamp–**assuming **that you only have one sensor reporting temperature values at each timestamp. | . In other words if the “sensor_id” was a field instead of a tag and you wanted to query for the temperature from that sensor you would have to query for the following 2 tables first: . | _measurement | _field | _value | _time | . | airSensors | sensor_id | TLM0100 | rfc3339time1 | . | airSensors | sensor_id | TLM0100 | rfc3339time2 | . | airSensors | sensor_id | TLM0101 | rfc3339time3 | . | airSensors | sensor_id | TLM0101 | rfc3339time4 | . | _measurement | _field | _value | _time | . | airSensors | temperature | 73.57230556533555 | rfc3339time1 | . | airSensors | temperature | 71.2194835668512 | rfc3339time2 | . | airSensors | temperature | 71.80350992863588 | rfc3339time3 | . | airSensors | temperature | 71.78232293801005 | rfc3339time4 | . To find the temperatures for the sensor TLM0100 the storage engine would need to read through the first table and pull out rows that have a _value of “TLM0100” then find matching timestamps in the second table and then merge those two tables and return them. When those tables become millions of rows long, you can see how this can slow down response time. Correctly using tags for the Air Quality Dataset . All of the problems in the section above are avoided when the sensor id is a tag. When you filter for the temperature for a specific “sensor_id” tag value, the storage engine simply reads out the first table as listed below because that series is indexed. The storage engine is able to return all the temperature results for a specific “sensor_id” even if the table has millions of rows (or the series has millions of points). | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0100 | temperature | 73.57230556533555 | rfc3339time1 | . | airSensors | TLM0100 | temperature | 71.2194835668512 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0101 | temperature | 71.80350992863588 | rfc3339time1 | . | airSensors | TLM0101 | temperature | 71.78232293801005 | rfc3339time2 | . The Purpose of Measurements . As covered in the previous section, measurements are grouped by the storage engine based on series. This allows the storage engine to quickly find points based on the measurements rather than scanning through all of your data to find the results of a query. The purpose of measurements is similar to the purpose of tags, but measurements offer a higher level of organization to your data. If your fields or tags are related, they should go into one measurement. Writing related data to the same measurement helps you avoid having to query for multiple measurements and perform joins. For example, imagine we’re developing a human health app and gathering blood oxygen levels, heart rate, and body temperature data. I would write the blood oxygen level, heart rate, and body temperature fields to one measurement because I foresee needing to simultaneously visualize and compare a combination of the metrics to assess human health. While you could separate out the fields into different measurements for a human health app, you’d most likely have to use joins() to perform math across measurements which are more computationally expensive. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/#using-tags-intelligently",
    "relUrl": "/docs/part-2/designing-your-schema/#using-tags-intelligently"
  },"9": {
    "doc": "Designing Your Schema",
    "title": "Data Partitioning",
    "content": "The final consideration for schema design in InfluxDB is finding the best approach to bucket and org data partitioning, especially when developing a customer facing IoT application on top of InfluxDB. There are 3 options: . | Single Bucket | Bucket per User | Org per Customer | . Remember authentication tokens can be restricted to read or write from individual buckets. Additionally each bucket is assigned one retention policy. Like tags and measurements, buckets are also indexed. Single Bucket . The single bucket design has the following advantages: . | Writing all of your data to a single bucket makes querying for data easier. You don’t have to keep track of which data exists within multiple buckets. You’ll likely need to use multiple measurements to organize your data effectively. However, you can still easily perform aggregations across multiple measurements by grouping the measurements together. | You can perform one downsampling task to downsample all of your data. Downsampling tasks are used to reduce the overall disk usage of your data by transforming high resolution data into lower resolution aggregates. | . The single bucket design has the following disadvantages: . | Each bucket is assigned one retention policy. With this design you can’t expire data subsets at different retention intervals. | If you need to query for a invicidual user’s data within a single bucket for your application, you should generate new read tokens for each individual customer. However, this design is less secure against malicious attacks than isolating different users’ data in separate buckets. | . The single bucket approach is good for use cases where: . | You’re developing a simple IoT application, like the Air sensor sample dataset. | You intend on treating all of your user’s data in the same way. Your data is being collected at similar intervals and a single retention policy is an effective solution for proper time series database management. | Your data is relatively non-sensitive and preventing a data breach is of little or no concern. | . Bucket per User . The bucket per user design has the following advantages: . | Your customer’s data is isolated and secure. | You can assign different retention policies to your different buckets depending on your customer’s needs. | . The bucket per user design has the following disadvantages: . | You can’t visualize multiple users’ data without joining the data across the buckets first. | Performing math across your users’ data is hard. For example if you want to know which user is reporting a max value for their field, you must first join all of your data together across the different user buckets first. | You’ll likely need to write multiple downsampling tasks to downsample all of your user’s data. You can automate some of this downsampling task creation with the use of parameterized queries, but it’ll require a little more work. | . The bucket per user design is good for use cases where: . | You’re developing a more sophisticated IoT application and the data is sensitive, like a human health application. | You’re writing sensitive data to InfluxDB and isolating your users’ data is a priority. | Your customers have different data requirements and you need to provide them with different time series data management solutions (including different retention policies and downsampling tasks). | . Org per Customer . The org per customer design has the following advantages: . | You can meet the data requirements of users with advanced high throughput and cardinality use cases. | Your users’ data is isolated and secure. | . The org per customer design has the following disadvantage: . | You can’t easily query for data across multiple organizations. | . The org per customer design is good for use cases where: . | You’re developing an industrial IoT solution that can be used for multiple companies. | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/#data-partitioning",
    "relUrl": "/docs/part-2/designing-your-schema/#data-partitioning"
  },"10": {
    "doc": "Designing Your Schema",
    "title": "Metrics vs. Events",
    "content": "Understanding the difference between metrics and events is helpful for designing your schema. Metrics are time series data collected at regular intervals. Metrics are typically pulled from the data source. Metrics usually require regular downsampling tasks to convert the high resolution data to lower resolution aggregates for efficient disk usage management. Events are metrics collected at irregular intervals. They are typically pushed from the data source. A common design for industrial IoT application development is to write metrics from an IoT device to an OSS InfluxDB instance at the edge or in the fog. Then when those metrics meet certain conditions those events get pushed to the cloud. Metrics have an advantage over events, in that if metrics are written at precisely the same time intervals, the InfluxDB storage engine can apply aggressive compression when writing those metrics to disk. For Cloud, this will result in a small advantage in terms of read performance. For OSS users, this will also mean that you will be using less disk for the same metrics. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/#metrics-vs-events",
    "relUrl": "/docs/part-2/designing-your-schema/#metrics-vs-events"
  },"11": {
    "doc": "Designing Your Schema",
    "title": "Enforcing a Schema",
    "content": "Enforcing a schema with InfluxDB is important for maintaining consistent writes and for preventing data injections or bad writes. You can enforce a schema at the bucket level with explicit bucket schemas. You have to use the CLI to create a bucket with an explicit schema. First create a bucket with the influxd bucket create command and specify an explicit schema. influx bucket create \\ --name \"Air sensor sample dataset\"\\ --schema-type explicit . Next create a schema columns file for each measurement you want to add. We’ll enforce a schema for our airSensors measurement for our Air sensor sample dataset. Our schema columns file will be a CSV, but you can use JSON or NDJSON. Name the schema columns file (“airSensors_schema.csv”) and save it to your desired path. name,type,data_type time,timestamp, sensor_id,tag, co,field,float humidity,field,float temperature,field,float . Now we can add the schema to our airSensor measurement with the influx bucket-schema create command: . influx bucket-schema create \\ --bucket \"Air sensor sample dataset\" \\ --name airSensors \\ --columns-file airSensors_schema.csv . Specify the bucket that you want to create the schema for, the measurement to which your schema columns file should be applied to, and the path to your schema columns file. Next Section . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/designing-your-schema/#enforcing-a-schema",
    "relUrl": "/docs/part-2/designing-your-schema/#enforcing-a-schema"
  },"12": {
    "doc": "Home",
    "title": "Time to Awesome",
    "content": "A book on InfluxDB . ",
    "url": "http://localhost:4000/time-to-awesome/#time-to-awesome",
    "relUrl": "/#time-to-awesome"
  },"13": {
    "doc": "Home",
    "title": "Status",
    "content": "This book a is a work in progress. ",
    "url": "http://localhost:4000/time-to-awesome/#status",
    "relUrl": "/#status"
  },"14": {
    "doc": "Home",
    "title": "Table of Contents",
    "content": "Part 1 . | Introduction to InfluxDB | Introduction to InfluxDB Tools | Setting Up InfluxDB | . Part 2 . | InfluxDB Data Model | Input Format vs Output Format | Designing Your Schema | Introduction to Flux | Querying and Data Transformations | Deletes | Optimizing Flux performance | . Part 3 . | Writing Data | Tasks | Checks and Notifications | VS Code Integration | Telegraf | Monitoring and Managing InfluxDB | Reference | . ",
    "url": "http://localhost:4000/time-to-awesome/#table-of-contents",
    "relUrl": "/#table-of-contents"
  },"15": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/",
    "relUrl": "/"
  },"16": {
    "doc": "InfluxDB Data Model",
    "title": "InfluxDB Data Model",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/influxdb-data-model/",
    "relUrl": "/docs/part-2/influxdb-data-model/"
  },"17": {
    "doc": "InfluxDB Data Model",
    "title": "Table of contents",
    "content": ". | InfluxDB Data Elements . | Buckets | Measurements | Tag Sets | Field Sets | . | Series and Points | Assumptions and Conventions . | Conventions Used In This Book | Meta Syntax for Examples | . | . The goal of this section is to provide the reader with a firm foundation in the InfluxDB data model, specifically: . | Understanding the InfluxDB input format (line protocol). | Understanding the InfluxDB output format (annotated CSV). | The relationships between these two formats. | Understanding how the Influxdb storage engine persists data in a table on disk | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/influxdb-data-model/#table-of-contents",
    "relUrl": "/docs/part-2/influxdb-data-model/#table-of-contents"
  },"18": {
    "doc": "InfluxDB Data Model",
    "title": "InfluxDB Data Elements",
    "content": "Buckets . All data in InfluxDB gets written to a bucket. A bucket is a container that can hold points for as many measurements as you like. Buckets have some important properties: . | They can be named whatever you want (within reason). | You can create tokens that can control read and write permissions for a bucket, scoped only to a specific bucket. | You must set a retention period on a bucket, upon creation. A retention period determines how long InfluxDB will store your time series data within that bucket. Retention periods are critical for time series database management. They provide users with a convenient solution for automatic expiration of old, useless data which enables them to focus on the recent, valuable data instead while reducing their storage bills. | . These topics will be covered in detail in a later section, for now, it is enough to know that measurements are stored in and read from a bucket. Measurements . A measurement is the highest level of data structure within a bucket. InfluxDB accepts one measurement per point. Use a measurement to organize similar data. In some ways, you can think of it as analogous to a table in a traditional database. Measurements are also indexed, which enables you to query data within a measurement more quickly when you filter for a specific measurement. Measurements must be a string type. Measurement names cannot begin with an underscore. To further understand measurements, let’s imagine you’re building a weather app and writing temperature data across multiple cities to InfluxDB. For this time series use case, you might create a measurement named “air_temperature”. Tag Sets . A tag set consists of key-value pairs, where the values are always strings. Tags are essentially metadata, typically encoding information about the source of the data. Imagine you’re building a weather app and writing temperature data across multiple cities to InfluxDB. For this time series use case, you might add a tag key to your data called “location” where the “location” tag key contains tag values for the cities weather you’re monitoring. This way you can easily query for all of the temperature data for “Los Angeles”, for example, by filtering for the “location” tag key with the “Los Angeles” tag value. Critically, tags are indexed, so they are an important element of designing for query performance. However, tags are also optional. Tag keys cannot begin with an underscore, because InfluxDB reserves leading underscores for its own purposes. Field Sets . A field set consists of key-value pairs, where the values can be strings, integers, or floats. Fields are the actual data to store, visualize, use for computations, etc… . Building on the weather app example, the temperature readings would be an example of a field. Field sets are required for InfluxDB. This is where you store the value of your time series data. Field values can be of either an integer, float, or string type. Since field values are almost always different, field keys are frequently referred to as just fields. Fields are not indexed. Field also cannot begin with an underscore. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/influxdb-data-model/#influxdb-data-elements",
    "relUrl": "/docs/part-2/influxdb-data-model/#influxdb-data-elements"
  },"19": {
    "doc": "InfluxDB Data Model",
    "title": "Series and Points",
    "content": "A series is defined by the unique combination of measurement, tag set(s), and fields. A point is a datapoint from a series at a specific timestamp. If you rewrite a duplicate point with identical measurement, tag set, field, and timestamp values, this write will overwrite your previous point. If you try to write a new point to the same series but you change the field type, this write will fail. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/influxdb-data-model/#series-and-points",
    "relUrl": "/docs/part-2/influxdb-data-model/#series-and-points"
  },"20": {
    "doc": "InfluxDB Data Model",
    "title": "Assumptions and Conventions",
    "content": "Before diving into more nuanced and technical topics around the InfluxDB Data Model, let’s take a moment to establish some baseline assumptions and conventions. Conventions Used In This Book . Chapters in this book will generally introduce concepts using abstract and simplified examples, and then follow with detailed examples using real world data. Meta Syntax for Examples . For the abstract and simplified examples, we will use names in the form of: . attributeorvaluen . Where “attributeorvalue” refers a column name of a table or a value in a point, and “n” is a number simply differentiating multiple identifiers of the same role. Roles can comprise any of the following: . | Measurement | Tag | Tag Value | Field | . Samples will also generally include: . | Field Values | Timestamps | . Field values will be represented by actual values. Timestamps are in the following timestamp formats: . | Unix: The unix timestamp is a way to track time as a running total of seconds–i.e. 1465839830100400200 | RFC3339: A standard for date and time representation from a Request For Comment document by the Internet Engineering Task Force (IETF)–​​i.e. 2019-08-28T22:00:000000000Z | Relative Duration: -1h | Duration: 1h | . Timestamps will be represented by actual values or by unixtime1 or rfc3339time1 . In case we want to refer to another timestamp in the same example, we will use unixtime2 or rfc3339time2. To refer to a measurement in an example, we will use: measurement1. In case we want to refer to another measurement in the same example, we will use measurement2, and so forth. An example of line protocol (explained in depth later) then, may look like: . measurement1,tag1=tagvalule1,tag2=tagvalue2 field1=1i,field2=2 1628858104 . From time to time an example may be focused on understanding a type. In such cases, we will use the form “atype” where “type” is the data type under focus. For example, if we are discussing that field names are always strings. we may say: . r._field == \"astring\" . Or if we are discussing type conflicts, we may say: . aint == afloat . Instead of an example with specific values such as: . 1i == 1.0 . Next Section . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/influxdb-data-model/#assumptions-and-conventions",
    "relUrl": "/docs/part-2/influxdb-data-model/#assumptions-and-conventions"
  },"21": {
    "doc": "Input Format vs Output Format",
    "title": "Input Format vs Output Format",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/input-format-vs-output-format/",
    "relUrl": "/docs/part-2/input-format-vs-output-format/"
  },"22": {
    "doc": "Input Format vs Output Format",
    "title": "Table of contents",
    "content": ". | Line Protocol . | Adding multiple fields | Types explained | Type conflicts | Adding tags | Adding timestamps | Note on Timestamp Precision | Overwriting points | . | Annotated CSV . | Raw CSV vs Annotated CSV | Header Row | Record Rows | Records vs Points | Annotation Rows | . | From Series to Tables on Disk . | Adding Fields | Adding Tags | . | Real World Data . | Exploring the Real Word Data Schema with Flux | Exercises with Real World Data | . | . The InfluxDB input format is line protocol. The InfluxDB output format is Annotated CSV. The input format is different from the InfluxDB persistence. The Annotated CSV output can match the InfluxDB persistence format with simple Flux queries. However, can add Flux transformations to your query such that the Annotated CSV output doesn’t reflect the InfluxDB persistence format. Understanding these subtle differences is critical for good schema design and for using InfluxDB optimally. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/input-format-vs-output-format/#table-of-contents",
    "relUrl": "/docs/part-2/input-format-vs-output-format/#table-of-contents"
  },"23": {
    "doc": "Input Format vs Output Format",
    "title": "Line Protocol",
    "content": "The smallest allowed line of line protocol includes a measurement, a single field, and a value for that field. A measurement is the highest grouping of data inside a bucket. A field is a part of that measurement and defines a single point. Fields are separated by measurements by a space. So, the smallest readable line looks like: . measurement1 field1=1i . Adding multiple fields . You need to supply at least one field, but you can supply as many as you need in a single line, separated by a comma: . measurement1 field1=1i,field2=1,field3=\"a\" . Line protocol is very compact, but each of the fields will be in their own series when stored in the database. More on that in From Series to Tables on Disk. Types explained . The types of a field value can be an integer, a float, or a string. By default a number will be interpreted as a float. The addition of the “i” after each number for my_field tells InfluxDB that I wanted those to be integers. Using quotes ensures that InfluxDB knows I want a string type. Type conflicts . Once you have created a series, you cannot change the type field type of the series. InfluxDB will reject the the following write: . measurement1 field1=1,field2=\"1\",field3=\"a\" . After having written this line: . measurement1 field1=1i,field2=1,field3=\"a\" . Notice how the field2 field value has been changed from a string to a float. Adding tags . Tags are another kind of data that you can add to a line in line protocol. Tags are useful because they are automatically indexed by InfluxDB. Using and querying for tags allows you to significantly improve your query performance. Additionally, tags are useful for categorizing your queries. Remember, a series is defined by a measurement(s), tag sets(s), and field key(s). Tags are defined after the measurement name, and separated from it by a comma. I can add a tag to the previous line protocol as such: . measurement1,tag1=tagvalue1 field1=1i,field2=1,field3=\"a\" 1626118680000000000 measurement1,tag1=tagvalue2 field1=2i,field2=2,field3=\"b\" 1626118740000000000 . The introduction of the tag key, “tag1”, with 2 different tag values, “tagvalue1” and “tagvalue2” produces 6 series–3 series come from the different fields for each of the 2 tag values. Adding timestamps . In cases where a timestamp is omitted from line protocol, InfluxDB will add a timestamp based on the current server time at the time of the write. Letting InfluxDB automatically supply a timestamp is very convenient, but is likely too imprecise for your application, so you will typically supply a timestamp as well. InfluxDB expects timestamps to be unix timestamps. InfluxDB also expects the timestamps to be in nanosecond resolution, but the write API allows you to define lower resolution precision if needed. The timestamp comes at the end of a line of line protocol and is separated from the last field value by a space. Supplying a timestamp to the above line protocol would look like this: . measurement1 field1=1i,field2=1,field3=\"a\" 1626118680000000000 measurement1 field1=2i,field2=2,field3=\"b\" 1626118740000000000 . Note on Timestamp Precision . The native resolution of time stamps in InfluxDB is nanoseconds. In InfluxDB, the unit of resolution (for example nanosecond, microsecond, millisecond) is called the “precision” of the time stamp. For context: . | There are 1,000 nanoseconds in a microsecond | There are 1,000,000 nanoseconds in a millisecond | There are 1,000,000,000 nanoseconds in a second | . This is important to keep in mind while constructing your application, because many systems do not handle nanosecond resolution, so it is necessary to convert between them. Note that InfluxDB tools do allow you to define the precision of your timestamps, so when writing, you can allow InfluxDB to handle the conversion for you. This will be covered in Part 3. Overwriting points . You can overwrite points in InfluxDB when you write data with the same series and same timestamp. For example, if you wrote this line of line protocol to InfluxDB: . measurement1 field1=2i,field2=2,field3=\"b\" 1626118740000000000 . You would then overwrite the 3 points by writing this line next. measurement1 field1=10i,field2=10,field3=\"overwritten\" 1626118740000000000 &lt;more examples that include partial overwrites, subsets of tags&gt; . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/input-format-vs-output-format/#line-protocol",
    "relUrl": "/docs/part-2/input-format-vs-output-format/#line-protocol"
  },"24": {
    "doc": "Input Format vs Output Format",
    "title": "Annotated CSV",
    "content": "Annotated CSV is the output format for InfluxDB. The annotated CSV output matches the InfluxDB persistence format with simple queries or when you don’t apply additional transformations to your data. Annotated CSV result is a stream of tables returned by Flux where each table represents a series for simple queries only. For example if you wrote this line protocol line to InfluxDB: . measurement1,tag1=tagvalue1 field1=1i . You would return the following full annotated CSV output when querying for all fields and tags (of which there are none) within the measurement: . #group,false,false,true,true,false,false,true,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,long,string,string,string #default,_result,,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 ,,0,rfc3339time1,rfc3339time2,2021-08-17T21:23:39.000000000Z,1,field1,Measurement1,tagvalue1 . Remember, that line of line protocol produces 1 series which is why one table is included in the annotated CSV output. Raw CSV vs Annotated CSV . The first thing to notice about the output format of annotated CSV is that it resembles the CSV format that you’re familiar with. To easily distinguish between the two we’ll refer to CSV as Raw CSV. Unlike raw CSV, annotated CSV contains the following rows: . | Header Row | Records Rows | Annotation Rows | . Header Row . The header row is similar to any header row in a CSV. It describes the column names for your time series data.The header row is found below the 3 Annotation rows. Our header row is: . ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 . Some of these headers are an intuitive translation from line protocol while others are not. The headers are: . | result. This column includes the name of the result as specified by the query. If no name is provided it will be empty. | table. This column contains a unique ID for each table in the annotated CSV output result. stream. In the example above we are only writing and querying for 1 series so the value of that column is set to 0 for that table. | _start.This column includes the start time for the time range over which you queried your data for. Specifying a range is necessary for any Flux query. | _stop.This column includes the stop time for the time range over which you queried your data for. Specifying a range is necessary for any Flux query. | _time.This column is the timestamp for your time series. The value of this column is either added upon write or included in the line protocol explicitly. | _value. This column contains the field values for the corresponding field keys in the same row under the _field column. | _field. This column contains the field keys. | _measurement. This column contains the name of the measurement. | tag1. This column contains the tag value for our tag1 tag key. | . For the rest of this section you can ignore the values for the _start, _stop, and result columns. Those values are assigned by the user during query execution. For now, just focus on understanding the similarities between your line protocol input format and the resulting annotated CSV. Record Rows . The records row(s) is directly below the header row. These rows contain our time series data. Each row contains one record. A record is a tuple of named values. Each table contains at least one record. When querying for your time series without adding any additional Flux transformations, a table . Records vs Points . Remember, a point is a datapoint from a series at a specific timestamp. A record can be a point but not always. For example, you could use Flux to drop all tag, field, and measurement columns. At this point the records in the annotated CSV wouldn’t reflect a point but rather time series data instead. Annotation Rows . The annotation rows are found in the first three lines of the full annotated CSV output. Annotations include metadata about your annotated CSV. When querying InfluxDB with the API, you can specify which headers you want to include in your annotated CSV output. All 3 headers are included by default when you export the results of your query through the InfluxDB UI. The 3 annotations are: . | ​​#group: A boolean that indicates the column is part of the group key. A group key is a list of columns for which every row in the table has the same value. A column is part of the group key if its ​​#group annotation is set to ​​true. Important Note: The exception is for the table column. The group key for the table is set to false because users can’t directly change the table number. The table record will always be the same across rows even though the group key is set to false. | #datatype: Describes the type of data or which line protocol element the column represents. | #default: The value to use for rows with an empty value. | . If annotations confuse you, don’t worry. The importance of annotations will become apparent in subsequent sections of this book, specifically around understanding Flux. Additionally it’s worth mentioning that the​​ #group annotation is the most important annotation for using Flux successfully. For now, be aware that InfluxDB applies a default group key to your data so that the tables in your annotated CSV output will each represent a single series for simple queries. This default application of group keys is the result of the way that series are stored on disk, described in the following section. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/input-format-vs-output-format/#annotated-csv",
    "relUrl": "/docs/part-2/input-format-vs-output-format/#annotated-csv"
  },"25": {
    "doc": "Input Format vs Output Format",
    "title": "From Series to Tables on Disk",
    "content": "Each series is stored as a table on disk. Data gets written to InfluxDB into buckets. When data is written to a bucket it is added to an appropriate table, or a new table is created if needed. Each table has exactly one measurement, one field, and a unique set of tag values. Indexes are created or amended on write to enable quickly finding these tables when queried. Additionally, all rows in all tables are indexed by time. Some people consider InfluxDB to be a “schemaless” database. However, that is not really accurate. More accurately, InfluxDB is a “schema on write.” That is to say, InfluxDB does not require a schema to be defined beforehand nor enforce a schema on writes. Instead, Influxdb builds a schema implicitly based on the writes you do make. While schema on write is a true convenience for developers, you should be cognizant of the schema that you are implicitly creating as you write data. A poorly designed schema can have a negative impact on things like ease of querying, performance, and cardinality. In this section we’ll learn about how line protocol produces series which get stored as tables on disk. Adding Fields . In this section we’ll break down how the line protocol is converted to series and written as tables in InfluxDB. Let’s review the line protocol examples above: . measurement1 field1=1i,field2=1,field3=\"a\" measurement1 field1=1i,field2=2,field3=\"b\" . When written, this will form 3 different time series with 2 points in each series, assuming each line was written one minute apart. The data will then be persisted in three separate tables: . | _measurement | _field | _value | _time | . | measurement1 | field1 | 1i | 2021-07-12T19:38:00.000000000Z | . | measurement1 | field1 | 1i | 2021-07-12T19:39:00.000000000Z | . | _measurement | _field | _value | _time | . | measurement1 | field2 | 1 | 2021-07-12T19:38:00.000000000Z | . | measurement1 | field2 | 2 | 2021-07-12T19:39:00.000000000Z | . | _measurement | _field | _value | _time | . | measurement1 | field3 | a | 2021-07-12T19:38:00.000000000Z | . | measurement1 | field3 | b | 2021-07-12T19:39:00.000000000Z | . This example provides the first insights into how the input format and the persistence/output format differ. Each series in InfluxDB is persisted with exactly one field, though line protocol allows writing with multiple fields. This is a critically important concept to understand, so it is worth repeating. The InfluxDB input format is different from the InfluxDB persistence and output format. These tables represent the minimum data roles that can exist in a series. A series must have at a minimum: . | A measurement name | A field key | A value for the field | A time | . These are represented in the database with a leading “_”. The leading underscore conveys that these are “slots” in the series that are enforced by the storage engine, as well as helps to avoid naming conflicts with potential tag names. When simply querying for this data, without adding any additional Flux transformations, the annotated CSV output looks like: . #group,false,false,true,true,false,false,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,long,string,string #default,_result,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement ,,0,rfc3339time1,rfc3339time2,2021-07-12T19:39:00.000000000Z,1,field1,measurement1 ,,0,rfc3339time1,rfc3339time2,2021-07-12T19:38:00.00000000Z,1,field1,measurement1 #group,false,false,true,true,false,false,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,string,string,string #default,_result,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement ,,1,rfc3339time1,rfc3339time2,2021-07-12T19:39:00.000000000Z,a,field3,measurement1 ,,1,rfc3339time1,rfc3339time2,2021-07-12T19:38:00.00000000Z,b,field3,measurement1 #group,false,false,true,true,false,false,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,double,string,string #default,_result,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement ,,2,rfc3339time1,rfc3339time2,2021-07-12T19:39:00.000000000Z,1,field2,measurement1 ,,2,rfc3339time1,rfc3339time2,2021-07-12T19:38:00.000000000Z,2,field2,measurement1 . Notice how the resulting annotated CSV contains 3 tables in the output. This is evident by the row separation and also by the value of the table column in the last stream of the table which is equal to 2 (remember annotated CSV counts the table results from 0). Group keys have been added to the data to produce these tables so that each table represents a series by default. Remember a column is part of a group key if all of the values in that column are identical within a single table. For example, the time and value columns are assigned a #group annotation of ​​false. Setting the #group annotation to ​​false allows the different timestamps and field values of points across a single series to be included in the same table. Adding Tags . Let’s review the line protocol example above with an added tag: . measurement1,tag1=\"tagvalue1\" field1=1i,field2=1,field3=\"a\" 1626118680000000000 measurement1,tag1=\"tagvalue2\" field1=2i,field2=2,field3=\"b\" 1626118740000000000 . The introduction of tag1 produces the following 6 series: . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue1 | field1 | 1i | 2021-07-12T19:38:00.000000000Z | . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue1 | field2 | 1 | 2021-07-12T19:38:00.000000000Z | . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue1 | field3 | a | 2021-07-12T19:38:00.000000000Z | . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue2 | field1 | 2i | 2021-07-12T19:39:00.000000000Z | . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue2 | field2 | 2 | 2021-07-12T19:39:00.000000000Z | . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue2 | field3 | b | 2021-07-12T19:39:00.000000000Z | . Ensure that you take the time to study and understand the relationship between the line protocol and the resulting tables as they are represented on disk by the storage engine. This relationship is critically important to achieving the most effective schema and querying for your application. When simply querying for this data, without adding any additional Flux transformations the annotated CSV output looks like: . #group,false,false,true,true,false,false,true,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,string,string,string,string #default,_result,,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 ,,0,rfc3339time1,rfc3339time2,2021-07-12T19:39:000000000Z,b,field3,measurement1,tagvalue2 #group,false,false,true,true,false,false,true,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,double,string,string,string #default,_result,,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 ,,1,rfc3339time1,rfc3339time2,2021-07-12T19:38:000000000Z,1,field2,measurement1,tagvalue1 #group,false,false,true,true,false,false,true,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,long,string,string,string #default,_result,,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 ,,2,2021-01-18T20:59:37Z,2021-08-17T19:59:37.097Z,2021-07-12T19:38:000000000Z,1,field1,measurement1,tagvalue1 #group,false,false,true,true,false,false,true,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,string,string,string,string #default,_result,,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 ,,3,rfc3339time1,rfc3339time2,2021-07-12T19:38:000000000Z,a,field3,measurement1,tagvalue1 #group,false,false,true,true,false,false,true,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,long,string,string,string #default,_result,,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 ,,4,rfc3339time1,rfc3339time2,2021-07-12T19:39:000000000Z,2,field1,measurement1,tagvalue2 #group,false,false,true,true,false,false,true,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,double,string,string,string #default,_result,,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 ,,5,rfc3339time1,rfc3339time2,2021-07-12T19:39:000000000Z,2,field2,measurement1,tagvalue2 . Notice how the resulting annotated CSV contains 6 tables in the output. This is evident by the row separation and also by the value of the table column in the last stream of the table which is equal to 5 (remember annotated CSV counts the table results from 0). Group keys have been added to the data to produce these tables so that each table represents a series by default. Remember a column is part of a group key if all of the values in that column are identical within a single table. For example, the time column is assigned a #group annotation of ​​false. Setting the #group annotation to ​​false allows the different timestamps of points across a single series to be included in the same table. Conversely, the _measurement column is assigned a #group annotation of ​​true. The _measurement column is assigned a #group annotation of ​​true. Setting the #group annotation to ​​true enforces that all of the records in that table have the same measurement value. Remember, a series is identified by a unique combination of measurements, tag sets, and fields. If a table is to represent a single series, the table must contain records with the same measurement, tag sets, and fields across all of the rows. Important note: You can use Flux to manipulate the group keys and the resulting number of tables in the the output Annotated CSV table stream. We’ll learn about how to do this in later chapters. For now, let’s focus on understanding how line protocol results in different series. We can extend the example by adding an additional tag, but in this case, note that there is only a single tag value for tag2: . measurement1,tag1=\"tagvalue1\",tag2=\"tagvalue3\" field1=1i,field2=1,field3=\"a\" 1626118680000000000 measurement1,tag1=\"tagvalue2\",tag2=\"tagvalue3\" field1=2i,field2=2,field3=\"b\" 1626118740000000000 . Again, each series is identified by their unique tag keys, tag values, and field key combinations. Because a series is defined in part by a unique set of tag values, in this case, the introduction of tag2 does not change the table count in the underlying data model. When the introduction of a tag does not change the table count in the underlying data model, the tag is referred to as a dependent tag: . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue1 | tagvalue2 | field1 | 1i | 2021-07-12T19:38:00.000Z | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue1 | tagvalue2 | field2 | 1 | 2021-07-12T19:38:00.000Z | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue1 | tagvalue2 | field3 | a | 2021-07-12T19:38:00.000Z | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue2 | tagvalue2 | field1 | 2i | 2021-07-12T19:39:00.000Z | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue2 | tagvalue2 | field2 | 2 | 2021-07-12T19:39:00.000Z | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue2 | tagvalue2 | field3 | b | 2021-07-12T19:39:00.000Z | . To demonstrate the impact of combinations of tag values on the creation of time series, here are three lines of line protocol, but with only one field: . measurement1,tag1=\"tagvalue1\",tag2=\"tagvalue4\" field1=1i 1626118620000000000 measurement1,tag1=\"tagvalue2\",tag2=\"tagvalue5\" field1=2i 1626118680000000000 measurement1,tag1=\"tagvalue3\",tag2=\"tagvalue6\" field1=3i 1626118740000000000 . In those 3 lines there are 3 unique combinations of tag values and the single field, so, despite the presence of six total tag values, there are only 3 series created: . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue1 | tagvalue4 | field1 | 1i | 2021-07-12T19:37:00.000Z | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue2 | tagvalue5 | field1 | 2i | 2021-07-12T19:38:00.000Z | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue3 | tagvalue6 | field1 | 3i | 2021-07-12T19:39:00.000Z | . In this example, there are only 2 unique combinations of tag values: . measurement1,tag1=\"tagvalue1\",tag2=\"tagvalue4\" field1=1i 1626118620000000000 measurement1,tag1=\"tagvalue1\",tag2=\"tagvalue4\" field1=2i 1626118680000000000 measurement1,tag1=\"tagvalue2\",tag2=\"tagvalue4\" field1=3i 1626118740000000000 . As a result, the first series contains two points because those two points have the same combination of field name and tag values, whereas the third point has a different set of tag values. | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue1 | tagvalue2 | field1 | 1i | 2021-07-12T19:37:00.000Z | . | measurement1 | tagvalue1 | tagvalue2 | field1 | 2i | 2021-07-12T19:38:00.000Z | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue3 | tagvalue4 | field1 | 3i | 2021-07-12T19:39:00.000Z | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/input-format-vs-output-format/#from-series-to-tables-on-disk",
    "relUrl": "/docs/part-2/input-format-vs-output-format/#from-series-to-tables-on-disk"
  },"26": {
    "doc": "Input Format vs Output Format",
    "title": "Real World Data",
    "content": "There is an excellent repository of semi-live line protocol data maintained by InfluxData. This is generally intended as sample data to help you get started exploring InfluxDB. Currently, 4 datasets are kept up to date there: . | Air Sensor Data: This dataset includes a single tag, which is an id for the specific air quality sensor that is reporting 3 fields, temperature, humidity, and carbon monoxide levels. | Bird Migration Data: This is geo-spatial dataset represents migratory movements of birds. It is tagged to aid geo-spacial querying. | NOAA National Buoy Center Data: This dataset provides the latest observations from the NOAA NDBC network of buoys. It contains a large number of tags and fields. | USGS Earthquake Data. The United States Geological Survey (USGS) earthquake dataset contains seismic activity data. This is a very large dataset, and contains even more tags and fields. | . While you can simply copy the Flux from any of the real world sample datasets into the Script Editor in the Data Explorer and visualize the data. I recommend creating a bucket and using the to() function to write the data to that bucket, as described in Write and Query Sample Data in Part 1. Writing the data to a bucket in InfluxDB allows you to use Flux to explore the schema of your dataset. Important Note: You might hit your series cardinality limit for Free Tier accounts if you write the larger datasets to InfluxDB. I recommend just writing the Air Sensor Data if you’re using the Free Tier account. Important Note: This section recommends that you use the InfluxDB UI to write data to InfluxDB only because writing data with other tools hasn’t been covered yet. We recommend writing data with the CLI or VS Code. If you prefer developing in with those tools look at the . Exploring the Real Word Data Schema with Flux . Let us turn our attention to 2 real world data sets: Air Sensor Data and the NOAA National Buoy Center Data. We’ll use Flux to get an understanding of our schema. Then we’ll run through some exercises to ensure our understanding of the relationship between the line protocol input format and the InfluxDB data model as persisted on disk by the storage engine. We’ll start by focusing on the Air sensor dataset, as it is the simplest dataset. The a Air sensor dataset contains: . | 1 measurement: airSensors | 3 fields: . | co | humidity | temperature | . | 1 tag: sensor_id . | 8 sensor_id tag values | . | . As you can see, the fields are the actual data, in this case all of type float, where the tag is metadata, defining which sensor produced the data. Explore your data schema with Flux to obtain the number of measurements, tag keys, tag values, and field keys in your data by using the schema package. To get the number of fields in the airSensors measurement from the Air sensor sample dataset, run the following Flux query in your preferred tool (CLI, VS Code, InfluxDB UI): . import \"influxdata/influxdb/schema\" schema.measurementFieldKeys( bucket: \"Air sensor sample dataset\", measurement: \"airSensors\" ) |&gt; count() . To get the number of tag keys in the airSensors measurement from the Air sensor sample dataset, run the following Flux query in your preferred tool (CLI, VS Code, InfluxDB UI): . import \"influxdata/influxdb/schema\" schema.measurementTagKeys( bucket: \"Air sensor sample dataset\", measurement: \"airSensors\" ) |&gt; count() . To get the number of tag values for the sensor_id tag key in the airSensors measurement from the Air sensor sample dataset, use the following Flux query (CLI, VS Code, InfluxDB UI): . import \"influxdata/influxdb/schema\" schema.measurementTagValues( bucket: \"Air sensor sample dataset\", tag: \"sensor_id\", measurement: \"example-measurement\" ) |&gt; count() . We can repeat the same approach for the NOAA National Buoy Center Data. We find that the NOAA National Buoy Center Data has the following schema: . | 1 measurement: ndbc | 21 fields: . | air_temp_degc | avg _wave_period_sec | dewpoint_temp_degc | dominate_wave _period_sec | gust_speed_mps | lat | lon | pressure_temdancy_hpa | sea_level_pressure_hpa | sea_surface_temp_degc | significant_Wave_height_m | station_currents | station_dart | station_elev | sation_met | station_visibility_mei | station_waterquality | water_level_ft | wave_dir_degt | wind_dir_degt | wind_spead_mps | . | 5 tag keys: . | station_id . | 113 station_id tag values | . | station_name . | 828 | . | station_owner . | 57 station_owner tag values | . | station_pgm . | 6 station_pgm tag values | . | station_type . | 4 station_type tag values | . | . | . Again, note that the 21 fields are all the different kinds of data that might be collected by a weather station, whereas the 5 tags contain metadata about which stations collected it. Exercises with Real World Data . Now that we understand the schema of the 2 real world data sets, Air Sensor Data and the NOAA National Buoy Center Data, try to answer the following questions to test your understanding of schema design, line protocol, and . Question 1: How many series will be created by the Air Sensor Data given the schema above? . Answer 1: (1 sensor_id tag x 8 unique tag values) x (3 fields) = 4 x 3 = 24 . Question 2: How many series will be created by the NOAA National Buoy Center Data given the schema above (assuming no tags are dependent tags)? . Answer 2: (1 station_id tag x 113 unique tag values) x (1 station_name tag x 828 unique tag values) x (1 station_name tag x 828 unique tag values) x (1 station_owner tag x 57 unique tag values) x (1 station_pgm tag x 6 unique tag values) x (1 station_type tag x 47 unique tag values) x (21 fields) = 113 x 828 x 56 x 6 x 47 x 21 = 31028816448 . Question 3: How would the following line protocol form the Air sensor sample dataset be organized into tables on disk? And how many points are in each series? . airSensors,sensor_id=TLM0100 temperature=71.17615703642676,humidity=35.12940716174776,co=0.5024058630839136 1626537623000000000 airSensors,sensor_id=TLM0101 temperature=71.80350992863588,humidity=34.864121891949736,co=0.4925449578765155 1626537623000000000 airSensors,sensor_id=TLM0102 temperature=72.02673296407973,humidity=34.91147650009415,co=0.4941631223400505 1626537623000000000 airSensors,sensor_id=TLM0103 temperature=71.34822444566278,humidity=35.19576623496297,co=0.4046734235304059 1626537623000000000 airSensors,sensor_id=TLM0200 temperature=73.57230556533555,humidity=35.77102288427073,co=0.5317633226995193 1626537623000000000 airSensors,sensor_id=TLM0201 temperature=c,humidity=35.17327249047271,co=0.5000439017037601 1626537623000000000 airSensors,sensor_id=TLM0202 temperature=75.28582430811852,humidity=35.668729783597556,co=0.48071553398947864 1626537623000000000 airSensors,sensor_id=TLM0203 temperature=74.75927935923579,humidity=35.89268792033798,co=0.4089308476612381 1626537623000000000 airSensors,sensor_id=TLM0100 temperature=71.2194835668512,humidity=35.12891266051405,co=0.4958773037139102 1626537633000000000 airSensors,sensor_id=TLM0101 temperature=71.78232293801005,humidity=34.88621453634278,co=0.5074032895942003 1626537633000000000 airSensors,sensor_id=TLM0102 temperature=72.07101160147653,humidity=34.938529830668536,co=0.5102716855442547 1626537633000000000 airSensors,sensor_id=TLM0103 temperature=71.32889101333731,humidity=35.21581883021604,co=0.4245915521103036 1626537633000000000 airSensors,sensor_id=TLM0200 temperature=73.55081075397399,humidity=35.74330537831752,co=0.5435288991742965 1626537633000000000 airSensors,sensor_id=TLM0201 temperature=74.06284877512215,humidity=35.17611147751894,co=0.4813785832360323 1626537633000000000 airSensors,sensor_id=TLM0202 temperature=75.29425020175684,humidity=35.64366062740866,co=0.4911462705616819 1626537633000000000 airSensors,sensor_id=TLM0203 temperature=74.77142594525142,humidity=35.941017361190255,co=0.42797647488504065 1626537633000000000 . Answer 3: . The line protocol would result in the following series and tables on disk. Each series contains two points from the line protocol above. | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0100 | temperature | 73.57230556533555 | rfc3339time1 | . | airSensors | TLM0100 | temperature | 71.2194835668512 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0101 | temperature | 72.02673296407973 | rfc3339time1 | . | airSensors | TLM0101 | temperature | 71.78232293801005 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0102 | temperature | 73.57230556533555 | rfc3339time1 | . | airSensors | TLM0102 | temperature | 72.07101160147653 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0103 | temperature | 71.34822444566278 | rfc3339time1 | . | airSensors | TLM0103 | temperature | 71.32889101333731 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0200 | temperature | 73.57230556533555 | rfc3339time1 | . | airSensors | TLM0200 | temperature | 73.55081075397399 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0201 | temperature | 73.57230556521233 | rfc3339time1 | . | airSensors | TLM0201 | temperature | 74.06284877512215 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0202 | temperature | 75.28582430811852 | rfc3339time1 | . | airSensors | TLM0202 | temperature | 75.29425020175684 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0203 | temperature | 74.75927935923579 | rfc3339time1 | . | airSensors | TLM0203 | temperature | 74.77142594525142 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0100 | humidity | 35.12940716174776 | rfc3339time1 | . | airSensors | TLM0100 | humidity | 35.12891266051405 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0101 | humidity | 34.864121891949736 | rfc3339time1 | . | airSensors | TLM0101 | humidity | 34.88621453634278 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0102 | humidity | 34.91147650009415 | rfc3339time1 | . | airSensors | TLM0102 | humidity | 34.938529830668536 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0103 | humidity | 35.19576623496297 | rfc3339time1 | . | airSensors | TLM0103 | humidity | 35.21581883021604 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0200 | humidity | 35.77102288427073 | rfc3339time1 | . | airSensors | TLM0200 | humidity | 35.74330537831752 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0201 | humidity | 35.17327249047271 | rfc3339time1 | . | airSensors | TLM0201 | humidity | 35.17611147751894 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0202 | humidity | 35.668729783597556 | rfc3339time1 | . | airSensors | TLM0202 | humidity | 35.64366062740866 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0203 | humidity | 35.89268792033798 | rfc3339time1 | . | airSensors | TLM0203 | humidity | 35.941017361190255 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0100 | co | 0.4925449578765155 | rfc3339time1 | . | airSensors | TLM0100 | co | 0.495877303713910 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0101 | co | 0.4925449578765155 | rfc3339time1 | . | airSensors | TLM0101 | co | 0.5074032895942003 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0102 | co | 0.4941631223400505 | rfc3339time1 | . | airSensors | TLM0102 | co | 0.5102716855442547 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0103 | co | 0.4046734235304059 | rfc3339time1 | . | airSensors | TLM0103 | co | 0.4245915521103036 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0200 | co | 0.5317633226995193 | rfc3339time1 | . | airSensors | TLM0200 | co | 0.5435288991742965 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0201 | co | 0.5000439017037601 | rfc3339time1 | . | airSensors | TLM0201 | co | 0.4813785832360323 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0202 | co | 0.48071553398947864 | rfc3339time1 | . | airSensors | TLM0202 | co | 0.4911462705616819 | rfc3339time2 | . | _measurement | sensor_id | _field | _value | _time | . | airSensors | TLM0203 | co | 0.4089308476612381 | rfc3339time1 | . | airSensors | TLM0203 | co | 0.42797647488504065 | rfc3339time2 | . Next Section . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/input-format-vs-output-format/#real-world-data",
    "relUrl": "/docs/part-2/input-format-vs-output-format/#real-world-data"
  },"27": {
    "doc": "Introduction to Flux",
    "title": "Introduction to Flux",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/introduction-to-flux/",
    "relUrl": "/docs/part-2/introduction-to-flux/"
  },"28": {
    "doc": "Introduction to Flux",
    "title": "Table of contents",
    "content": ". | Basic Flux Concepts . | Flux is a Functional Language | Declarative | Flux Is Strongly And Statically Typed | Flux Objects Are Immutable | Flux Parameters Are Named | Flux Parameter Types Can Be Overloaded | Pipe Forwarding | Flux Operates on Streams of Tables | Flux Supports Only Very Limited Looping | Dot vs. Bracket Notation | Packages . | Built In Functions | Imports | . | . | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/introduction-to-flux/#table-of-contents",
    "relUrl": "/docs/part-2/introduction-to-flux/#table-of-contents"
  },"29": {
    "doc": "Introduction to Flux",
    "title": "Basic Flux Concepts",
    "content": "Flux is the native language of InfluxDB 2.0. Flux is used for: . | Writing queries to retrieve data. | Transforming and shaping data as needed. | Integrating with other data sources. | . Flux is a functional language designed specifically to work with the InfluxDB data format. To get the most power out of Flux, it is very useful to understand the underlying InfluxDB data model and how it relates to your schema, so make sure to read and understand the section on designing your schema above. In the previous section, you were exposed to “just enough” Flux, but there are some important flux concepts that are useful to understand. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/introduction-to-flux/#basic-flux-concepts",
    "relUrl": "/docs/part-2/introduction-to-flux/#basic-flux-concepts"
  },"30": {
    "doc": "Introduction to Flux",
    "title": "Flux is a Functional Language",
    "content": "Flux is a functional language from the ground up. Most Flux code that you write is essentially creating and linking together functions. Functions can be explicitly named, or as often as not, anonymous, meaning you declare the function inline without naming it. Earlier, we were introduced to a filter. |&gt; filter(fn: (r) =&gt; r._measurement == \"measurement1\") . filter() is a function that itself takes a function as an argument. However, most commonly, the function for a filter is defined inline and supplied anonymously. Alternatively you could name the filter and use the filter name: . afilter_function = (r) =&gt; r._measurement == \"measurement1\" from(bucket: \"bucket1\") |&gt; filter(fn: afilter_function) . We can zero in a bit on the function my_filter_function and tease apart the components a bit more. Because Flux is a functional language, functions are first class objects. So the first step is to specify the identifier (“my_filter_function”) and the assignment operator (“=”). The next part is the parameter list, in this case simple “r”, for row. Followed by the lambda operator(“ =&gt;”). Finally, the function body itself. afilter_function = (r) =&gt; r._measurement == \"measurement1\" . A function body can be multiline, which is handled syntactically as such: . afilter_function = (r) =&gt; r._measurement == \"measurement1\" and r._field == \"field1\" . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/introduction-to-flux/#flux-is-a-functional-language",
    "relUrl": "/docs/part-2/introduction-to-flux/#flux-is-a-functional-language"
  },"31": {
    "doc": "Introduction to Flux",
    "title": "Declarative",
    "content": "Flux is a declarative language. This means that your Flux is executed to accomplish the expressed goal in the Flux code, and is not necessarily done in the specific manner that you specify. This boils down to the Flux execution engine applying a planner to optimize the order of operations that you have specified to achieve better performance. You will always get the results that you are asking for, but Flux may achieve them slightly differently than the specific manner in which you specified. This will be covered in more depth in the section on optimization Flux. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/introduction-to-flux/#declarative",
    "relUrl": "/docs/part-2/introduction-to-flux/#declarative"
  },"32": {
    "doc": "Introduction to Flux",
    "title": "Flux Is Strongly And Statically Typed",
    "content": "The Flux language is strongly typed. However, the typing is implicit. While you do not need to declare the types of your objects, the types are inferred when your program is run, and type mismatches cause errors. Additionally, you cannot change the type of an object in Flux at run time. The type of an object is immutable. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/introduction-to-flux/#flux-is-strongly-and-statically-typed",
    "relUrl": "/docs/part-2/introduction-to-flux/#flux-is-strongly-and-statically-typed"
  },"33": {
    "doc": "Introduction to Flux",
    "title": "Flux Objects Are Immutable",
    "content": "The value of an object is also immutable. For example: . astring = \"hi\" astring = \"bye\" . Will result in an error: . @2:1-2:18: variable \"astring\" reassigned . Note that data in tables is NOT immutable. This will be covered in depth in the section on transforming data. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/introduction-to-flux/#flux-objects-are-immutable",
    "relUrl": "/docs/part-2/introduction-to-flux/#flux-objects-are-immutable"
  },"34": {
    "doc": "Introduction to Flux",
    "title": "Flux Parameters Are Named",
    "content": "With the exception of tables passed through the pipe forward operator, all Flux parameters are named. This makes your Flux code somewhat more self documenting, but also allows for more non-breaking changes to the Flux language. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/introduction-to-flux/#flux-parameters-are-named",
    "relUrl": "/docs/part-2/introduction-to-flux/#flux-parameters-are-named"
  },"35": {
    "doc": "Introduction to Flux",
    "title": "Flux Parameter Types Can Be Overloaded",
    "content": "In many cases, a single parameter can accept arguments of multiple types. This is covered in detail in the case of the range() section in the next section. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/introduction-to-flux/#flux-parameter-types-can-be-overloaded",
    "relUrl": "/docs/part-2/introduction-to-flux/#flux-parameter-types-can-be-overloaded"
  },"36": {
    "doc": "Introduction to Flux",
    "title": "Pipe Forwarding",
    "content": "Flux is designed to transform data by piping the data between functions, each function transforming the data in turn. The operator for this is the pipe forward operator “|&gt;”. So you can see the following code starts with a from(), and then pipe forwards the results of that from to range(). from(bucket: \"bucket1\") |&gt; range(start: -5m) . That can then be pipe forwarded into more functions, for example, filter(): . from(bucket: \"bucket1\") |&gt; range(start: -5m) |&gt; filter(fn: (r) =&gt; r._measurement == \"measurement1\") . As discussed above, a function body is defined with a set of parameters, the lambda operator, and the function operations. Additionally, when calling a function, all parameters are required named parameters. However, pipe forwarding has an implicit argument being passed between the functions, which is the stream of tables that was modified by the previous function. A function that can be to the right of the pipe forward operator declares this with a special designation in it’s parameter list, the “pipe receive literal.” . (tables=&lt;-) . So, a function body that accepts pipe forwarded data looks like this: . functionName = (tables=&lt;-) =&gt; tables |&gt; functionOperations . A practical example is the application of a set of common filters: . afilter_function = (tables=&lt;-) =&gt; filter(fn: (r) =&gt; r._measurement == \"measurement1\") filter(fn: (r) =&gt; r._field == \"field1\") filter(fn: (r) =&gt; r._value &gt; 80.0) from(bucket: \"bucketa\") |&gt; range(start: -5m) |&gt; afilter_function() . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/introduction-to-flux/#pipe-forwarding",
    "relUrl": "/docs/part-2/introduction-to-flux/#pipe-forwarding"
  },"37": {
    "doc": "Introduction to Flux",
    "title": "Flux Operates on Streams of Tables",
    "content": "The consequence of the pipe forward operator is that Flux functions operate on every row of every table applied to them. To review, as you write data to InfluxDB, it is written to the storage engine in separate tables, each with a unique combination of the measurement, tag value combination, and fields. When you query that data back, those tables are read and streamed to flux, and each row of each table is then passed through each function. You cannot ask Flux to operate on one table, but not others. Nor can you ask Flux to operate on one row, but not others. Every row in every table will undergo the same transformations. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/introduction-to-flux/#flux-operates-on-streams-of-tables",
    "relUrl": "/docs/part-2/introduction-to-flux/#flux-operates-on-streams-of-tables"
  },"38": {
    "doc": "Introduction to Flux",
    "title": "Flux Supports Only Very Limited Looping",
    "content": "Flux more or less does not allow looping. The closest thing to a loop capability is the map() function which applies a function to each record. In Flux, many of the things that you expect to accomplish in a loop, you accomplish rather by using successive functions that apply the transformations to the data. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/introduction-to-flux/#flux-supports-only-very-limited-looping",
    "relUrl": "/docs/part-2/introduction-to-flux/#flux-supports-only-very-limited-looping"
  },"39": {
    "doc": "Introduction to Flux",
    "title": "Dot vs. Bracket Notation",
    "content": "Because Flux takes inspiration from javascript, Flux supports both dot notation and bracket notation to access members. The following 2 lines are equivalent. filter(fn: (r) =&gt; r._measurement == \"measurement1\") filter(fn: (r) =&gt; r[\"_measurement\"] == \"measurement1\") . There is no official convention to determine which to use, and both are equally valid. However, generated code often uses the bracket notation because if the generated code is based on user supplied data, there is a chance that the data itself has characters that can cause syntax errors. For example, if I generate a tag that includes a period, such as sensor.001, generated code that used dot notation will be broken: . filter(fn: (r) =&gt; r.sensor.001 == \"astring\") vs. filter(fn: (r) =&gt; r[\"sensor.001\"] == \"astring\") . The first line would be a syntax error, so code generators typically would produce the second one. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/introduction-to-flux/#dot-vs-bracket-notation",
    "relUrl": "/docs/part-2/introduction-to-flux/#dot-vs-bracket-notation"
  },"40": {
    "doc": "Introduction to Flux",
    "title": "Packages",
    "content": "Flux comes with a large library of functions. These functions are organized into packages, collectively under the stdlib. Built In Functions . The stdlib comes with a set of “built-in functions,” meaning that you can use them without importance. These are the most commonly used functions, and all the functions introduced so far (from(), range(), and filter()) are built-in. Imports . Other packages are designed for use with more specific domains, and must be imported in order to be used. So, for example, if you wanted to work with CSV within your flux function, you would need to import the csv package, like so: . import \"csv\" . After which, functions in the csv package can be accessed with dot notation: . csv.from(csv: csvData, mode: \"raw\") . Bracket notation is also supported, which you may see occasionally: . csv[\"from\"](csv: csvData, mode: \"raw\") . Here’s a minimal example of importing the csv package and calling a function from it: . import \"csv\" csvData = \"value,key\\n1,a\" csv.from(csv: csvData, mode: \"raw\") |&gt; yield() . Next Section . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/introduction-to-flux/#packages",
    "relUrl": "/docs/part-2/introduction-to-flux/#packages"
  },"41": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "Introduction to InfluxDB Tools",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/"
  },"42": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "Table of contents",
    "content": ". | Flux and the Task Engine | InfluxDB User Interface | Telegraf | Awesome CLI | Rest API | Client Libraries | VSCode Plugin | Stacks and Templates | . To honor our commitment to developer happiness, InfluxData offers a wide range of tools working with time series data easily for all types of developers. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#table-of-contents",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#table-of-contents"
  },"43": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "Flux and the Task Engine",
    "content": "Flux is a functional query and scripting language. Flux enables you to: . | Transform and analyze data. | Write Tasks | . Flux has a Javascript inspired syntax that makes it easily composable and readable. Pipe-forward operators separate out functions and make data transformations flow smoothly. Flux is also testable, shareable, and contributable. The Task Engine executes Flux scripts on a schedule. It allows you to: . | Process data to make visualizations screaming fast | Get alerted if your data stops writing or breaches certain thresholds | Periodically call an external service with data from InfluxDB | . The Task engine can tackle all the points above with no additional code or operations on your part. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#flux-and-the-task-engine",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#flux-and-the-task-engine"
  },"44": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "InfluxDB User Interface",
    "content": "The InfluxDB UI provides a complete user interface for working with time series data and InfluxDB. The InfluxDB UI enables you to: . | Build queries to visualize your time series data. You can select from a wide variety of visualization types. The InfluxDB UI also supports geotemporal map visualizations. | Edit Flux code in the Flux Script Editor. | Build dashboards and notebooks. | Manage multiple users in your Organization. | Build and manage tasks. Tasks are Flux scripts that run on a schedule. | Build checks and notifications. Checks and notifications are specialized types of tasks which enable alert creation. | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#influxdb-user-interface",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#influxdb-user-interface"
  },"45": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "Telegraf",
    "content": "Have more stringent requirements for your writes? Need batching, retries, and other features? Don’t write this code yourself, just use Telegraf. Telegraf is InfluxData’s plugin driven collection agent for metric and events. With over 200 input plugins, Telegraf probably has an input plugin that fits your needs. But even if there isn’t a plugin for your exact use case, you can use telegraf to easily reformat any data type into your preferred output format, be it line protocol, json, csv, etc…. Telegraf isn’t just a tool for writing data to a destination data store. Telegraf processor plugins and aggregator plugins enable you to do so much more with your data than sophisticated collection and writes. For example, you can easily add data transformations with Starlark, and you can even use the execd processor plugin which makes Telegraf extensible in any language. It’s no wonder that Telegraf has close to 11,000 stars on github. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#telegraf",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#telegraf"
  },"46": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "Awesome CLI",
    "content": "The InfluxDB CLI allows users to interact with InfluxDB with ease. Features like configuration profiles and environment variable support make scripting and controlling InfluxDB a breeze. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#awesome-cli",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#awesome-cli"
  },"47": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "Rest API",
    "content": "All of InfluxDB is wrapped in a REST API. This API is highly compatible between OSS and Cloud. The API allows you to write data and query data, of course, but also has everything you need to manage the database, including creating resources like authentication tokens, buckets, users, etc… . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#rest-api",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#rest-api"
  },"48": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "Client Libraries",
    "content": "You aren’t limited to making REST calls on your own. If you prefer, InfluxDB has client libraries written in 13 languages. These Client Libraries enable you to drop InfluxDB functionality into an existing code base with ease or create a new code base. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#client-libraries",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#client-libraries"
  },"49": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "VSCode Plugin",
    "content": "If you are a Visual Studio (VS) Code user, then it’s easy to add InfluxDB to your workflow using Flux VS Code Extension. Using the Flux VS Code Extension to write Flux and run the query. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#vscode-plugin",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#vscode-plugin"
  },"50": {
    "doc": "Introduction to InfluxDB Tools",
    "title": "Stacks and Templates",
    "content": "Want to back up or move your whole InfluxDB configuration? Add it to github so you can restore from a backup? Even use a gitops workflow to integrate with your existing CD (continuous deployment) process? InfluxDB supports all of this with our Stacks and Templates features. A Template is a prepackaged InfluxDB configuration that contains multiple InfluxDB resources. Templates include everything from Telegraf configuration, to Dashboards, to Alerts, and more. Use Templates to: . | get up and running with InfluxDB for a variety of common use cases with a Community Template,a community-contributed Templates. | quickly get setup with a new InfluxDB instance | backup your Dashboard, Alert, and Task configurations. | . Applying an existing Community Template is as easy as copy and pasting a URL for the Template yaml in the UI: . A Stack is a stateful InfluxDB template that lets you add, update, and remove template resources. Next Section . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb-tools/#stacks-and-templates",
    "relUrl": "/docs/part-1/introduction-to-influxdb-tools/#stacks-and-templates"
  },"51": {
    "doc": "Introduction to InfluxDB",
    "title": "Introduction to InfluxDB",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb/",
    "relUrl": "/docs/part-1/introduction-to-influxdb/"
  },"52": {
    "doc": "Introduction to InfluxDB",
    "title": "Table of contents",
    "content": ". | What is InfluxDB | The InfluxDB Advantage | Time to Awesome | Write and Query sample data | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb/#table-of-contents",
    "relUrl": "/docs/part-1/introduction-to-influxdb/#table-of-contents"
  },"53": {
    "doc": "Introduction to InfluxDB",
    "title": "What is InfluxDB",
    "content": "InfluxData is the company behind InfluxDB and Telegraf. InfluxDB, released in 2013, is the best time series database available for storing metrics and sensing data. It has since evolved into a full blown time series application development platform used by thousands of developers to create customer facing IoT, server monitoring, financial applications, bespoke monitoring applications for thousands of servers and devices, and many many other applications. Take a look at various case studies and customer testimonials from IBM, Adobe, Hulu, Cisco and more. InfluxDB is more than the leading time series database. InfluxDB also includes the InfluxDB User Interface (InfluxDB UI) and Flux. The InfluxDB UI is a time series management, visualization, and dashboarding tool. It also offers a script editor for Flux. Flux is a functional scripting and query language that enables data processing tasks like sophisticated data transformation and alerting. Telegraf is the open source server agent for collecting metrics and events. Telegraf is plugin driven and compiles into a single binary. There is a huge collection of input, output, aggregator, and parser plugins that enable developers to collect data, apply transformations to it, and write it to the destination datastore of their choice. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb/#what-is-influxdb",
    "relUrl": "/docs/part-1/introduction-to-influxdb/#what-is-influxdb"
  },"54": {
    "doc": "Introduction to InfluxDB",
    "title": "The InfluxDB Advantage",
    "content": "Paul Dix, founder and CTO of InfluxData, frames decision making around the concept of minimizing “time to awesome” or the hurdle to adoption and value. The developer experience is our priority at InfluxData. If you are thinking about creating an application related to time stamped data (IoT, Sensor Monitoring, Server Monitoring, Finance, etc…), InfluxDB is the easiest and most powerful development platform for you. This book will show you why that’s true. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb/#the-influxdb-advantage",
    "relUrl": "/docs/part-1/introduction-to-influxdb/#the-influxdb-advantage"
  },"55": {
    "doc": "Introduction to InfluxDB",
    "title": "Time to Awesome",
    "content": "You can choose between downloading and running the Open Source (OSS) version or creating a free account in InfluxDB Cloud and letting us run it for you. The OSS version comes as a single binary, so you can just download it and run it. There are also packages for popular Linux distributions. You can also sign up for a free cloud account which is just as easy as “click, click, authenticate your email, and get going”. Either way, getting up and running with InfluxDB literally takes a couple of minutes. Finally, when it comes to choosing between the OSS version of InfluxDB cloud, either choice is the right choice because we put a lot of effort into keeping the API’s consistent and compatible across our two offerings. So if you need, you can always switch over to the other later. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb/#time-to-awesome",
    "relUrl": "/docs/part-1/introduction-to-influxdb/#time-to-awesome"
  },"56": {
    "doc": "Introduction to InfluxDB",
    "title": "Write and Query sample data",
    "content": "The fastest way to write data into InfluxDB is to write some sample data with the script editor in the InfluxDB UI. Writing a sample dataset is a great way to get some meaningful data into the platform to get a feel for InfluxDB. You can pick whichever sample dataset you want to use, but we’ll use the NOAA ​​water sample data in this section. After setting up InfluxDB, navigate the Explorer page and click the + Create Bucket button. Name your bucket “noaa”. A bucket is a named location where you store your data in InfluxDB. Now navigate to the Script Editor and copy and paste the following Flux code from the documentation. You don’t have to understand this code right now, the only part to pay attention to is the to() function in the last line. import \"experimental/csv\" relativeToNow = (tables=&lt;-) =&gt; tables |&gt; elapsed() |&gt; sort(columns: [\"_time\"], desc: true) |&gt; cumulativeSum(columns: [\"elapsed\"]) |&gt; map(fn: (r) =&gt; ({ r with _time: time(v: int(v: now()) - (r.elapsed * 1000000000))})) csv.from(url: \"https://influx-testdata.s3.amazonaws.com/noaa.csv\") |&gt; relativeToNow() |&gt; to(bucket: \"noaa\", org: \"example-org\") . Make sure to change the following parameters in the to() function: . | The bucket you want to write the NOAA sample dataset to (if you created a “noaa” bucket already, then ignore this step). | The org to the email you used to register for a Cloud account or set up your OSS instance. | . Finally, hit** Submit**. Easily query your data through the UI by using the Query Builder. Simply select for the data you want to visualize and hit Submit. Next Section . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/introduction-to-influxdb/#write-and-query-sample-data",
    "relUrl": "/docs/part-1/introduction-to-influxdb/#write-and-query-sample-data"
  },"57": {
    "doc": "Optimizing Flux Performance",
    "title": "Optimizing Flux Performance",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/optimizing-flux-performance/",
    "relUrl": "/docs/part-2/optimizing-flux-performance/"
  },"58": {
    "doc": "Optimizing Flux Performance",
    "title": "Table of contents",
    "content": ". | Optimizing Flux Performance . | General recommendations for Flux performance optimization | Taking advantage of pushdown patterns | Using schema mutations properly | Using variables to avoid querying data multiple times | Dividing processing work across multiple tasks | The Flux Profiler package | Using the Flux extension for Visual Studio Code to streamline Flux optimization discovery | Other tips | Best practices for receiving help | . | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/optimizing-flux-performance/#table-of-contents",
    "relUrl": "/docs/part-2/optimizing-flux-performance/#table-of-contents"
  },"59": {
    "doc": "Optimizing Flux Performance",
    "title": "Optimizing Flux Performance",
    "content": "So you’re using InfluxDB Cloud and you’re taking full advantage of Flux to create custom data processing tasks, checks, and notifications. However, you notice that some of your Flux scripts aren’t executing as quickly as you expect. In this section, we’ll learn about best practices and tools for optimizing Flux performance. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/optimizing-flux-performance/",
    "relUrl": "/docs/part-2/optimizing-flux-performance/"
  },"60": {
    "doc": "Optimizing Flux Performance",
    "title": "General recommendations for Flux performance optimization",
    "content": "Before diving into some of the tools you can use to optimize Flux performance, let’s dive into some general recommendations for Flux performance optimization: . | Take advantage of pushdown patterns. | Schema mutation functions should be applied at the end of your query. | Use variables to avoid querying data multiple times. | Divide processing work across multiple tasks when needed. | . We’ll discuss each of these recommendations in detail in the following sections. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/optimizing-flux-performance/#general-recommendations-for-flux-performance-optimization",
    "relUrl": "/docs/part-2/optimizing-flux-performance/#general-recommendations-for-flux-performance-optimization"
  },"61": {
    "doc": "Optimizing Flux Performance",
    "title": "Taking advantage of pushdown patterns",
    "content": "In order to provide context for the optimization guidelines, let’s first take a moment to understand how Flux works. Flux is able to query data efficiently because some functions push down the data transformation workload to storage rather than performing the transformations in memory. Combinations of functions that do this work are called pushdown patterns. It’s best to try and use pushdown patterns whenever you can optimize your Flux query. To learn more about pushdown patterns and how Flux works, please read “Solution 2: Learning about memory optimizations and new pushdown patterns to optimize your Flux scripts” from Top 5 Hurdles for Intermediate Flux Users and Resources for Optimizing Flux. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/optimizing-flux-performance/#taking-advantage-of-pushdown-patterns",
    "relUrl": "/docs/part-2/optimizing-flux-performance/#taking-advantage-of-pushdown-patterns"
  },"62": {
    "doc": "Optimizing Flux Performance",
    "title": "Using schema mutations properly",
    "content": "Schema mutation functions are any functions that change the columns in your Flux tables. They include functions like keep(), drop(), rename(), duplicate(), and set(). If you’re using an aggregates or selector function in your query, try to include the schema mutation functions after applying aggregation functions to preserve any pushdown patterns that you might have. Additionally, try replacing keep() or drop() with changes to the group key whenever possible. For example, when executing a join() across two fields from two buckets, join on all the like-columns instead of dropping the columns afterwards. We generate the data for this example with the array.from() function: . import \"array\" import \"experimental\" start = experimental.subDuration( d: -10m, from: now(), ) bucket1 = array.from(rows: [{_start: start, _stop: now(), _time: now(),_measurement: \"mymeas\", _field: \"myfield\", _value: \"foo1\"}]) |&gt; yield(name: \"bucket1\") bucket2 = array.from(rows: [{_start: start, _stop: now(), _time: now(),_measurement: \"mymeas\", _field: \"myfield\", _value: \"foo2\"}]) |&gt; yield(name: \"bucket2\") . The annotated CSV output from our query looks like this: . Don’t use drop() unnecessarily after a join(): . join(tables: {bucket1: bucket1, bucket2: bucket2}, on: [\"_time\"], method: \"inner\") |&gt; drop(columns:[\"_start_field1\", \"_stop_field1\", \"_measurement_field1\", \"myfield1\"]) |&gt; yield(name: \"bad_join\") . Do replace with changes to the group key by joining on like-columns: . join(tables: {bucket1: bucket1, bucket2: bucket2}, on: [\"_start\",\"_stop\"\"_time\", \"_measurement\",\"_field\"], method: \"inner\") |&gt; yield(name: \"good_join\") . To yield the same result: . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/optimizing-flux-performance/#using-schema-mutations-properly",
    "relUrl": "/docs/part-2/optimizing-flux-performance/#using-schema-mutations-properly"
  },"63": {
    "doc": "Optimizing Flux Performance",
    "title": "Using variables to avoid querying data multiple times",
    "content": "Rather than query data multiple times, store the result in a variable and reference it. In other words: . Don’t do this: . from(bucket: \"my-bucket\") |&gt; range(start: -1h) |&gt; filter(fn: (r) =&gt; r._measurement == \"my_measurement\") |&gt; mean() |&gt; set(key: \"agg_type\",value: \"mean_temp\") |&gt; to(bucket: \"downsampled\", org: \"my-org\", tagColumns:[\"agg_type\"]) from(bucket: \"my-bucket\") |&gt; range(start: -1h) |&gt; filter(fn: (r) =&gt; r._measurement == \"my_measurement\") |&gt; count() |&gt; set(key: \"agg_type\",value: \"count_temp\") |&gt; to(bucket: \"downsampled\", org: \"my-org\", tagColumns: [\"agg_type\"]) . Do this instead: . data = from(bucket: \"my-bucket\") |&gt; range(start: -1h) |&gt; filter(fn: (r) =&gt; r._measurement == \"my_measurement\") data |&gt; mean() |&gt; set(key: \"agg_type\",value: \"mean_temp\") |&gt; to(bucket: \"downsampled\", org: \"my-org\", tagColumns: [\"agg_type\"]) data |&gt; count() |&gt; set(key: \"agg_type\",value: \"count_temp\") |&gt; to(bucket: \"downsampled\", org: \"my-org\", tagColumns: [\"agg_type\"]) . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/optimizing-flux-performance/#using-variables-to-avoid-querying-data-multiple-times",
    "relUrl": "/docs/part-2/optimizing-flux-performance/#using-variables-to-avoid-querying-data-multiple-times"
  },"64": {
    "doc": "Optimizing Flux Performance",
    "title": "Dividing processing work across multiple tasks",
    "content": "Are you trying to perform schema mutations, pivots, joins, complex math, and mappings all in the same task? If you are and you’re experiencing a long execution time, consider splitting up some of this work across multiple tasks instead. Separating the processing work out and executing these tasks in parallel can help you reduce your overall execution time. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/optimizing-flux-performance/#dividing-processing-work-across-multiple-tasks",
    "relUrl": "/docs/part-2/optimizing-flux-performance/#dividing-processing-work-across-multiple-tasks"
  },"65": {
    "doc": "Optimizing Flux Performance",
    "title": "The Flux Profiler package",
    "content": "The Flux Profiler package provides performance information based on your query. As per the documentation, the following Flux query: . import \"profiler\" option profiler.enabledProfilers = [\"query\", \"operator\"] from(bucket: \"noaa\") |&gt; range(start: 2019-08-17T00:00:00Z, stop: 2019-08-17T00:30:00Z) |&gt; filter(fn: (r) =&gt; r._measurement == \"h2o_feet\" and r._field == \"water_level\" and r.location == \"coyote_creek\" ) |&gt; map(fn: (r) =&gt; ({ r with _value: r._value * 12.0, _measurement: \"h2o_inches\" })) |&gt; drop(columns: [\"_start\", \"_stop\"]) . Yields the following tables from the Profiler: . The Flux Profiler outputs performance information about your query in nanoseconds. | The first table provides information about your entire query including the total duration time it took to execute the query as well as the time it spent compiling, in the queue, etc. | The second table provides information about where the query is spending the most amount of time. | . The two most important columns to pay attention to are the TotalDuration column from the first table and the DurationSum column from the second table. This query is executed very quickly, so I don’t have to worry about optimizing it. However, I’ll describe the thought process for optimizing it further. First I would try to identify which part of the query is taking the longest to execute. From the query above, we can see that the merged_ReadRange5_filter operation has the largest DurationSum of 529282 ns. If I was planning to convert this query to a task and perform this transformation work on a schedule, the first thing I should consider is querying data over a shorter range and running the task more frequently. Next, I notice that the map() function contributes the second longest DurationSum value. Taking a look back at my map() function, I have to wonder if renaming the measurement with map() is the most efficient way. Perhaps I should try using the set() function instead like so: . from(bucket: \"noaa\") |&gt; range(start: 2019-08-17T00:00:00Z, stop: 2019-08-17T00:30:00Z) |&gt; filter(fn: (r) =&gt; r._measurement == \"h2o_feet\" and r._field == \"water_level\" and r.location == \"coyote_creek\" ) |&gt; drop(columns: [\"_start\", \"_stop\"]) |&gt; set(key: \"_measurement\",value: \"h2o_inches\") |&gt; map(fn: (r) =&gt; ({ r with _value: r._value * 12.0, })) . Also notice that I switch the order of the functions and apply drop() and set() functions before the map() function. After running the Profiler, I see a decrease in the TotalDuration time which indicates that these were all good changes. Since performance optimizations are continuously made to Flux and everyone’s schema is very different, there aren’t hard rules for Flux performance optimization. Instead, I encourage you to take advantage of the Profiler and perform some experimentation to find solutions that work best for you. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/optimizing-flux-performance/#the-flux-profiler-package",
    "relUrl": "/docs/part-2/optimizing-flux-performance/#the-flux-profiler-package"
  },"66": {
    "doc": "Optimizing Flux Performance",
    "title": "Using the Flux extension for Visual Studio Code to streamline Flux optimization discovery",
    "content": "If you haven’t given it a try already, I encourage you to install the Flux extension for Visual Studio Code. To query your InfluxDB Cloud account with the Flux extension, you must first configure it and connect to your cloud account. I enjoy using the Flux extension and VS Code when trying to debug complicated Flux scripts or trying to optimize the performance of my Flux scripts because I can save my Flux scripts and compare outputs from the Profiler simultaneously. The original “bad_join” query (red) is commented out because I ran it first. Its TotalDuration time was 17608617 ns. Joining on multiple like-columns and removing the drop() improves the performance to 14160858 ns. I decided to test the queries described in the “Using schema mutations properly” section above. The Profiler confirms my hypothesis: joining on multiple like-columns is more efficient than dropping redundant ones retroactively. While you can also perform this work in the InfluxDB UI, I find the side-by-side comparison of Profiler outputs helpful for this type of experimentation. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/optimizing-flux-performance/#using-the-flux-extension-for-visual-studio-code-to-streamline-flux-optimization-discovery",
    "relUrl": "/docs/part-2/optimizing-flux-performance/#using-the-flux-extension-for-visual-studio-code-to-streamline-flux-optimization-discovery"
  },"67": {
    "doc": "Optimizing Flux Performance",
    "title": "Other tips",
    "content": "Here are a list of other substitutions or ideas to consider when trying to optimize the performance of your Flux query: . | Can you use the experimental.join() function instead of join() function? | Can you apply any groups that will reduce the number of rows in your table(s) before applying a map() function? | Can you tune any regexes to be as specific as possible? | Can you use rows.map() instead of map()? | Does |&gt; sort(columns: [\"_time\"], desc: false) |&gt; limit(n:1) perform better than |&gt; last()? | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/optimizing-flux-performance/#other-tips",
    "relUrl": "/docs/part-2/optimizing-flux-performance/#other-tips"
  },"68": {
    "doc": "Optimizing Flux Performance",
    "title": "Best practices for receiving help",
    "content": "When asking for help with optimizing the performance of your Flux script, whether it’s on the community forums, Slack, or through support, please include the following information in your post or request: . | What is the query that’s having an issue? . | Make sure to share it as well as including the output from the Profiler. | . | What is the cardinality of your data? (how many series) . | Try using the InfluxDB Operational Monitoring Template to help you find the cardinality of your data. | Alternatively try using the schema.cardinality() function to help you find the cardinality of your data. | . | What is the density of your data? (how many points per unit time in each series) . | Try using the InfluxDB Cloud Usage Template to help you identify the data into your InfluxDB Cloud account. | . | General information about how your data is structured (which measurements, fields and tag keys exist) is always helpful. | Try using the schema package to share your data structure. | . | What is your expectation of how fast a query should run? What’s the basis for that expectation? | . Including as much of this information in a post will help us assist you better and more quickly. The above points also apply to issues with hitting memory limits. Part 3 . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/optimizing-flux-performance/#best-practices-for-receiving-help",
    "relUrl": "/docs/part-2/optimizing-flux-performance/#best-practices-for-receiving-help"
  },"69": {
    "doc": "Part 1",
    "title": "Part 1",
    "content": "Welcome to InfluxDB . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1",
    "relUrl": "/docs/part-1"
  },"70": {
    "doc": "Part 2",
    "title": "Part 2",
    "content": "Understanding InfluxDB . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2",
    "relUrl": "/docs/part-2"
  },"71": {
    "doc": "Part 3",
    "title": "Part 3",
    "content": "Using InfluxDB . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-3",
    "relUrl": "/docs/part-3"
  },"72": {
    "doc": "Querying and Data Transformations",
    "title": "Querying and Data Transformations",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/",
    "relUrl": "/docs/part-2/querying-and-data-transformations/"
  },"73": {
    "doc": "Querying and Data Transformations",
    "title": "Table of contents",
    "content": ". | Querying With Flux . | from() | range() . | start and stop | now() . | Calling range() with Relative Durations | . | Defining Ranges with Integers | Defining Ranges with Times | Calculating Start and Stop Times | Start and Stop Types Can Be Different | . | filter() . | Filter Basics | Anatomy of a Row | Filtering Measurements | Filtering Tags | Filtering by Field | Filter by Exists | Filtering by Field Value | Compound Filters | Regular Expressions . | If, Then, Else | . | Types In Comparisons | . | Queries and the Data Model | . | Flux Data Transformations . | Grouping . | group() | group() and Type Conflicts | drop()/keep() | rename() | Creating a Single Table or Ungrouping | . | Windowing | Windowing and aggregateWindow() | Real World Data Example of Grouping . | Default Grouping | group() | drop()/keep() | Grouping and Type Conflicts | Creating a Single Table | . | Aggregations . | mean() | min() and max() | count() | Aggregates and Selectors | . | Yielding . | Returning multiple aggregations with multiple yield() functions | Using variables to perform multiple aggregations | . | Pivoting . | The fieldsAsCol() function | . | Mapping . | In place transformation | New column(s) | Conditionally transform data | Changing types | The rows.map() function | . | Returning values and arrays . | Returning records | Returning columns | . | Reducing | Manipulating Time . | Converting timestamp formants | Calculating durations | Truncating or rounding timestamps | Shifting time | Other time manipulations | . | Regex . | The Regexp Package | . | The String Package | Combining Data Streams . | Join | Math across measurements | Union | . | Accessing External Data Sources . | The Flux SQL package | CSV . | experimental csv.from() | csv.from() | . | JSON | . | Materialized Views or Downsampling Tasks | . | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#table-of-contents",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#table-of-contents"
  },"74": {
    "doc": "Querying and Data Transformations",
    "title": "Querying With Flux",
    "content": "In the vernacular of Flux, a Flux script is called a “query.” This is despite the fact that you can write valid and useful Flux that doesn’t even query your own data at all. For our purposes now, we can think of writing a query as retrieving targeted data from the storage engine, as opposed to transforming and shaping the data, which will be discussed in detail in the following sections. As described in the section “Just Enough Flux” in the previous chapter, you can see that a typical simple query involves 3 parts: . | The source bucket | The range of time | A set of filters | . Additionally, a query may contain a yield() statement depending on circumstances. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#querying-with-flux",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#querying-with-flux"
  },"75": {
    "doc": "Querying and Data Transformations",
    "title": "from()",
    "content": "In most cases, a query starts by specifying a bucket to query from using the bucket name: . from(bucket: \"bucket1\") . In some cases, you may wish to use the bucket’s id instead: . from(bucketID: \"497b48e409406cc7\") . Typically, developers will address a bucket by its name for a few reasons. First, of course the bucket name is much more readable, the role of the bucket can be encoded in the name. Additionally, there may be times when deleting a bucket and creating a new one with the same name is the most expedient way to delete data. Addressing the bucket by its id has the advantage of being immutable. Someone can change the bucket name, and the query usin the id will continue working. There are cases that will be described below where you use a different kind of “from”, for example sql.from() or csv.from() or array.from() to bring in data from other sources. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#from",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#from"
  },"76": {
    "doc": "Querying and Data Transformations",
    "title": "range()",
    "content": "The range function is required directly after from()and its purpose is to specify the points to include based on their timestamps. range() has only two parameters. start and stop . An argument for start is required, whereas stop is optional. In the case where you leave out an argument for stop, Flux will substitute now(), which is the current time when execution is scheduled. now() . now() always returns the time when a Flux script is scheduled to start execution. This has some important implications: . | If your script is not run as part of a task, now() will return the time at the very start of execution of the script. If there are any delays, for example due to queuing as a result of excessive load, etc… now() will begin | If your script is running as part of a task, now() will return the time that your script was scheduled to run. | Every call to now() in the script will return the same time. | . Calling range() with Relative Durations . Possibly the most common way to use the range function is to use a start time like this: . range(start: -5m) . This says to provide all of the data that is available starting five minutes ago. This is inclusive, meaning that any data that is timestamped exactly with the nanosecond exactly five minutes ago will be included. Any data that is five minutes and one nanosecond older or more will not be included. Conversely, stop is exclusive. That is to say that if you have any data that is timestamped exactly with the stop argument, it will NOT be included with the results. So, for example, if there is data that is timestamped precisely 1 minute ago, and you have the following queries, that data will be included in the second query, but not the first. bucket(name: \"bucket1\") |&gt; range(start: -2m, stop: -1m) bucket(name: \"bucket1\") |&gt; range(start: -1m) . When a stop argument is not supplied Flux simply substitutes now(). So the following queries are equivalent: . bucket(name: \"bucket1\") |&gt; range(start: -1m, stop: now()) bucket(name: \"bucket1\") |&gt; range(start: -1m) . However, this is not true when the start time is in the future. This can happen if your timestamps are, for some reason, post-dated. If your start time is in the future, than now() is, logically before the start time, so this will cause an error: . bucket(name: \"bucket1\") |&gt; range(start: 1m) . Simply support a stop duration that is later than the start to ensure that it works. bucket(name: \"bucket1\") |&gt; range(start: 1m, stop: 2m) . A duration is a type in Flux. So it is unquoted, and consists of a signed integer and unit. The following duration units are supported: . 1ns // 1 nanosecond 1us // 1 microsecond 1ms // 1 millisecond 1s // 1 second 1m // 1 minute 1h // 1 hour 1d // 1 day 1w // 1 week 1mo // 1 calendar month 1y // 1 calendar year . So, for example, to select a week’s worth of data starting two weeks in the past, you can use relative durations like this: . |&gt; range(start: -2w, stop: -1w) . Durations represent a span of time, not a specific time. Therefore, Flux does not understand things like: . |&gt; range(start: now() - 5m) . That will result in an error because now() returns a specific time, whereas 5m represents a span of time. The types are not compatible. It is possible to do calculations based on times and durations, and this will be covered in detail in a later section. Durations are not addable, either, so the following will throw an error: . |&gt; range(start: -5m + -2m) . Defining Ranges with Integers . The start and stop parameters also accept integers. For example, you have already seen: . |&gt; range(start: 0) . The integer represents the nanoseconds that have transpired since Thursday, January 1, 1970 12:00:00 AM, GMT, also known as “Unix Time.” . This is extremely useful, as many systems with which you may want to integrate natively use Unix Time. For example, , 12:00 AM, GMT is represented as 1609480800000 in Unix time. However, in this case, notice that the time here is represented as milliseconds, not nanoseconds. To perform this conversion, simply multiply the milliseconds by 1,000,000, or you can define the precision when you write the data to the database. So, for all of the data starting from Jan 1, 2021: . bucket(name: \"bucket1\") |&gt; range(start: 1609480800000000000) . Unlike durations, integers are, of course, addable, so, to go back a year, this would work: . |&gt; range(start: -365 * 24 * 60 * 60 * 100000000) . As with durations, if you supply an integer in the future, you must supply a stop time that is later. Defining Ranges with Times . The third type that is accepted by start and stop is a time. A time object is expressed as RFC3339 timestamps. For example the following all represent the start of Unix Time: . | 1970-01-01 | 1970-01-01T00:00:00Z | 1970-01-01T00:00:00.000Z | . So, to get data from the start of some day to now: . bucket(name: \"bucket1\") |&gt; range(start: 2021-07-27) . To get data for some day in the past: . from(bucket: \"bucket1\") |&gt; range(start: 2021-07-25, stop: 2021-07-26) . By adding the “T” you can get arbitrarily fine grained resolution as well. For example, to skip the first nanosecond: . bucket(name: \"bucket1\") |&gt; range(start: 2021-07-27T00:00:00.0000000001Z) . If you only care about seconds, you can leave off the fraction: . bucket(name: \"bucket1\") |&gt; range(start: 2021-07-27T00:00:01Z) . Calculating Start and Stop Times . It is possible to compute start and stop times for the range. &lt;something here about subtracting time and adding time&gt; . Start and Stop Types Can Be Different . The start and stop parameters do not require the same type to be used. The following work fine. from(bucket: \"operating-results\") |&gt; range(start: -3d, stop: 2021-07-26) from(bucket: \"operating-results\") |&gt; range(start: 1627347600000000000, stop: -1h) . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#range",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#range"
  },"77": {
    "doc": "Querying and Data Transformations",
    "title": "filter()",
    "content": "A filter function must either implicitly or explicitly return a boolean value. A filter function operates on each row of each table, and in cases where there return value is true, the row is retained in the table. In cases where the return value is false, the row is removed from the table. Filter Basics . A very common filter is to filter by measurement. filter(fn: (r) =&gt; r._measurement == \"measurement1\") . The actual function is the argument for the fn parameter: . (r) =&gt; r._measurement == \"measurement1\" . “(r)” is the parameter list. A filter function always expects to only have a single parameter, and for it to be called “r.” Then the function body is a simple boolean expression that will evaluate to true or false. This function will return true when the _measurement for a row is “sensors” and so therefore the function will emit a stream of tables where all of the data has the sensor measurement. Naturally, you can omit the sensors measurement in the same manner: . filter(fn: (r) =&gt; r._measurement != \"measurement1\") . Anatomy of a Row . When read from the storage engine and passed into the filter function, by default, before being transformed by other functions, every row has the same essential object model. Flux uses a leading underscore (“_”) to delineate reserved member names. In Flux, each member of a row is called a “column,” or sometimes a “field” depending on the context. Drawing from a row of the example air sensor data, we can see how this is represented as a row in Flux. | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue1 | fieldname1 | 1.0 | rfc3339time1 | . | r._measurement is a string that is the measurement which defines the table that row is saved into. | r._field is a string that is the name of the field which defines the table that the row is saved into. | r._value is the actual value of the field. | r._time is the time stamp of the row. | . Additionally, each tag value is accessible by its tag name. For example, r.tag1, which in this example has a value of “tagvalue1.” . Finally, there are two additional context specific members added. These members are determined by the query, not the underlying data: . | r._start is the start time of the range() in the query. | r._stop() is the stop time of the range() in the query. | . For example, if you query with a range of 5 minutes in the past (range(start: -5m)), you will get a _start and _stop 5 minutes apart: . | _measurement | tag1 | _field | _value | _start | _stop | _time | . | measurement1 | tagvalue1 | fieldname1 | 1.0 | 2021:08:20T20:00:000000000Z | 2021:08:20T20:05:000000000Z | rfc3339time1 | . When you are filtering, you therefore have all of these columns to work from. Filtering Measurements . A discussed in the data model section above, a measurement is the highest order aggregation of data inside a bucket. It is, therefore, the most common subject, and typically first, filter, as it filters out the most irrelevant data in a single statement. Additionally, every table written by the storage engine has exactly one measurement, so the storage engine can quickly find the relevant tables and return them. filter(fn: (r) =&gt; r._measurement == \"measurement1\") . Given the two tables below, only the first will be returned if the preceding filter is applied: . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue1 | field1 | 1i | rfc3339time1 | . | measurement1 | tagvalue1 | field1 | 2i | rfc3339time2 | . | _measurement | tag1 | _field | _value | _time | . | measurement2 | tagvalue1 | field1 | 1.0 | rfc3339time1 | . | measurement2 | tagvalue1 | field1 | 2.0 | rfc3339time2 | . Filtering Tags . Multiple measurements can share the same tag set. As such, filtering by tag is sometimes secondary to filtering by measurement. The storage engine keeps track of where the tables with different tags for specific measurements are, so filtering by tag is typically reasonably fast. The following tables have different measurements, but the same tag values, so the following filter will return both tables: . |&gt; filter(fn: (r) =&gt; r.tag1 == \"tagvalue1\") . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue1 | field1 | 1i | rfc3339time1 | . | measurement1 | tagvalue1 | field1 | 2i | rfc3339time2 | . | _measurement | tag1 | _field | _value | _time | . | measurement2 | tagvalue1 | field1 | 1.0 | rfc3339time1 | . | measurement2 | tagvalue1 | field1 | 2.0 | rfc3339time2 | . If you only want one measurement with that tag value, you can simply include both filters. The following will return only the first table: . |&gt; filter(fn: (r) =&gt; r._measurement == \"measurement1\") |&gt; filter(fn: (r) =&gt; r.tag1 == \"tagvalue1\") . Filtering by Field . Filtering by field is extremely common, and also very fast, as fields are part of the group key of tables. Given the following table, if you are interested in records in field1 in measurement1, you can simply query like so, and get back only the first table: . |&gt; filter(fn: (r) =&gt; r._field == \"field1\") . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue1 | field1 | 1i | rfc3339time1 | . | measurement1 | tagvalue1 | field1 | 2i | rfc3339time2 | . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue1 | field2 | 1.0 | rfc3339time1 | . | measurement1 | tagvalue1 | field2 | 2.0 | rfc3339time2 | . | _measurement | tag1 | _field | _value | _time | . | measurement2 | tagvalue1 | field2 | 3.0 | rfc3339time1 | . | measurement2 | tagvalue1 | field2 | 4.0 | rfc3339time2 | . However, this won’t work for field2, as that field name exists in measurement2 as well. Simply include a measurement filter as well: . |&gt; filter(fn: (r) =&gt; r._measurement == \"measurement1\") |&gt; filter(fn: (r) =&gt; r._field == \"field1\") . This will return only the first table. Filter by Exists . There may be circumstances where you wish to only operate on tables that contain a specific tag value. You can use exists or not exists for this. The following will ensure that only tables which contain the “tag1” are returned: . |&gt; filter(fn: (r) =&gt; exists r.tag1) . Similarly, if you want to retain only tables that do not conain the “tag1”, use: . |&gt; filter(fn: (r) =&gt; not exists r.tag1) . To illustrate that point, take the following two tables. Each record has a different time stamp. The second table differs only in that those points were recorded with an additional tag, “tag2”. | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue1 | field1 | 1i | rfc3339time1 | . | measurement1 | tagvalue1 | field1 | 2i | rfc3339time2 | . | _measurement | tag1 | tag2 | _field | _value | _time | . | measurement1 | tagvalue1 | tagvalue2 | field1 | 3i | rfc3339time1 | . | measurement1 | tagvalue1 | tagvalue2 | field1 | 4i | rfc3339time2 | . The following query will return the first table: . | &gt; filter(fn: (r) =&gt; not exists r.tag2) | . If you only wanted to return the second table with the points that lack the “tag2”, you can use not exists. Instead you must must drop that column all together. We’ll cover that in more detail in later sections. Filtering by Field Value . Filtering by measurement(s), tag(s), or field(s) remove entire tables from the response. You can also filter out individual rows in tables. The most common way to do this is to filter by value. For example, if we take our few rows of air sensor data, and first filter by field: . |&gt; filter(fn: (r) =&gt; r._field == \"fieldname1\") . We are left with these two tables. | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue1 | field1 | 1.0 | rfc3339time1 | . | measurement1 | tagvalue1 | field1 | 2.0 | rfc3339time2 | . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue2 | field1 | 3.0 | rfc3339time1 | . | measurement1 | tagvalue2 | field1 | 3.0 | rfc3339time2 | . If we also add a filter for value, we can filter out individual rows. For example: . |&gt; filter(fn: (r) =&gt; r._field == \"field1\") |&gt; filter(fn: (r) =&gt; r._value &lt; 2.0) . Will result in the following stream of tables being emitted:. Note that the first table has dropped a single row: . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue1 | field1 | 2.0 | rfc3339time2 | . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue2 | field1 | 3.0 | rfc3339time1 | . | measurement1 | tagvalue2 | field1 | 3.0 | rfc3339time2 | . The row where the field value was less than 2 was dropped. Compound Filters . Boolean expressions in Flux can be compounded with “or” and “and.” For example, to retrieve all the tables with either the fields temperature or humidity, but no others, you use: . |&gt; filter(fn: (r) =&gt; r._field == \"field1\" or r._field == \"filed2\") . You can aggregate with or using different members of r if needed: . |&gt; filter(fn: (r) =&gt; r._field == \"field1\" or exist r.tag1) . You can use and as well: . |&gt; filter(fn: (r) =&gt; r._field == \"field1\" and r.tag1 == \"tagvalue1\") . However, this is less commonly used because it is equivalent to simply supplying two filters. The follow two filters is equivalent to, and arguably easier to read and modify: . |&gt; filter(fn: (r) =&gt; r._field == \"field1\") |&gt; filter(fn: (r) =&gt; r.sensor_id == \"tagvalue1\") . Regular Expressions . Sometimes your code will need to find substrings or even more complex pattern matching. Flux supports regular expressions for this purpose. There are two regex operators in Flux, “=~” for matches, and “!~” for does not match. The operators expect a string on the left side, and regular expression on the right. You define a regular expression object by surrounding your regex in “/.” For example to find all values of tag1 that include the string “tag”. You can use: . |&gt; filter(fn: (r) =&gt; r.tag1 =~ /tag/) . In this case, the regex operator is “matches”, i.e. find all of the tag1 values that match the regex, and the regex itself is tag. Every table where the tag1 tag value contains the string “tag” will be returned. To exclude all such tables, simply use the “does not match” version: . |&gt; filter(fn: (r) =&gt; r.tag1 !~ /tag/) . Flux supports the full range of regex expressiveness. To match the pattern of 3 capital letters and 4 digits: . |&gt; filter(fn: (r) =&gt; r._value =~ /[A-Z]{3}[0-9]{4}/) . These operators work on any field that is of type string. So you can use this to filter by measurement, field name, and even field value when the field value is a string type. However, it is important to note that the Flux storage engine cannot leverage the layout of tables when using regular expressions, so it must often scan every table, or even every row, to find matches. This can cause your queries to run much more slowly. Therefore, if you are regularly using regular expressions to filter your data, consider adding additional tags instead. If, Then, Else . Flux also supports “if, then, else” statements. This can be useful if you want to express more complex conditions in a readable manner. The following two filters are equivalent: . filter(fn: (r) =&gt; if r._value &lt; 2.0 then true else false) filter(fn: (r) =&gt; r._value &lt; 1.0) . Naturally, you can rather return the result of a boolean expression: . filter(fn: (r) =&gt; if r.tag1 == \"tagvalue1\" then r._value &lt; 2.0 else false) . If then else can also be chained: . filter(fn: (r) =&gt; if r.tag1 == \"tagvalue1\" then r._value &lt; 2.0 else if r.tag1 == \"tagvalue2\" then r._value &lt; 3.0 else false) . Types In Comparisons . As a strongly typed language, in general, Flux does not support comparisons between variables with different types. Flux does support comparing integers to floats: . int( v: \"1\") == 1.0 . But does not support comparing other data types. For example, this will cause an error: . \"1\" == 1.0 unsupported binary expression string == float . Times can be compared: . 2021-07-12T19:38:00.000Z &lt; 2021-07-12T19:39:00.000Z . But cannot be compared to, for example, Unix Time integers: . 2021-07-12T19:38:00.000Z &lt; 1627923626000000000 unsupported binary expression time &lt; int . But this can be done with some explicit casting: . uint(v: 2021-07-12T19:38:00.000Z) &lt; 1627923626000000000 . Details on how to cast time between different formats so that you calculate and compare time and durations from different formats is covered in detail in a future section. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#filter",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#filter"
  },"78": {
    "doc": "Querying and Data Transformations",
    "title": "Queries and the Data Model",
    "content": "This section focused on queries that only found and filtered data. As such, the results are a subset of tables and a subset of rows of the tables stored in the storage engine. There may be few tables, but the only change to the tables returned is that there may be rows filtered out. In other words, the pattern from() |&gt; range() |&gt; filter() will not transform your data as stored on disk other than perhaps filtering it. The next section will go further and delve into many of the options for transforming the shape of the data. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#queries-and-the-data-model",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#queries-and-the-data-model"
  },"79": {
    "doc": "Querying and Data Transformations",
    "title": "Flux Data Transformations",
    "content": "In addition to retrieving data from disk, Flux is a powerful data transformation tool. You can use Flux to shape your data however needed, as well as apply powerful mathematical transformations to your data as well. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#flux-data-transformations",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#flux-data-transformations"
  },"80": {
    "doc": "Querying and Data Transformations",
    "title": "Grouping",
    "content": "To review, when you write data to InfluxDB, the storage engine persists it in tables, where each table is defined by a “group key.” The group key used to persist the data is a measurement name, a unique set of tag values, and a field name. Consider the following example of 6 tables with two rows each, all containing the same measurement, but there are: . | Two tags, with a total of 5 tag values . | tag1 has the 2 tag values: . | tagvalue1 | tagvalue4 | . | tag2 has 3 tag values: . | tagvalue2 | tagvalue3 | tagvalue5 | . | . | Two fields . | field1 | field2 | . | . Where the line protocol would look like: . measurement,tag1=tagvalue1,tag2=tagvalue2 field1=0.0 unixtime1 measurement,tag1=tagvalue1,tag2=tagvalue2 field1=1.0 unixtime2 measurement,tag1=tagvalue4,tag2=tagvalue2 field1=0.0 unixtime1 measurement,tag1=tagvalue4,tag2=tagvalue2 field1=1.0 unixtime2 measurement,tag1=tagvalue1,tag2=tagvalue3 field1=0.0 unixtime1 measurement,tag1=tagvalue1,tag2=tagvalue3 field1=1.0 unixtime2 measurement,tag1=tagvalue4,tag2=tagvalue3 field1=0.0 unixtime1 measurement,tag1=tagvalue4,tag2=tagvalue3 field1=1.0 unixtime2 measurement,tag1=tagvalue1,tag2=tagvalue5 field1=0.0 unixtime1 measurement,tag1=tagvalue1,tag2=tagvalue5 field1=1.0 unixtime2 measurement,tag1=tagvalue4,tag2=tagvalue5 field1=0.0 unixtime1 measurement,tag1=tagvalue4,tag2=tagvalue5 field1=1.0 unixtime2 measurement,tag1=tagvalue1,tag2=tagvalue2 field2=0.0 unixtime1 measurement,tag1=tagvalue1,tag2=tagvalue2 field2=1.0 unixtime2 measurement,tag1=tagvalue4,tag2=tagvalue2 field2=0.0 unixtime1 measurement,tag1=tagvalue4,tag2=tagvalue2 field2=1.0 unixtime2 measurement,tag1=tagvalue1,tag2=tagvalue3 field2=0.0 unixtime1 measurement,tag1=tagvalue1,tag2=tagvalue3 field2=1.0 unixtime2 measurement,tag1=tagvalue4,tag2=tagvalue3 field2=0.0 unixtime1 measurement,tag1=tagvalue4,tag2=tagvalue3 field2=1.0 unixtime2 measurement,tag1=tagvalue1,tag2=tagvalue5 field2=0.0 unixtime1 measurement,tag1=tagvalue1,tag2=tagvalue5 field2=1.0 unixtime2 measurement,tag1=tagvalue4,tag2=tagvalue5 field2=0.0 unixtime1 measurement,tag1=tagvalue4,tag2=tagvalue5 field2=1.0 unixtime2 . I encourage you to replace the metasyntax timestamps with actual unix timestamps and try out the grouping on your own. You can use can use this unix timestamp converter to get two unix timestamps of your choice or you can use the following two values: . | 1628229600000000000 (or 2021-08-06T06:00:00.000000000Z) | 1628229900000000000 (or 2021-08-06T06:05:00.000000000Z) | . Then write the data to InfluxDB with the CLI, API, or InfluxDB UI. The following query will return the following 12 separate tables: . from(bucket: \"bucket1\") |&gt; range(start: 0) . Note that an extra row has been added to each table to denote if each column is part of the group key. The start column has been removed from the table response for simplicity. The table column has been added to help you keep track of the number of columns Remember, the group key for the table column is an exception and it’s always set to false. The group key for the table is set to false because users can’t directly change the table number. The table record will always be the same across rows even though the group key is set to false. | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue2 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue2 | 1.0 | rfc3339time2 | . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 1 | measurement1 | field1 | tagvalue4 | tagvalue2 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field1 | tagvalue4 | tagvalue2 | 1.0 | rfc3339time2 | . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 2 | measurement1 | field1 | tagvalue1 | tagvalue3 | 0.0 | rfc3339time1 | . | 2 | measurement1 | field1 | tagvalue1 | tagvalue3 | 1.0 | rfc3339time2 | . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 3 | measurement1 | field1 | tagvalue4 | tagvalue3 | 0.0 | rfc3339time1 | . | 3 | measurement1 | field1 | tagvalue4 | tagvalue3 | 1.0 | rfc3339time2 | . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 4 | measurement1 | field1 | tagvalue1 | tagvalue5 | 0.0 | rfc3339time1 | . | 4 | measurement1 | field1 | tagvalue1 | tagvalue5 | 1.0 | rfc3339time2 | . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 5 | measurement1 | field1 | tagvalue4 | tagvalue5 | 0.0 | rfc3339time1 | . | 5 | measurement1 | field1 | tagvalue4 | tagvalue5 | 1.0 | rfc3339time2 | . Aaa . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 6 | measurement1 | field2 | tagvalue1 | tagvalue2 | 0.0 | rfc3339time1 | . | 6 | measurement1 | field2 | tagvalue1 | tagvalue2 | 1.0 | rfc3339time2 | . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 7 | measurement1 | field2 | tagvalue4 | tagvalue2 | 0.0 | rfc3339time1 | . | 7 | measurement1 | field2 | tagvalue4 | tagvalue2 | 1.0 | rfc3339time2 | . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 8 | measurement1 | field2 | tagvalue1 | tagvalue3 | 0.0 | rfc3339time1 | . | 8 | measurement1 | field2 | tagvalue1 | tagvalue3 | 1.0 | rfc3339time2 | . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 9 | measurement1 | field2 | tagvalue4 | tagvalue3 | 0.0 | rfc3339time1 | . | 9 | measurement1 | field2 | tagvalue4 | tagvalue3 | 1.0 | rfc3339time2 | . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 10 | measurement1 | field2 | tagvalue1 | tagvalue5 | 0.0 | rfc3339time1 | . | 10 | measurement1 | field2 | tagvalue1 | tagvalue5 | 1.0 | rfc3339time2 | . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 11 | measurement1 | field2 | tagvalue4 | tagvalue5 | 0.0 | rfc3339time1 | . | 11 | measurement1 | field2 | tagvalue4 | tagvalue5 | 1.0 | rfc3339time2 | . By default each field name will be in a separate table, and then there are 6 unique combinations of tag values grouped with each field: . | tagvalue1 and tagvalue2 | tagvalue4 and tagvalue2 | tagvalue1 and tagvalue3 | tagvalue4 and tagvalue3 | tagvalue1 and tagvalue5 | tagvalue4 and tagvalue5 | . group() . The group() function can be to redefine the group keys, which will then result in regrouping the tables. We can begin by examining how defined the group key to a single column can affect the tables. from(bucket: \"bucket1\") |&gt; range(start: 0) |&gt; group(columns: [\"tag1\"]) . We know that there are 2 tag values for tag1 (tagvalue1 and tagvalue4), so we can predict that there will be two tables after the grouping: . | Not In Group Key | Not In Group Key | Not In Group Key | In Group Key | Not in Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue2 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue2 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue3 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue3 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue5 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue5 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field2 | tagvalue1 | tagvalue2 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field2 | tagvalue1 | tagvalue2 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field2 | tagvalue1 | tagvalue3 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field2 | tagvalue1 | tagvalue3 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field2 | tagvalue1 | tagvalue5 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field2 | tagvalue1 | tagvalue5 | 1.0 | rfc3339time2 | . | Not In Group Key | Not In Group Key | Not In Group Key | In Group Key | Not in Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 1 | measurement1 | field1 | tagvalue4 | tagvalue2 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field1 | tagvalue4 | tagvalue2 | 1.0 | rfc3339time2 | . | 1 | measurement1 | field1 | tagvalue4 | tagvalue3 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field1 | tagvalue4 | tagvalue3 | 1.0 | rfc3339time2 | . | 1 | measurement1 | field1 | tagvalue4 | tagvalue5 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field1 | tagvalue4 | tagvalue5 | 1.0 | rfc3339time2 | . | 1 | measurement1 | field2 | tagvalue4 | tagvalue2 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field2 | tagvalue4 | tagvalue2 | 1.0 | rfc3339time2 | . | 1 | measurement1 | field2 | tagvalue4 | tagvalue3 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field2 | tagvalue4 | tagvalue3 | 1.0 | rfc3339time2 | . | 1 | measurement1 | field2 | tagvalue4 | tagvalue5 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field2 | tagvalue4 | tagvalue5 | 1.0 | rfc3339time2 | . If we group by both tags, then we can predict that their will then be 6 tables because, as described above, there are three unique combinations of tag values: . from(bucket: \"bucket1\") |&gt; range(start: 0) |&gt; group(columns: [\"tag1\", \"tag2\"]) . | Not In Group Key | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue2 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue2 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field2 | tagvalue1 | tagvalue2 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field2 | tagvalue1 | tagvalue2 | 1.0 | rfc3339time2 | . | Not In Group Key | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 1 | measurement1 | field1 | tagvalue4 | tagvalue2 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field1 | tagvalue4 | tagvalue2 | 1.0 | rfc3339time2 | . | 1 | measurement1 | field2 | tagvalue4 | tagvalue2 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field2 | tagvalue4 | tagvalue2 | 1.0 | rfc3339time2 | . | Not In Group Key | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 2 | measurement1 | field1 | tagvalue1 | tagvalue3 | 0.0 | rfc3339time1 | . | 2 | measurement1 | field1 | tagvalue1 | tagvalue3 | 1.0 | rfc3339time2 | . | 2 | measurement1 | field2 | tagvalue1 | tagvalue3 | 0.0 | rfc3339time1 | . | 2 | measurement1 | field2 | tagvalue1 | tagvalue3 | 1.0 | rfc3339time2 | . | Not In Group Key | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 3 | measurement1 | field1 | tagvalue4 | tagvalue3 | 0.0 | rfc3339time1 | . | 3 | measurement1 | field1 | tagvalue4 | tagvalue3 | 1.0 | rfc3339time2 | . | 3 | measurement1 | field2 | tagvalue4 | tagvalue3 | 0.0 | rfc3339time1 | . | 3 | measurement1 | field2 | tagvalue4 | tagvalue3 | 1.0 | rfc3339time2 | . | Not In Group Key | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 4 | measurement1 | field1 | tagvalue1 | tagvalue5 | 0.0 | rfc3339time1 | . | 4 | measurement1 | field1 | tagvalue1 | tagvalue5 | 1.0 | rfc3339time2 | . | 4 | measurement1 | field2 | tagvalue1 | tagvalue5 | 0.0 | rfc3339time1 | . | 4 | measurement1 | field2 | tagvalue1 | tagvalue5 | 1.0 | rfc3339time2 | . | Not In Group Key | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 5 | measurement1 | field1 | tagvalue4 | tagvalue5 | 0.0 | rfc3339time1 | . | 5 | measurement1 | field1 | tagvalue4 | tagvalue5 | 1.0 | rfc3339time2 | . | 5 | measurement1 | field2 | tagvalue4 | tagvalue5 | 0.0 | rfc3339time1 | . | 5 | measurement1 | field2 | tagvalue4 | tagvalue5 | 1.0 | rfc3339time2 | . Grouping by field alone, we can predict that we will see a total of 2 tables, because the data set has only 2 field names. from(bucket: \"bucket1\") |&gt; range(start: 0) |&gt; group(columns: [\"_field\"]) . | Not In Group Key | Not In Group Key | In Group Key | Not In Group Key | Not In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue2 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue2 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field1 | tagvalue4 | tagvalue2 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue4 | tagvalue2 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue3 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue3 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field1 | tagvalue4 | tagvalue3 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue4 | tagvalue3 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue5 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue5 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field1 | tagvalue4 | tagvalue5 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue4 | tagvalue5 | 1.0 | rfc3339time2 | . | Not In Group Key | Not In Group Key | In Group Key | Not In Group Key | Not In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 1 | measurement1 | field2 | tagvalue1 | tagvalue2 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field2 | tagvalue1 | tagvalue2 | 1.0 | rfc3339time2 | . | 1 | measurement1 | field2 | tagvalue4 | tagvalue2 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field2 | tagvalue4 | tagvalue2 | 1.0 | rfc3339time2 | . | 1 | measurement1 | field2 | tagvalue1 | tagvalue3 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field2 | tagvalue1 | tagvalue3 | 1.0 | rfc3339time2 | . | 1 | measurement1 | field2 | tagvalue4 | tagvalue3 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field2 | tagvalue4 | tagvalue3 | 1.0 | rfc3339time2 | . | 1 | measurement1 | field2 | tagvalue1 | tagvalue5 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field2 | tagvalue1 | tagvalue5 | 1.0 | rfc3339time2 | . | 1 | measurement1 | field2 | tagvalue4 | tagvalue5 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field2 | tagvalue4 | tagvalue5 | 1.0 | rfc3339time2 | . Any combination of columns can be used for grouping depending on your purposes. For example, we can ask for tables for each field with each value for tag1. We can predict that there will be 4 such tables, because there are two fields and two tag values for tag1: . from(bucket: \"bucket1\") |&gt; range(start: 0) |&gt; group(columns: [\"_field\", \"tag1\"]) . | Not In Group Key | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue2 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue2 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue3 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue3 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue5 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue5 | 1.0 | rfc3339time2 | . | Not In Group Key | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 1 | measurement1 | field1 | tagvalue4 | tagvalue2 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field1 | tagvalue4 | tagvalue2 | 1.0 | rfc3339time2 | . | 1 | measurement1 | field1 | tagvalue4 | tagvalue3 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field1 | tagvalue4 | tagvalue3 | 1.0 | rfc3339time2 | . | 1 | measurement1 | field1 | tagvalue4 | tagvalue5 | 0.0 | rfc3339time1 | . | 1 | measurement1 | field1 | tagvalue4 | tagvalue5 | 1.0 | rfc3339time2 | . | Not In Group Key | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 2 | measurement1 | field2 | tagvalue1 | tagvalue2 | 0.0 | rfc3339time1 | . | 2 | measurement1 | field2 | tagvalue1 | tagvalue2 | 1.0 | rfc3339time2 | . | 2 | measurement1 | field2 | tagvalue1 | tagvalue3 | 0.0 | rfc3339time1 | . | 2 | measurement1 | field2 | tagvalue1 | tagvalue3 | 1.0 | rfc3339time2 | . | 2 | measurement1 | field2 | tagvalue1 | tagvalue5 | 0.0 | rfc3339time1 | . | 2 | measurement1 | field2 | tagvalue1 | tagvalue5 | 1.0 | rfc3339time2 | . | Not In Group Key | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 3 | measurement1 | field2 | tagvalue4 | tagvalue2 | 0.0 | rfc3339time1 | . | 3 | measurement1 | field2 | tagvalue4 | tagvalue2 | 1.0 | rfc3339time2 | . | 3 | measurement1 | field2 | tagvalue4 | tagvalue3 | 0.0 | rfc3339time1 | . | 3 | measurement1 | field2 | tagvalue4 | tagvalue3 | 1.0 | rfc3339time2 | . | 3 | measurement1 | field2 | tagvalue4 | tagvalue5 | 0.0 | rfc3339time1 | . | 3 | measurement1 | field2 | tagvalue4 | tagvalue5 | 1.0 | rfc3339time2 | . group() and Type Conflicts . In the above example, the values for field1 and field2 were always floats. Therefore, when grouping both of those fields into the same tables, it worked. However, recall that a column in InfluxDB can only have one type. Given the following two tables which differ only in that they have a different field name, and their field values have a different type: . | table | _measurement | _field | _value | _time | . | 0 | measurement1 | field1 | 1i | rfc3339time2 | . | table | _measurement | _field | _value | _time | . | 1 | measurement1 | field2 | 1.0 | rfc3339time2 | . An attempt to group these two tables into the same table will result in the following error: . from(bucket: \"bucket1\") |&gt; range(start: 0) |&gt; group() schema collision: cannot group integer and float types together . The simplest way to address this is to convert all of the values to floats using the toFloat() function. This function simply converts the value field for each record into a float if possible. from(bucket: \"bucket1\") |&gt; range(start: 0) |&gt; toFloat() |&gt; group() . | table | _measurement | _field | _value | _time | . | 0 | measurement1 | field1 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field2 | 1.0 | rfc3339time2 | . drop()/keep() . Another way to affect the group keys is to simply remove columns that are in the group key using the drop() or keep() functions. These two functions operate in the same manner, it’s just a matter of supplying a list of columns to eliminate vs. preserve. Note that the following are equivalent: . from(bucket: \"bucket1\") |&gt; range(start: 0) |&gt; drop(columns: [\"tag1\",\"tag2\"]) from(bucket: \"bucket1\") |&gt; range(start: 0) |&gt; keep(columns: [\"_measurement\",\"_field\",\"_value\",\"_time\"]) . In both cases the effect is to remove both of the tag columns from the table. Because tags are always in the group key by default, this change will leave only _measurement and _field in the group key. Because there is only one measurement, this will result in grouping solely by _field. | Not in Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | _value | _time | . | 0 | measurement1 | field1 | 0.0 | 2021-08-17T21:22:00.000000000Z | . | 0 | measurement1 | field1 | 1.0 | 2021-08-17T21:22:01.000000000Z | . | 0 | measurement1 | field1 | 0.0 | 2021-08-17T21:22:02.000000000Z | . | 0 | measurement1 | field1 | 1.0 | 2021-08-17T21:22:03.000000000Z | . | 0 | measurement1 | field1 | 0.0 | 2021-08-17T21:22:04.000000000Z | . | 0 | measurement1 | field1 | 1.0 | 2021-08-17T21:22:05.000000000Z | . | 0 | measurement1 | field1 | 0.0 | 2021-08-17T21:22:00.000000000Z | . | 0 | measurement1 | field1 | 1.0 | 2021-08-17T21:22:01.000000000Z | . | 0 | measurement1 | field1 | 0.0 | 2021-08-17T21:22:02.000000000Z | . | 0 | measurement1 | field1 | 1.0 | 2021-08-17T21:22:03.000000000Z | . | 0 | measurement1 | field1 | 0.0 | 2021-08-17T21:22:04.000000000Z | . | 0 | measurement1 | field1 | 1.0 | 2021-08-17T21:22:05.000000000Z | . | Not in Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | 1 | _measurement | _field | _value | _time | . | 1 | measurement1 | field1 | 0.0 | 2021-08-17T21:22:00.000000000Z | . | 1 | measurement1 | field1 | 1.0 | 2021-08-17T21:22:01.000000000Z | . | 1 | measurement1 | field1 | 0.0 | 2021-08-17T21:22:02.000000000Z | . | 1 | measurement1 | field1 | 1.0 | 2021-08-17T21:22:03.000000000Z | . | 1 | measurement1 | field1 | 0.0 | 2021-08-17T21:22:04.000000000Z | . | 1 | measurement1 | field1 | 1.0 | 2021-08-17T21:22:05.000000000Z | . | 1 | measurement1 | field1 | 0.0 | 2021-08-17T21:22:00.000000000Z | . | 1 | measurement1 | field1 | 1.0 | 2021-08-17T21:22:01.000000000Z | . | 1 | measurement1 | field1 | 0.0 | 2021-08-17T21:22:02.000000000Z | . | 1 | measurement1 | field1 | 1.0 | 2021-08-17T21:22:03.000000000Z | . | 1 | measurement1 | field1 | 0.0 | 2021-08-17T21:22:04.000000000Z | . | 1 | measurement1 | field1 | 1.0 | 2021-08-17T21:22:05.000000000Z | . Note that drop() and keep() are both susceptible to the same type conflicts that can cause errors with group(). rename() . The rename() function does not change the group key, but simply changes the names of the columns. It works by providing the function with a mapping of old column names to new column names. Given the following very simple table: . | _measurement | tag1 | _field | _value | _time | . | measurement1 | tagvalue1 | field1 | 1i | rfc3339time1 | . The following code simply renames the tag1 column: . from(bucket: \"bucket1\") |&gt; range(start: 0) |&gt; rename(columns: {\"tag1\":\"tag2\"}) . | _measurement | tag2 | _field | _value | _time | . | measurement1 | tagvalue1 | field1 | 1i | rfc3339time1 | . You can rename any column which can be very useful for things like formatting data to present to users. However, renaming can have some unintended consequences. For example, if you rename the _value column, certain functions will have surprising results or fail because they operate on _value. from(bucket: \"bucket1\") |&gt; range(start: 0) |&gt; rename(columns: {\"_value\":\"astring\"}) . | _measurement | tag1 | _field | astring | _time | . | measurement1 | tagvalue1 | field1 | 1i | rfc3339time1 | . from(bucket: \"bucket1\") |&gt; range(start: 0) |&gt; rename(columns: {\"_value\":\"astring\"}) |&gt; toFloat() . | _measurement | tag1 | _field | _value | astring | _time | . | measurement1 | tagvalue1 | field1 | | 1i | rfc3339time1 | . In this case, because there was no _value column to convert from, the toFloat() method had no data to place in the _value column that it creates. Creating a Single Table or Ungrouping . Finally, it is possible to put all of the data into a single table assuming that you avoid type conflicts. This is achieved by using the group() function with no arguments. Basically making the group key empty, so all of the data gets grouped into a single table. This is effectively the same as ungrouping. from(bucket: \"bucket1\") |&gt; range(start: 0) |&gt; group() . | Not In Group Key | Not In Group Key | In Group Key | Not In Group Key | Not In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | tag1 | tag2 | _value | _time | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue2 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue2 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field1 | tagvalue4 | tagvalue2 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue4 | tagvalue2 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue3 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue3 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field1 | tagvalue4 | tagvalue3 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue4 | tagvalue3 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue5 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue1 | tagvalue5 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field1 | tagvalue4 | tagvalue5 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field1 | tagvalue4 | tagvalue5 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field2 | tagvalue1 | tagvalue2 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field2 | tagvalue1 | tagvalue2 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field2 | tagvalue4 | tagvalue2 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field2 | tagvalue4 | tagvalue2 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field2 | tagvalue1 | tagvalue3 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field2 | tagvalue1 | tagvalue3 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field2 | tagvalue4 | tagvalue3 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field2 | tagvalue4 | tagvalue3 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field2 | tagvalue1 | tagvalue5 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field2 | tagvalue1 | tagvalue5 | 1.0 | rfc3339time2 | . | 0 | measurement1 | field2 | tagvalue4 | tagvalue5 | 0.0 | rfc3339time1 | . | 0 | measurement1 | field2 | tagvalue4 | tagvalue5 | 1.0 | rfc3339time2 | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#grouping",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#grouping"
  },"81": {
    "doc": "Querying and Data Transformations",
    "title": "Windowing",
    "content": "Windowing is when you group data by the start and stop times with the window() function. In previous sections the _start and _stop columns have been omitted for simplicity because they typically represent the start and stop times defined in the range function. To illustrate let’s filter our data for the first two tables and include the _start and _stop columns: . from(bucket: \"bucket1\") |&gt; range(start: 2021-08-17T00:00:00, stop:2021-08-17T3:00:00 ) |&gt; filter(fn: (r) =&gt; r[\"tag1\"] == \"tagvalue1\" or r[\"tag1\"] == \"tagvalue4\") |&gt; filter(fn: (r) =&gt; r[\"tag2\"] == \"tagvalue2\") |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"field1\") . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _start | _stop | _measurement | _field | tag1 | tag2 | _value | _time | . | 0 | 2021-08-17T00:00:00 | 2021-08-17T03:00:00 | measurement1 | field1 | tagvalue1 | tagvalue2 | 0.0 | 2021-08-17T01:00:00 | . | 0 | 2021-08-17T00:00:00 | 2021-08-17T03:00:00 | measurement1 | field1 | tagvalue1 | tagvalue2 | 1.0 | 2021-08-17T02:00:00 | . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _start | _stop | _measurement | _field | tag1 | tag2 | _value | _time | . | 1 | 2021-08-17T00:00:00 | 2021-08-17T03:00:00 | measurement1 | field1 | tagvalue4 | tagvalue2 | 0.0 | 2021-08-17T01:00:00 | . | 1 | 2021-08-17T00:00:00 | 2021-08-17T03:00:00 | measurement1 | field1 | tagvalue4 | tagvalue2 | 1.0 | 2021-08-17T02:00:00 | . If you apply the window() function the values in the _start and _stop column will change to reflect the defined window period: . from(bucket: \"bucket1\") |&gt; range(start: 2021-08-17T00:00:00, stop:2021-08-17T3:00:00 ) |&gt; filter(fn: (r) =&gt; r[\"tag1\"] == \"tagvalue1\" or r[\"tag1\"] == \"tagvalue4\") |&gt; filter(fn: (r) =&gt; r[\"tag2\"] == \"tagvalue2\") |&gt; window(period: 90m) . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _start | _stop | _measurement | _field | tag1 | tag2 | _value | _time | . | 0 | 2021-08-17T00:00:00 | 2021-08-17T01:30:00 | measurement1 | field1 | tagvalue1 | tagvalue2 | 0.0 | 2021-08-17T01:00:00 | . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _start | _stop | _measurement | _field | tag1 | tag2 | _value | _time | . | 1 | 2021-08-17T00:00:00 | 2021-08-17T01:30:00 | measurement1 | field1 | tagvalue4 | tagvalue2 | 0.0 | 2021-08-17T01:00:00 | . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _start | _stop | _measurement | _field | tag1 | tag2 | _value | _time | . | 2 | 2021-08-17T01:30:00 | 2021-08-17T03:00:00 | measurement1 | field1 | tagvalue1 | tagvalue2 | 1.0 | 2021-08-17T02:00:00 | . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _start | _stop | _measurement | _field | tag1 | tag2 | _value | _time | . | 3 | 2021-08-17T01:30:00 | 2021-08-17T03:00:00 | measurement1 | field1 | tagvalue4 | tagvalue2 | 1.0 | 2021-08-17T02:00:00 | . Windowing is performed for two main reasons: . | To aggregate data across fields or tags with timestamps in the same period. | To transform high precision series into a lower resolution aggregation. | . To aggregate data across fields or tags with similar timestamps, you can first apply the window() function like above, then you can group your data by the _start times. Now data that’s in the same window will be in the same table, so you can apply an aggregation after: . from(bucket: \"bucket1\") |&gt; range(start: 2021-08-17T00:00:00, stop:2021-08-17T3:00:00 ) |&gt; filter(fn: (r) =&gt; r[\"tag1\"] == \"tagvalue1\" or r[\"tag1\"] == \"tagvalue4\") |&gt; filter(fn: (r) =&gt; r[\"tag2\"] == \"tagvalue2\") |&gt; window(period: 90m) |&gt; group(columns: [\"_start\"], mode:\"by\") |&gt; yield(name:\"after group\") |&gt; sum() |&gt; yield(name:\"after sum\") . The result after the first yield, “after group” looks like: . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | . | table | _start | _stop | _measurement | _field | tag1 | tag2 | _value | _time | . | 0 | 2021-08-17T00:00:00 | 2021-08-17T01:30:00 | measurement1 | field1 | tagvalue1 | tagvalue2 | 0.0 | 2021-08-17T01:00:00 | . | 0 | 2021-08-17T00:00:00 | 2021-08-17T01:30:00 | measurement1 | field1 | tagvalue4 | tagvalue2 | 0.0 | 2021-08-17T01:00:00 | . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _start | _stop | _measurement | _field | tag1 | tag2 | _value | _time | . | 1 | 2021-08-17T01:30:00 | 2021-08-17T03:00:00 | measurement1 | field1 | tagvalue1 | tagvalue2 | 1.0 | 2021-08-17T02:00:00 | . | 1 | 2021-08-17T01:30:00 | 2021-08-17T03:00:00 | measurement1 | field1 | tagvalue4 | tagvalue2 | 1.0 | 2021-08-17T02:00:00 | . The result after the first yield, “after sum” looks like: . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | Not in Group Key | . | table | _start | _stop | _measurement | _field | tag1 | tag2 | _value | . | 0 | 2021-08-17T00:00:00 | 2021-08-17T01:30:00 | measurement1 | field1 | tagvalue1 | tagvalue2 | 0.0 | . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | . | table | _start | _stop | _measurement | _field | tag1 | tag2 | _value | . | 1 | 2021-08-17T01:30:00 | 2021-08-17T03:00:00 | measurement1 | field1 | tagvalue1 | tagvalue2 | 2.0 | . The sum() function is an aggregator so the _time column is removed because there isn’t a timestamp associated with the sum of two values. Keep in mind that in this example the timestamps in the _time column in the “after group” output happen to be the same, but this aggregation across fields within time windows would work even if the timestamps were different. The _time column isn’t a part of the group key. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#windowing",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#windowing"
  },"82": {
    "doc": "Querying and Data Transformations",
    "title": "Windowing and aggregateWindow()",
    "content": "The most common reason for using the window() function is to transform high precision data into lower resolution aggregations. Simply applying a sum() after a window would calculate the sum of the data for each series within the window period. To better illustrate window() function let’s look at the following simplified input data: . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _start | _stop | _measurement | _field | _value | _time | . | 0 | 2021-08-17T00:00:00 | 2021-08-17T01:30:00 | measurement1 | field1 | 0.0 | 2021-08-17T00:30:00 | . | 0 | 2021-08-17T00:00:00 | 2021-08-17T01:30:00 | measurement1 | field1 | 1.0 | 2021-08-17T01:00:00 | . | 0 | 2021-08-17T00:00:00 | 2021-08-17T01:30:00 | measurement1 | field1 | 2.0 | 2021-08-17T01:30:00 | . | 0 | 2021-08-17T00:00:00 | 2021-08-17T01:30:00 | measurement1 | field1 | 3.0 | 2021-08-17T02:00:00 | . The following query would return two tables with the sum for all the points in the series within a 90 min window: . from(bucket: \"bucket1\") |&gt; range(start: 2021-08-17T00:00:00, stop:2021-08-17T3:00:00 ) |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"field1\") |&gt; window(period: 90m) |&gt; sum() . | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | . | table | _start | _stop | _measurement | _field | _value | . | 1 | 2021-08-17T00:00:00 | 2021-08-17T01:30:00 | measurement1 | field1 | 5.0 | . By using the window() function following an aggregation function, we’ve reduced the number of points in our series by half. We’ve transformed a higher resolution data set into a lower resolution sum over 90 min windows. This combination of functions introduces another similar function, the aggregateWindow() function. The aggregateWindow() function windows data and applies an aggregate function or selector function to the data. You can think of the aggregateWindow() function as being a combination of the window() function followed by an aggregate or selector function. The difference between the window() function and the aggregateWindow() function is that the aggregateWindow() function applies a group to your data at the end so that your lower resolution aggregations aren’t separated into different tables by their window period. Instead all lower resolution aggregations are grouped together. In other words, these two queries are equivalent: . query1 = from(bucket: \"bucket1\") |&gt; range(start: 2021-08-17T00:00:00, stop:2021-08-17T3:00:00 ) |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"field1\" or r[\"_field\"] == \"field2\" ) |&gt; window(period: 90m) |&gt; sum() |&gt; group(columns: [\"_field\"],mode:\"by\") query2 = from(bucket: \"bucket1\") |&gt; range(start: 2021-08-17T00:00:00, stop:2021-08-17T3:00:00 ) |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"field1\" or r[\"_field\"] == \"field2\" ) |&gt; aggregateWindow(every: 90m, fn: sum) . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#windowing-and-aggregatewindow",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#windowing-and-aggregatewindow"
  },"83": {
    "doc": "Querying and Data Transformations",
    "title": "Real World Data Example of Grouping",
    "content": "Having reviewed grouping and aggregation using clean instructive data, it is worthwhile to review the concepts again, but looking at real world data. The NOAA buoy data is a good sample set to look at due to its complexity. For example, to take a closer look at wind speeds, the following query will simply return all of the tables with the field “wind_speed_mps.” . from(bucket: \"noaa\") |&gt; range(start: -12h) |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"wind_speed_mps\") . This query reads back data for a total of around 628 tables (this value will be more or less depending on the time range queried). Due to the combination of tag values and the restricted time range, most of the tables returned have only a single row. Here are the first few rows as an example. Default Grouping . | In Group Key | In Group Key | Not in Group Key | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | . | _start | _stop | _time | _value | _field | _measurement | station_id | station_name | station_owner | station_pgm | station_type | . | 2021-08-03T03:35:12.486582468Z | 2021-08-03T15:35:12.486582468Z | 2021-08-03T05:00:00Z | 9 . | wind_speed_mps | ndbc | 46303 . | Southern Georgia Strait | Environment and Climate Change Canada | International Partners | buoy | . | In Group Key | In Group Key | Not in Group Key | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | . | _start | _stop | _time | _value | _field | _measurement | station_id | station_name | station_owner | station_pgm | station_type | . | 2021-08-03T03:35:12.486582468Z | 2021-08-03T15:35:12.486582468Z | 2021-08-03T05:00:00Z | 7.7 . | wind_speed_mps | ndbc | FWYF1 | Fowey Rock, FL | NDBC | NDBC Meteorological/Ocean | fixed | . | In Group Key | In Group Key | Not in Group Key | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | . | _start | _stop | _time | _value | _field | _measurement | station_id | station_name | station_owner | station_pgm | station_type | . | 2021-08-03T03:35:12.486582468Z | 2021-08-03T15:35:12.486582468Z | 2021-08-03T04:30:00Z | 2.1 . | wind_speed_mps | ndbc | GELO1 | Geneva on the Lake Light, OH | NWS Eastern Region | IOOS Partners | fixed | . | In Group Key | In Group Key | Not in Group Key | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | . | _start | _stop | _time | _value | _field | _measurement | station_id | station_name | station_owner | station_pgm | station_type | . | 2021-08-03T03:35:12.486582468Z | 2021-08-03T15:35:12.486582468Z | 2021-08-03T05:18:00Z | 4.6 . | wind_speed_mps | ndbc | FMOA1 | 8734673 - Fort Morgan, AL | NOAA NOS PORTS | NOS/CO-OPS | fixed | . | In Group Key | In Group Key | Not in Group Key | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | . | _start | _stop | _time | _value | _field | _measurement | station_id | station_name | station_owner | station_pgm | station_type | . | 2021-08-03T03:35:12.486582468Z | 2021-08-03T15:35:12.486582468Z | 2021-08-03T05:18:00Z | 6.7 . | wind_speed_mps | ndbc | TXPT2 | 8770822 - Texas Point, Sabine Pass, TX | TCOON | IOOS Partners | fixed | . | In Group Key | In Group Key | Not in Group Key | Not in Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | In Group Key | . | _start | _stop | _time | _value | _field | _measurement | station_id | station_name | station_owner | station_pgm | station_type | . | 2021-08-03T03:35:12.486582468Z | 2021-08-03T15:35:12.486582468Z | 2021-08-03T05:30:00Z | 2 . | wind_speed_mps | ndbc | 45170 . | Michigan City Buoy, IN | Illinois-Indiana Sea Grant and Purdue Civil Engineering | IOOS Partners | buoy | . group() . The group() function can be to redefine the group keys, which will then result in entirely different tables. For example: . from(bucket: \"noaa\") |&gt; range(start: -12h) |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"wind_speed_mps\") |&gt; group(columns: [\"station_type\"]) . This call to group() tells Flux to make only the single column station_type to be in the set of columns in the group key. station_type has four possible values (“buoy”,”fixed”, “oilrig”, and “other”). As a result, we know that the results will then contain exactly 4 tables. Here are excerpts from those tables: . | In Group Key | In Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | In Group Key | . | _start | _stop | _time | _value | _field | _measurement | station_id | station_name | station_owner | station_pgm | station_type | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:00:00Z | 3 . | wind_speed_mps | ndbc | 22102 . | | Korean Meteorological Administration | International Partners | buoy | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:00:00Z | 4 . | wind_speed_mps | ndbc | 22103 . | | Korean Meteorological Administration | International Partners | buoy | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:00:00Z | 2 . | wind_speed_mps | ndbc | 22104 . | | Korean Meteorological Administration | International Partners | buoy | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:00:00Z | 6 . | wind_speed_mps | ndbc | 22105 . | | Korean Meteorological Administration | International Partners | buoy | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:00:00Z | 5 . | wind_speed_mps | ndbc | 22106 . | | Korean Meteorological Administration | International Partners | buoy | . | ... | | | | | | | | | | | . | In Group Key | In Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | In Group Key | . | _start | _stop | _time | _value | _field | _measurement | station_id | station_name | station_owner | station_pgm | station_type | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T04:30:00Z | 5 . | wind_speed_mps | ndbc | 32ST0 | Stratus | Woods Hole Oceanographic Institution | IOOS Partners | fixed | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:00:00Z | 4.1 . | wind_speed_mps | ndbc | 62103 . | Channel Lightship | UK Met Office | International Partners | fixed | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T04:45:00Z | 0 . | wind_speed_mps | ndbc | ACXS1 | Bennetts Point, ACE Basin Reserve, SC | National Estuarine Research Reserve System | NERRS | fixed | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:30:00Z | 5.7 . | wind_speed_mps | ndbc | AMAA2 | East Amatuli Island Light, AK | NDBC | NDBC Meteorological/Ocean | fixed | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T04:30:00Z | 0 . | wind_speed_mps | ndbc | ANMN6 | Field Station, Hudson River Reserve, NY | National Estuarine Research Reserve System | NERRS | fixed | . | ... | | | | | | | | | | | . | In Group Key | In Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | In Group Key | . | _start | _stop | _time | _value | _field | _measurement | station_id | station_name | station_owner | station_pgm | station_type | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:00:00Z | 2.6 . | wind_speed_mps | ndbc | 62114 . | Tartan &quot;A&quot; AWS | Private Industry Oil Platform | International Partners | oilrig | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:00:00Z | 2.1 . | wind_speed_mps | ndbc | 62121 . | Carrack AWS | Private Industry Oil Platform | International Partners | oilrig | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:00:00Z | 1.5 . | wind_speed_mps | ndbc | 62144 . | Clipper AWS | Private Industry Oil Platform | International Partners | oilrig | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:00:00Z | 2.6 . | wind_speed_mps | ndbc | 62145 . | North Sea | Private Industry Oil Platform | International Partners | oilrig | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:00:00Z | 1.5 . | wind_speed_mps | ndbc | 62146 . | Lomond AWS | Private Industry Oil Platform | International Partners | oilrig | . | ... | | | | | | | | | | | . | In Group Key | In Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | In Group Key | . | _start | _stop | _time | _value | _field | _measurement | station_id | station_name | station_owner | station_pgm | station_type | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:20:00Z | 9 . | wind_speed_mps | ndbc | 41002 . | SOUTH HATTERAS - 225 NM South of Cape Hatteras | NDBC | NDBC Meteorological/Ocean | other | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:20:00Z | 6 . | wind_speed_mps | ndbc | 41009 . | CANAVERAL 20 NM East of Cape Canaveral, FL | NDBC | NDBC Meteorological/Ocean | other | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:20:00Z | 12 . | wind_speed_mps | ndbc | 41010 . | CANAVERAL EAST - 120NM East of Cape Canaveral | NDBC | NDBC Meteorological/Ocean | other | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:20:00Z | 2 . | wind_speed_mps | ndbc | 41013 . | Frying Pan Shoals, NC | NDBC | NDBC Meteorological/Ocean | other | . | 2021-08-03T03:50:09.78158678Z | 2021-08-03T15:50:09.78158678Z | 2021-08-03T05:20:00Z | 8 . | wind_speed_mps | ndbc | 41040 . | NORTH EQUATORIAL ONE- 470 NM East of Martinique | NDBC | NDBC Meteorological/Ocean | other | . | ... | | | | | | | | | | | . You may note that _start and _stop are also in the group key. Remember that these columns are added by Flux to specify the time range of the data being returned. For data from a single query, these values will always be the same for all rows, and thus will not change the number of tables. You can further group by including multiple columns. For example, one can add station_pgm (the name of the partner organization providing the data) to the group key as well: . from(bucket: \"noaa\") |&gt; range(start: -12h) |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"wind_speed_mps\") |&gt; group(columns: [\"station_type\", \"station_pgm\"]) . Now we can see in the returned tables, that because station_type and station_pgm are in the group key, the unique combinations of those values are in separate tables. For example IOOS Partners have both buoy stations and fixed stations, so those different station types are grouped into separate tables. | In Group Key | In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | In Group Key | In Group Key | . | _start | _stop | _time | _value | _field | _measurement | station_id | station_name | station_owner | station_pgm | station_type | . | 2021-08-03T04:11:46.849771273Z | 2021-08-03T16:11:46.849771273Z | 2021-08-03T05:08:00Z | 2 . | wind_speed_mps | ndbc | 41033 . | Fripp Nearshore, SC (FRP2) | CORMP | IOOS Partners | buoy | . | 2021-08-03T04:11:46.849771273Z | 2021-08-03T16:11:46.849771273Z | 2021-08-03T05:08:00Z | 2 . | wind_speed_mps | ndbc | 41037 . | Wrightsville Beach Offshore, NC (ILM3) | CORMP | IOOS Partners | buoy | . | 2021-08-03T04:11:46.849771273Z | 2021-08-03T16:11:46.849771273Z | 2021-08-03T05:00:00Z | 7 . | wind_speed_mps | ndbc | 41052 . | South of St. John, VI | Caribbean Integrated Coastal Ocean Observing System (CarICoos) | IOOS Partners | buoy | . | 2021-08-03T04:11:46.849771273Z | 2021-08-03T16:11:46.849771273Z | 2021-08-03T05:00:00Z | 4 . | wind_speed_mps | ndbc | 41053 . | San Juan, PR | Caribbean Integrated Coastal Ocean Observing System (CarICoos) | IOOS Partners | buoy | . | 2021-08-03T04:11:46.849771273Z | 2021-08-03T16:11:46.849771273Z | 2021-08-03T05:00:00Z | 6 . | wind_speed_mps | ndbc | 41056 . | Vieques Island, PR | Caribbean Integrated Coastal Ocean Observing System (CarICoos) | IOOS Partners | buoy | . | ... | | | | | | | | | | | . | In Group Key | In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | In Group Key | In Group Key | . | _start | _stop | _time | _value | _field | _measurement | station_id | station_name | station_owner | station_pgm | station_type | . | 2021-08-03T04:11:46.849771273Z | 2021-08-03T16:11:46.849771273Z | 2021-08-03T04:30:00Z | 5 . | wind_speed_mps | ndbc | 32ST0 | Stratus | Woods Hole Oceanographic Institution | IOOS Partners | fixed | . | 2021-08-03T04:11:46.849771273Z | 2021-08-03T16:11:46.849771273Z | 2021-08-03T04:30:00Z | 7 . | wind_speed_mps | ndbc | 41NT0 | NTAS - Northwest Tropical Atlantic | Woods Hole Oceanographic Institution | IOOS Partners | fixed | . | 2021-08-03T04:11:46.849771273Z | 2021-08-03T16:11:46.849771273Z | 2021-08-03T05:18:00Z | 3.1 . | wind_speed_mps | ndbc | ANPT2 | 8775241 - Aransas, Aransas Pass, TX | TCOON | IOOS Partners | fixed | . | 2021-08-03T04:11:46.849771273Z | 2021-08-03T16:11:46.849771273Z | 2021-08-03T05:30:00Z | 4.1 . | wind_speed_mps | ndbc | APNM4 | Alpena Harbor Light, Alpena, MI | GLERL | IOOS Partners | fixed | . | 2021-08-03T04:11:46.849771273Z | 2021-08-03T16:11:46.849771273Z | 2021-08-03T04:40:00Z | 1.5 . | wind_speed_mps | ndbc | BHRI3 | Burns Harbor, IN | NWS Central Region | IOOS Partners | fixed | . | ... | | | | | | | | | | | . drop()/keep() . Another way to affect the group keys is to simply remove columns that are in the group key using the drop() or keep() functions. These two functions operate in the same manner, it’s just a matter of supplying a list of columns to eliminate vs. preserve. The following are equivalent: . from(bucket: \"noaa\") |&gt; range(start: -12h) |&gt; filter(fn: (r) =&gt; r._measurement == \"ndbc\") |&gt; filter(fn: (r) =&gt; r._field == \"wind_speed_mps\") |&gt; drop(columns: [\"_measurement\",\"_start\",\"_stop\",\"station_name\",\"station_owner\"]) from(bucket: \"noaa\") |&gt; range(start: -12h) |&gt; filter(fn: (r) =&gt; r._measurement == \"ndbc\") |&gt; filter(fn: (r) =&gt; r._field == \"wind_speed_mps\") |&gt; keep(columns: [\"_field\",\"_value\",\"_time\",\"station_id\",\"station_pgm\", \"station_type\"]) . The first thing to notice is that applying keep() cleans up the data dramatically, making it much easier to read and work with. A common use of drop() and keep() therefore, is just to make the data more readable. However, if you drop a column that is in the group key, this will impact the tables. For example, the query above results in 559 tables, because it leaves station_id, station_pgm, and station_type all in the group key, and the combination of those unique sets of tag values, adds up to 559 different combinations. If we also drop the station_id, this drops the tag with the most unique values: . from(bucket: \"noaa\") |&gt; range(start: -12h) |&gt; filter(fn: (r) =&gt; r._measurement == \"ndbc\") |&gt; filter(fn: (r) =&gt; r._field == \"wind_speed_mps\") |&gt; drop(columns: [\"_measurement\",\"_start\",\"_stop\",\"station_name\",\"station_owner\",\"station_id\"]) . This results in a total of only 12 tables. There are a total of 6 station_pgm values, each with about 2 station_types, so only a total of 12 tables. Grouping and Type Conflicts . It is possible using group(), drop(), keep() or other functions to remove _field from the group key. Remember that Flux is strongly typed, so a column cannot contain values of multiple types. As a result, it is possible to create errors when grouping. Because tag values are always strings, and _start, _stop, and _time are always times, this problem almost always happens due to _fields. So, if we rerun one the queries above without filtering the field using the following: . from(bucket: \"noaa\") |&gt; range(start: -24h) |&gt; group(columns: [\"station_type\", \"station_pgm\"]) . Part of the output is an error message: . schema collision: cannot group float and string types together . This happens because some of the fields (station_met, station_currents, station_waterquality, station_dart) are all strings, so cannot be grouped into tables with the fields that are floats. One way to solve this is to keep the _field column in the group key: . from(bucket: \"noaa\") |&gt; range(start: -24h) |&gt; group(columns: [\"station_type\", \"station_pgm\", \"_field\"]) . Though of course this will result in creating many more tables. Creating a Single Table . Finally, it is possible to put all of the data into a single table assuming that you avoid type conflicts. This is achieved by using the group() function with no arguments. Basically making the group key empty, so all of the data gets grouped into a single table. from(bucket: \"noaa\") |&gt; range(start: -12h) |&gt; filter(fn: (r) =&gt; r._measurement == \"ndbc\") |&gt; filter(fn: (r) =&gt; r._field == \"wind_speed_mps\") |&gt; group() . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#real-world-data-example-of-grouping",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#real-world-data-example-of-grouping"
  },"84": {
    "doc": "Querying and Data Transformations",
    "title": "Aggregations",
    "content": "In the Flux vernacular, an “aggregation” is a summary of a table. Indeed, one of the key reasons to regroup is in order to summarize as desired. As an example, to answer the question “do the different station types have different average windows speeds?” the overall approach would be to: . | Query the time range of interest | Filter to just the wind_speed_mps field | Group the results into one table for each station type | Calculate the mean for each table | Put all the results into a single table | . The Flux looks like this: . from(bucket: \"noaa\") |&gt; range(start: -12h) |&gt; filter(fn: (r) =&gt; r._measurement == \"ndbc\") |&gt; filter(fn: (r) =&gt; r._field == \"wind_speed_mps\") |&gt; group(columns: [\"station_type\"]) |&gt; mean() |&gt; group() . | _start | _stop | _station_type | _value | . | 2021-08-05T01:28:12.024108193Z | 2021-08-05T13:28:12.024108193Z | buoy | 4.997794117647059 | . | 2021-08-05T01:28:12.024108193Z | 2021-08-05T13:28:12.024108193Z | fixed | 3.1083950617283946 | . | 2021-08-05T01:28:12.024108193Z | 2021-08-05T13:28:12.024108193Z | oilrig | 6.883999999999998 | . | 2021-08-05T01:28:12.024108193Z | 2021-08-05T13:28:12.024108193Z | other | 5.675675675675675 | . That last step of collapsing all the data for each table into a mean is what Flux calls “aggregation.” . The following sections cover some of the most common Flux aggregations. mean() . This function is typically used as demonstrated above: . |&gt; mean() . When called without arguments, mean() function will use the _value column. However, it is possible that pivoted data will have more than one column that could be aggregated, and, additionally, it is possible to have renamed _value. In such cases, as demonstrated here: . from(bucket: \"noaa\") |&gt; range(start: -12h) |&gt; filter(fn: (r) =&gt; r._measurement == \"ndbc\") |&gt; filter(fn: (r) =&gt; r._field == \"wind_speed_mps\") |&gt; group(columns: [\"station_type\"]) |&gt; rename(columns: {\"_value\":\"wind_speed\"}) |&gt; mean() |&gt; group() runtime error @7:6-7:12: mean: column \"_value\" does not exist . This can be fixed easily by specifying the column in the mean() function: . from(bucket: \"noaa\") |&gt; range(start: -12h) |&gt; filter(fn: (r) =&gt; r._measurement == \"ndbc\") |&gt; filter(fn: (r) =&gt; r._field == \"wind_speed_mps\") |&gt; group(columns: [\"station_type\"]) |&gt; rename(columns: {\"_value\":\"wind_speed\"}) |&gt; mean(column: \"wind_speed\") |&gt; group() . | _start | _stop | _station_type | wind_speed | . | 2021-08-05T01:28:12.024108193Z | 2021-08-05T13:28:12.024108193Z | buoy | 4.997794117647059 | . | 2021-08-05T01:28:12.024108193Z | 2021-08-05T13:28:12.024108193Z | fixed | 3.1083950617283946 | . | 2021-08-05T01:28:12.024108193Z | 2021-08-05T13:28:12.024108193Z | oilrig | 6.883999999999998 | . | 2021-08-05T01:28:12.024108193Z | 2021-08-05T13:28:12.024108193Z | other | 5.675675675675675 | . Note that because mean() aggregates data from all rows in a table, most columns get dropped. Only the columns in the group key and the column that was subject to the mean() function is preserved. min() and max() . These will always return exactly one row, with the lowest or highest value in the _value column for each table. Like with the mean() funciton, you can specify the column you want to use, but the _value column is used by default. from(bucket: \"noaa\") |&gt; range(start: -12h) |&gt; filter(fn: (r) =&gt; r._measurement == \"ndbc\") |&gt; filter(fn: (r) =&gt; r._field == \"wind_speed_mps\") |&gt; keep(columns: [\"_value\",\"station_type\",\"_time\"]) |&gt; min() |&gt; group() . | _start | _stop | _time | _value | _field | _masurement | _station_id | _station_name | station_owner | station_pgm | station_type | . | 2021-08-05T02:27:53.856929358Z | 2021-08-05T14:27:53.856929358Z | 2021-08-05T05:00:00Z | 1 | wind_speed_mps | ndbc | 45152 | Lake Nipissing | Environment and Climate Change Canada | ,International Partners | buoy | . | 2021-08-05T02:27:53.856929358Z | 2021-08-05T14:27:53.856929358Z | 2021-08-05T04:30:00Z | 0 | wind_speed_mps | ndbc | ANMN6 | Field Station, Hudson River Reserve, NY | National Estuarine Research Reserve System | NERRS | fixed | . | 2021-08-05T02:27:53.856929358Z | 2021-08-05T14:27:53.856929358Z | 2021-08-05T05:30:00Z | 0 | wind_speed_mps | ndbc | KGRY | Green Canyon 338 / Front Runner | Federal Aviation Administration | Marine METAR | oilrig | . | 2021-08-05T02:27:53.856929358Z | 2021-08-05T14:27:53.856929358Z | 2021-08-05T05:30:00Z | 0 | wind_speed_mps | ndbc | 46025 | ,\"Santa Monica Basin - 33NM WSW of Santa Monica, CA\" | NDBC | NDBC Meteorological/Ocean | other | . In this case, all of the columns are retained. This is because min() and max() return a row per table. Effectively picking a row and filtering out the rest. These functions do not, therefore, need to combine values from different rows, so all of the columns are retained. Note that this can cause group() to fail if there are type conflicts in columns, as covered later in the section on type conflicts. Of course, the data can be cleaned up by dropping unwanted columns: . from(bucket: \"noaa\") |&gt; range(start: -12h) |&gt; filter(fn: (r) =&gt; r._measurement == \"ndbc\") |&gt; filter(fn: (r) =&gt; r._field == \"wind_speed_mps\") |&gt; group(columns: [\"station_type\"]) |&gt; min() |&gt; group() |&gt; keep(columns: [\"_time\", \"_value\", \"station_type\"]) . | _time | _value | station_type | . | 2021-08-05T05:00:00Z | 1 | buoy | . | 2021-08-05T04:30:00Z | 0 | fixed | . | 2021-08-05T05:30:00Z | 0 | oilrig | . | 2021-08-05T05:30:00Z | 0 | other | . count() . The count() function returns the number of rows in a table. This can be particularly useful for counting events. In this case, it is used to count the number of the different station types reporting in: . from(bucket: \"noaa\") |&gt; range(start: -12h) |&gt; filter(fn: (r) =&gt; r._measurement == \"ndbc\") |&gt; filter(fn: (r) =&gt; r._field == \"wind_speed_mps\") |&gt; group(columns: [\"station_type\"]) |&gt; count() |&gt; group() . | _start | _stop | _value | _station_type | . | 2021-08-05T04:00:59.334352876Z | 2021-08-05T16:00:59.334352876Z | 107 | buoy | . | 2021-08-05T04:00:59.334352876Z | 2021-08-05T16:00:59.334352876Z | 395 | fixed | . | 2021-08-05T04:00:59.334352876Z | 2021-08-05T16:00:59.334352876Z | 25 | oilrig | . | 2021-08-05T04:00:59.334352876Z | 2021-08-05T16:00:59.334352876Z | 73 | other | . As in the case of mean(), because count() combines values from different columns, only columns in the group key and the _value column are retained. As expected, for cases where the _value column does not exist in the tables to be counted, you can specify a different column to count: . from(bucket: \"noaa\") |&gt; range(start: -12h) |&gt; filter(fn: (r) =&gt; r._measurement == \"ndbc\") |&gt; filter(fn: (r) =&gt; r._field == \"wind_speed_mps\") |&gt; group(columns: [\"station_type\"]) |&gt; rename(columns: {\"_value\":\"windspeed\"}) |&gt; count(column: \"windspeed\") |&gt; group() . Aggregates and Selectors . While all transformations that summarize your data typically refered to as “aggregations” in Flux vernacular there are actually two types of aggregates: . | Aggregates: These functions return a single row output for every input table. The output also has the same group key as the input table(s)–the _time column is usually dropped. Aggregates include but are not limited to the following functions: . | count() | mean() | mode() | median() | and more… | . | Selectors: These functions return a one ore more rows for every input table. The output is an unmodified record–the _time column is typically included. Aggregates include but are not limited to the following functions: . | min() | max() | distinct() | first() | and more… | . | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#aggregations",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#aggregations"
  },"85": {
    "doc": "Querying and Data Transformations",
    "title": "Yielding",
    "content": "The yield() function determines which table inputs should be returned in a flux script. The yield() function also assigns a name to the output of a Flux query. The name is stored in the default annotation. For example if we query the following table: . | _measurement | tag1 | _field | _value | _time | . | Measurement1 | tagvalue1 | field1 | 1i | 2021-09-17T21:22:52.00Z | . Without the yield function: . from(bucket: \"bucket1\") |&gt; range(start: 2021-08-17T21:22:52.00Z) |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"Measurement1\" and r[\"tag1\"] == \"tagvalue1\" and r[\"_field\"] == \"field1\" ) . The following Annotated CSV output is returned. Notice the default annotation is set to _results by default. #group,false,false,true,true,false,false,true,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,long,string,string,string #default,_results,,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 ,,0,2021-08-17T21:22:52.452072242Z,2021-08-17T21:23:52.452072242Z,2021-08-17T21:23:39.010094213Z,1,field1,Measurement1,tagvalue1 . Now if we add the yield() function: . from(bucket: \"bucket1\") |&gt; range(start: 2021-08-17T21:22:52.452072242Z) |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"Measurement1\" and r[\"tag1\"] == \"tagvalue1\" and r[\"_field\"] == \"field1\" ) |&gt; yield(name: \"myFluxQuery\") . The following Annotated CSV output is returned. Notice the default annotation has been changed to myFluxQuery. #group,false,false,true,true,false,false,true,true,true #datatype,string,long,dateTime:RFC3339,dateTime:RFC3339,dateTime:RFC3339,long,string,string,string #default,myFluxQuery,,,,,,,, ,result,table,_start,_stop,_time,_value,_field,_measurement,tag1 ,,0,2021-08-17T21:22:52.452072242Z,2021-08-17T21:23:52.452072242Z,2021-08-17T21:23:39.010094213Z,1,field1,Measurement1,tagvalue1 . The yield() function is important because invoking multiple yield() functions allows you to return multiple table streams from a single Flux script simultaneously. Returning multiple aggregations with multiple yield() functions . Imagine that you want to return the min(), max(), and mean() values of a single table: . | _measurement | _field | _value | _time | . | measurement1 | field1 | 1.0 | rfc3339time1 | . | measurement1 | field1 | 2.0 | rfc3339time2 | . | measurement1 | field1 | 4.0 | rfc3339time3 | . | measurement1 | field1 | 5.0 | rfc3339time4 | . We’ll use this meta syntactic example a lot. If you want to try following the solutions out for yourself, include the following Flux at the top of your script to produce the table above: import \"array\" . import \"experimental\" rfc3339time1 = experimental.subDuration(d: -1m, from: now()) rfc3339time2 = experimental.subDuration(d: -2m, from: now()) rfc3339time3 = experimental.subDuration(d: -3m, from: now()) rfc3339time4 = experimental.subDuration(d: -4m, from: now()) data = array.from(rows: [ {_time: rfc3339time1, _value: 1.0, _field: \"field1\", _measurement: \"measurement1\"}, {_time: rfc3339time2, _value: 2.0, _field: \"field1\", _measurement: \"measurement1\"}, {_time: rfc3339time3, _value: 4.0, _field: \"field1\", _measurement: \"measurement1\"}, {_time: rfc3339time4, _value: 5.0, _field: \"field1\", _measurement: \"measurement1\"}]) |&gt; yield(name: \"metasyntaticExample\") . New Flux users, especially those from a SQL or InfluxQL background have the inclination to run the following Flux query: . data |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"Measurement1\" and r[\"tag1\"] == \"tagvalue1\" and r[\"_field\"] == \"field1\" ) |&gt; min() |&gt; max() |&gt; mean() . This is because they’re accustomed to being able to perform SELECT min(\"field1\"), max(\"field1\"), mean(\"field1\"). However, the Flux query above would actually just return the min value. Flux is pipe forwarded, so you must use multiple yield() functions to return the min, max, and mean together: . data |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"Measurement1\" and r[\"tag1\"] == \"tagvalue1\" and r[\"_field\"] == \"field1\" ) |&gt; min() |&gt; yield(name: \"min\") data |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"Measurement1\" and r[\"tag1\"] == \"tagvalue1\" and r[\"_field\"] == \"field1\" ) |&gt; max() |&gt; yield(name: \"max\") data |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"Measurement1\" and r[\"tag1\"] == \"tagvalue1\" and r[\"_field\"] == \"field1\" ) |&gt; mean() |&gt; yield(name: \"mean\") . The above script would result in three tables: . Result: min . | _measurement | _field | _value | _time | . | measurement1 | field1 | 1.0 | rfc3339time1 | . Result: max . | _measurement | _field | _value | _time | . | measurement1 | field1 | 5.0 | rfc339time4 | . Result: mean . | _measurement | _field | _value | . | measurement1 | field1 | 3.0 | . An Aside: Remember that the mean() function doesn’t return a timestamp column because it’s an aggregator. There isn’t a timestamp associated with the mean value. Using variables to perform multiple aggregations . While the Flux query above will yield all three transformations, it’s not an efficient query because you’re querying for the entire dataset multiple times. Instead store the base query in a variable and reference it like so: . data = from(bucket: \"bucket1\") |&gt; range(start: 0) |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"Measurement1\" and r[\"tag1\"] == \"tagvalue1\" and r[\"_field\"] == \"field1\" ) data_min = data |&gt; min() |&gt; yield(name: \"min\") data_max = data |&gt; max() |&gt; yield(name: \"max\") data_mean = data |&gt; mean() |&gt; yield(name: \"mean\") . Important Note: Make sure not to name your variables the same as function names to avoid naming conflicts. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#yielding",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#yielding"
  },"86": {
    "doc": "Querying and Data Transformations",
    "title": "Pivoting",
    "content": "The pivot() function rotates column values into rows in a table. The most common use case for pivoting() data is for when users want to perform math across fields at the same timestamp. The pivot() function has 3 input parameters: . | rowKey: the list of columns that determines the row output | columnKey: the list of columns that determines the column output | valueColumn: the column from which the column values populate the cells of the pivoted table | . Given the following input data: . | Not in Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | _value | _time | . | 0 | measurement1 | field1 | 1.0 | rfc3339time1 | . | 0 | measurement1 | field1 | 2.0 | rfc3339time2 | . | Not in Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | _value | _time | . | 1 | measurement1 | field2 | 3.0 | rfc3339time1 | . | 1 | measurement1 | field2 | 4.0 | rfc3339time2 | . We perform the following pivot: . data |&gt; pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\") . | Not in Group Key | In Group Key | Not in Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | field2 | field1 | _time | . | 0 | measurement1 | 3.0 | 1.0 | rfc3339time1 | . | 0 | measurement1 | 4.0 | 2.0 | rfc3339time2 | . Oftentimes users also want to pivot() on tags to compare a single field across multiple tags. For instance if a user wanted to calculate the difference between the last temperature value across two sensors from the Air Sensor sample dataset, they could uses the following query: . from(bucket: \"Air sensor sample dataset\") |&gt; range(start: 0) |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"airSensors\") |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"co\" |&gt; filter(fn: (r) =&gt; r[\"sensor_id\"] == \"TLM0100\" or r[\"sensor_id\"] == \"TLM0101\") // the limit function is used to return the first two records in each table stream |&gt; limit(n:2) |&gt; yield(name: \"before pivot\") |&gt; pivot(rowKey:[\"_time\"], columnKey: [\"sensor_id\"], valueColumn: \"_value\") |&gt; yield(name: \"after pivot\") . Where the first yield returns the “before pivot” result: . | Not in Group Key | Not in Group Key | In Group Key | Not in Group Key | In Group Key | Not in Group Key | . | table | _measurement | _field | _value | sensor_id | _time | . | 0 | airSensors | co | 0.4901148636678805 | TLM0100 | rfc3339time1 | . | 0 | airSensors | co | 0.4850389571399865 | TLM0100 | rfc3339time2 | . | Not in Group Key | Not in Group Key | In Group Key | Not in Group Key | In Group Key | Not in Group Key | . | table | _measurement | _field | _value | sensor_id | _time | . | 1 | airSensors | co | 0.48242588117742446 | TLM0101 | rfc3339time1 | . | 1 | airSensors | co | 0.47503934770988365 | TLM0101 | rfc3339time2 | . Where the second yield() returns the “after pivot” result: . | Not in Group Key | Not in Group Key | In Group Key | Not in Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | _field | TLM0101 | TLM0100 | _time | . | 0 | airSensors | co | 0.48242588117742446 | 0.4901148636678805 | rfc3339time1 | . | 0 | airSensors | co | 0.47503934770988365 | 0.4850389571399865 | rfc3339time2 | . You can also pivot on multiple columns. This allows you to include values across fields and tags within the same record in a table. Let’s take the previous example but this time we filter for two fields instead of one and pivot on both the sensor_id and field: . from(bucket: \"Air sensor sample dataset\") . |&gt; range(start: 0) |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"airSensors\") |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"co\" or r[\"_field\"] == \"temperature\" |&gt; filter(fn: (r) =&gt; r[\"sensor_id\"] == \"TLM0100\" or r[\"sensor_id\"] == \"TLM0101\") |&gt; yield(name: \"before pivot on two fields and sensors\") |&gt; pivot(rowKey:[\"_time\"], columnKey: [\"sensor_id\",\"_field\"], valueColumn: \"_value\") |&gt; yield(name: \"after pivot before pivot on two fields and sensors\") . Where the first yield returns the “before pivot on two fields and sensors” result: . | Not in Group Key | Not in Group Key | In Group Key | Not in Group Key | In Group Key | Not in Group Key | . | table | _measurement | _field | _value | sensor_id | _time | . | 0 | airSensors | co | 0.4901148636678805 | TLM0100 | rfc3339time1 | . | 0 | airSensors | co | 0.4850389571399865 | TLM0100 | rfc3339time2 | . | Not in Group Key | Not in Group Key | In Group Key | Not in Group Key | In Group Key | Not in Group Key | . | table | _measurement | _field | _value | sensor_id | _time | . | 1 | airSensors | co | 0.48242588117742446 | TLM0101 | rfc3339time1 | . | 1 | airSensors | co | 0.47503934770988365 | TLM0101 | rfc3339time2 | . | Not in Group Key | Not in Group Key | In Group Key | Not in Group Key | In Group Key | Not in Group Key | . | table | _measurement | _field | _value | sensor_id | _time | . | 2 | airSensors | temperature | 71.21039164125095 | TLM0100 | rfc3339time1 | . | 2 | airSensors | temperature | 71.24535411172452 | TLM0100 | rfc3339time2 | . | Not in Group Key | Not in Group Key | In Group Key | Not in Group Key | In Group Key | Not in Group Key | . | table | _measurement | _field | _value | sensor_id | _time | . | 3 | airSensors | temperature | 71.83744572272158 | TLM0101 | rfc3339time1 | . | 3 | airSensors | temperature | 71.85395748942119 | TLM0101 | rfc3339time2 | . Where the second yield() returns the “after pivot before pivot on two fields and sensors” result: . | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | Not in Group Key | . | table | _measurement | TLM0100_co | TLM0101_co | TLM0100_temperature | TLM0101_temperature | _time | . | 0 | airSensors | 0.4901148636678805 | 0.48242588117742446 | 71.21039164125095 | 71.83744572272158 | rfc3339time1 | . | 0 | airSensors | 0.4850389571399865 | 0.47503934770988365 | 71.24535411172452 | 71.85395748942119 | rfc3339time2 | . The fieldsAsCol() function . Pivoting fields on the timestamp column, as described in the first pivoting example, is the most common type of pivoting. Users frequently expect that their data be presented in that way, where the column name contains the field key and the field values are in that column. This application of the pivot() function is so commonly used that the schema.fieldsAsCols() function was created. This function works identically to: . |&gt; pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\") . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#pivoting",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#pivoting"
  },"87": {
    "doc": "Querying and Data Transformations",
    "title": "Mapping",
    "content": "The map() function is an extremely powerful tool. It applies a function to each record in the table. Use the map() function to: . | Perform a transformation on values in a column and replace the original values transformation. | Add new columns to store the transformations or new data. | Conditionally transform records with conditional query logic within the map function. | Change the types of values in a column. | . For this section we’ll use the map() function to transform the following data from the Air Sensor sample dataset: . data = from(bucket: \"Air sensor sample dataset\") |&gt; range(start: 0) |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"airSensors\") |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"co\") |&gt; filter(fn: (r) =&gt; r[\"sensor_id\"] == \"TLM0100\") |&gt; yield(name:\"map\") . | Not in Group Key | Not in Group Key | In Group Key | Not in Group Key | In Group Key | Not in Group Key | . | table | _measurement | _field | _value | sensor_id | _time | . | 0 | airSensors | co | 0.4901148636678805 | TLM0100 | rfc3339time1 | . | 0 | airSensors | co | 0.4850389571399865 | TLM0100 | rfc3339time2 | . In place transformation . The map() function requires a single input parameter: . | fn: The function to apply to each record in the table stream. | . To perform an in-column transformation make sure to reuse a column name in the function. For example, imagine that our TM0100 sensor is faulty and consistently off by 0.02 ppm. We can add 0.02 to every record in the _value column in our data with the map function: . data |&gt; map(fn: (r) =&gt; ({ r with r._value: r._value + 0.02})) . Which yields the following result: . | Not in Group Key | In Group Key | In Group Key | Not in Group Key | In Group Key | Not in Group Key | . | table | _measurement | _field | _value | sensor_id | _time | . | 0 | airSensors | co | 0.5101148636678805 | TLM0100 | rfc3339time1 | . | 0 | airSensors | co | 0.5050389571399865 | TLM0100 | rfc3339time2 | . In other words, the with operator updates a column if that column already exists. New column(s) . You can use the map function to add new columns to your data. For example we could perform the following column to add a new column with the adjustment value and then calculate the true value with the map() function: . data |&gt; map(fn: (r) =&gt; ({ r with adjustment: 0.02 , trueValue: r._value + r.adjustment})) . Which yields the following result: . | Not in Group Key | In Group Key | In Group Key | In Group Key | Not in Group Key | Not in Group Key | In Group Key | Not in Group Key | . | table | _measurement | _field | adjustment | _value | trueValue | sensor_id | _time | . | 0 | airSensors | co | 0.02 | 0.5101148636678805 | 0.5101148636678805 | TLM0100 | rfc3339time1 | . | 0 | airSensors | co | 0.02 | 0.5050389571399865 | 0.5050389571399865 | TLM0100 | rfc3339time2 | . In other words, the with operator creates a new column if one doesn’t already exist. You can also add new columns with the map() function without the with operator. However, when you use the map() function in this way you drop all of the columns that aren’t explicitly mapped. For example, the following query: . data |&gt; map(fn: (r) =&gt; ({adjustment: 0.02, _time:r._time})) . Yields the following result: . | Not in Group Key | In Group Key | Not in Group Key | . | table | adjustment | _time | . | 0 | 0.02 | rfc3339time1 | . | 0 | 0.02 | rfc3339time2 | . Note: You can also use the set() function to create a new column with a string value. Conditionally transform data . Conditionally transforming data with the map() function is an especially useful feature. This combination unlocks another level of sophisticated transformation work. A common use for conditional mapping is to assign conditions or state to numeric values. This is especially common for users who want to create custom checks. Suppose that any co value greater than 0.49 is concerning and and value below that is normal, then we can write the following query to summarize that behaviour in a new tag or column with conditional mapping: data . |&gt; map(fn: (r) =&gt; ({r with level: if r._value &gt;= 0.49 then \"warn\" else \"normal\" }) ) . The query above yields the following result: . | table | _measurement | _field | _value | level | sensor_id | _time | . | 0 | airSensors | co | 0.4901148636678805 | warning | TLM0100 | rfc3339time1 | . | 0 | airSensors | co | 0.4850389571399865 | normal | TLM0100 | rfc3339time2 | . Changing types . Changing data types is useful for a variety of situations including: . | Performing math across fields with different data types with the map() function | To address some of the challenges around grouping data with different datatypes | Preparing data for further transformation work both with Flux and outside of InfluxDB | . If we wanted to change the our data from a float to an integer we would perform the following query: . data |&gt; map(fn: (r) =&gt; ({ r with _value: int(v: r._value)})) . | Not in Group Key | In Group Key | In Group Key | Not in Group Key | In Group Key | Not in Group Key | . | table | _measurement | _field | _value | sensor_id | _time | . | 0 | airSensors | co | 0 | TLM0100 | rfc3339time1 | . | 0 | airSensors | co | 0 | TLM0100 | rfc3339time2 | . Note: you can also use the toFloat(), toInt(), and toString() function to convert values in the _value column to a float, integer, and string respectively. However, the map() function allows you to convert any column you like. You might also want to use the map() function to conditionally convert types when querying for multiple fields. The rows.map() function . The rows.map() function is a simplified version of the map() function. It is much more efficient but also more limited than the map() function. Remember the map() function can modify group keys. However, the rows.map() function cannot. Attempts to modify columns in the group key are ignored. For example, if we tried to change the measurement name with the rows.map() function it would be unsuccessful. However we could adjust the field value like beofre: . data |&gt; rows.map( fn: (r) =&gt; ({r with _measurement: \"in group key so it's ignored\"})) |&gt; rows.map(fn: (r) =&gt; ({ r with r._value: r._value + 0.02})) . Which yields the following result: . | Not in Group Key | In Group Key | In Group Key | Not in Group Key | In Group Key | Not in Group Key | . | table | _measurement | _field | _value | sensor_id | _time | . | 0 | airSensors | co | 0.5101148636678805 | TLM0100 | rfc3339time1 | . | 0 | airSensors | co | 0.5050389571399865 | TLM0100 | rfc3339time2 | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#mapping",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#mapping"
  },"88": {
    "doc": "Querying and Data Transformations",
    "title": "Returning values and arrays",
    "content": "Sometimes users need to be able to query their data, obtain a value or array of values, and then incorporate those values in subsequent transformation work. The fromRecord() and fromColumns() functions allow you to return individual records and columns, respectively. Returning records . The findRecord() function requires two input parameters: . | fn: The predicate function for returning the table with matching keys, provided by the user. | idx: The index of the record you want to extract. | . The easiest way to use the fromRecord() function is to query our data so that you have only one row in your output that contains the scalar value you want to extract. This way you can just set the fn parameter to true idx to 0. data = from(buket : \"bucket1\") |&gt; range(start: 0) |&gt; filter(fn:(r) =&gt; r._measurement == \"measurement1\" and r._field = \"field1\") meanRecord = data |&gt; mean() |&gt; findRecord( fn: (key) =&gt; true, idx: 0) data |&gt; map(fn: (r) =&gt; ({ value_mult_by_mean: r._value * meanRecord._value })) |&gt; yield(name: \"final result\") . Given that the first yield() function returns “data”: . | _measurement | _field | _value | _time | . | measurement1 | field1 | 1.0 | rfc339time1 | . | measurement1 | field1 | 2.0 | rfc339time2 | . | measurement1 | field1 | 4.0 | rfc339time3 | . | measurement1 | field1 | 5.0 | rfc339time4 | . Then meanRecord._value = 4.0. Therefore the second yield() function returns “final result”: . | _measurement | _field | _value | value_mult_by_mean | _time | . | measurement1 | field1 | 1.0 | 4.0 | rfc339time1 | . | measurement1 | field1 | 2.0 | 8.0 | rfc339time2 | . | measurement1 | field1 | 4.0 | 16.0 | rfc339time3 | . | measurement1 | field1 | 5.0 | 20.0 | rfc339time4 | . To illustrate how to use fromRecord() let’s use the Air Sensor sample dataset to calculate the water vapour pressure from one sensor with the mean temperature. The equation for the water vapour pressure is: . water vapour pressure = humidity * ( gas constant * temperature/ molecular weight of water). For this example, we’ll incorporate the following hypothetical assumption: we want to use the mean temperature instead of the actual temperature because our temperature sensors are faulty. Let’s also assume that the temperature and humidity values are in the correct units for simplicity. Therefore, we can calculate the water vapour pressure with the following Flux: . data = from(bucket: \"Air sensor sample dataset\") |&gt; range(start: 0) |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"airSensors\") |&gt; filter(fn: (r) =&gt; r[\"sensor_id\"] == \"TLM0100\") |&gt; limit(n:5) meanRecord = data |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"temperature\") |&gt; yield(name:\"raw temperature\") |&gt; mean() |&gt; findRecord(fn: (key) =&gt; true, idx: 0) data |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"humidity\") |&gt; map(fn: (r) =&gt; ({ r with mean_record: meanRecord._value})) |&gt; map(fn: (r) =&gt; ({ r with water_vapor_pressure: r._value * (8.31 * meanRecord._value / 18.02)})) |&gt; yield(name:\"final result\") . Where the output of the first yield() function returns the “raw temperature”: . | _measurement | _field | _value | sensor_id | _time | . | airSensor | temperature | 71.18548279203421 | TLM0100 | rfc3339time1 | . | airSensor | temperature | 71.22676508109254 | TLM0100 | rfc3339time2 | . | airSensor | temperature | 71.27370100659799 | TLM0100 | rfc3339time3 | . | airSensor | temperature | 71.28825526616907 | TLM0100 | rfc3339time4 | . | airSensor | temperature | 71.25024765248021 | TLM0100 | rfc3339time5 | . And the output of the second yield() function returns the “final result”: . | _measurement | _field | _value | sensor_id | mean_record | water_vapor_pressure | _time | . | airSensor | temperature | 71.18548279203421 | TLM0100 | 71.2448903596748 | 1153.9546087866322 | rfc3339time1 | . | airSensor | temperature | 71.22676508109254 | TLM0100 | 71.2448903596748 | 1153.9546087866322 | rfc3339time2 | . | airSensor | temperature | 71.27370100659799 | TLM0100 | 71.2448903596748 | 1153.9546087866322 | rfc3339time3 | . | airSensor | temperature | 71.28825526616907 | TLM0100 | 71.2448903596748 | 1153.9546087866322 | rfc3339time4 | . | airSensor | temperature | 71.25024765248021 | TLM0100 | 71.2448903596748 | 1153.9546087866322 | rfc3339time5 | . Another common use for the findRecord() function is extracting a timestamp at the time of an event (or when some of your data meets a certain condition) and then using that timestamp to query for other data at the time of the event. For example, we can query for humidity from one sensor in the Air Sensor sample dataset after the first time the temperature exceeded 72.2 degrees. data = from(bucket: \"Air sensor sample dataset\") |&gt; range(start: 0) |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"airSensors\") |&gt; filter(fn: (r) =&gt; r[\"sensor_id\"] == \"TLM0101\") tempTime = data |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"temperature\") |&gt; filter(fn: (r) =&gt; r[\"_value\"] &gt;= 72.2) |&gt; findRecord(fn: (key) =&gt; true, idx: 0) data |&gt; range(start: tempTime._time) |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"humidity\") . This example brings up two other interesting points about the range() and filter() function: . | You can use the range() function multiple times within the same query to further reduce the output of your query. | You can also further limit the response to within a specific time range with the filter() function instead of using range twice. In other words we could have replaced the last three lines with: | . data |&gt; filter(fn: (r) =&gt; r[\"_field\"] &gt;= tempTime._time) |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"humidity\") . Returning columns . You can also return entire arrays that contain the values from a single column with Flux with the findColumn() function. The findColumn() function is similar to the findRecord() function and requires the following two input parameters: . | fn: The predicate function for returning the table with matching keys, provided by the user. | column: The column of the records you want to extract in an array. | . Let’s replace the findRecord() function from the last example in the previous section, Returning records, with findColumn(). data = from(bucket: \"Air sensor sample dataset\") |&gt; range(start: 0) |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"airSensors\") |&gt; filter(fn: (r) =&gt; r[\"sensor_id\"] == \"TLM0101\") tempTime = data |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"temperature\") |&gt; filter(fn: (r) =&gt; r[\"_value\"] &gt;= 72.2) |&gt; findRecord(fn: (key) =&gt; true, column: \"_time\") data |&gt; range(start: tempTime[0]) |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"humidity\") . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#returning-values-and-arrays",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#returning-values-and-arrays"
  },"89": {
    "doc": "Querying and Data Transformations",
    "title": "Reducing",
    "content": "The reduce() function is used to perform custom aggregations. The reduce() function takes two parameters: . | fn: the reducer function, where you define the function that you want to apply to each record in the table with the identity. | identity: where you define the initial values when creating a reducer function. | . For this section we’ll use the following data: . | In Group Key | In Group Key | Not In Group Key | Not In Group Key | . | _measurement | _field | _value | _time | . | measurement1 | field1 | 1.0 | rcc3339time1 | . | measurement1 | field1 | 2.0 | rcc3339time2 | . | measurement1 | field1 | 4.0 | rcc3339time3 | . | measurement1 | field1 | 5.0 | rcc3339time4 | . Here is a simple example of how to uses the reduce() function to calculate the sum of the values: . data = from(bucket: \"bucket1\") |&gt; range(start:0) |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"Measurement1\" and r[\"_field\"] == \"field1\" ) data |&gt; reduce( fn: (r, accumulator) =&gt; ({ sum: r._value + accumulator.sum }), identity: {sum: 0.0} ) |&gt; yield(name: \"sum_reduce\") . The sum identity is initialized at 0.0. The reducer function takes the accumulator.sum and adds it to the field value in each record. The output of the reducer function is given back as the input into the accumulator.sum. The Flux above yields following result: \\ . | In Group Key | In Group Key | Not In Group Key | . | _measurement | _field | sum | . | measurement1 | field1 | 12.0 | . Only columns that are part of the group key are included in the output of the reduce() function. To further understand the reduce() function, let’s calculate the min(), max(), and mean() simultaneously with the reduce() function. data |&gt; reduce( identity: {count: 0.0, sum: 0.0, min: 0.0, max: 0.0, mean: 0.0}, fn: (r, accumulator) =&gt; ({ count: accumulator.count + 1.0, sum: r._value + accumulator.sum, min: if accumulator.count == 0.0 then r._value else if r._value &lt; accumulator.min then r._value else accumulator.min, max: if accumulator.count == 0.0 then r._value else if r._value &gt; accumulator.max then r._value else accumulator.max, mean: (r._value + accumulator.sum) / (accumulator.count + 1.0) }) ) |&gt; yield(name: \"min_max_mean_reduce\") . The Flux above yields following result: . | In Group Key | In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | . | _measurement | _field | count | sum | min | max | mean | . | measurement1 | field1 | 4.0 | 12.0 | 1.0 | 5.0 | 3.0 | . Generally, the reduce() function isn’t more performant than built-in aggregators and selectors. Therefore, you shouldn’t use the query above to calculate the min, max, and mean. Instead, store your data in a variable and apply the min(), max(), and mean() functions separately with corresponding yield() functions to simultaneously deliver the results, as described previously in the Yielding section. The reducer() function is intended to be used to apply custom aggregations. For example, the following example uses the reducer() function to find the necessary variables used to calculate the slope and y-intercept for linear regression: . |&gt; reduce( fn: (r, accumulator) =&gt; ({ sx: r.x + accumulator.sx, sy: r.y + accumulator.sy, N: accumulator.N + 1.0, sxy: r.x * r.y + accumulator.sxy, sxx: r.x * r.x + accumulator.sxx, }), identity: { sxy: 0.0, sx: 0.0, sy: 0.0, sxx: 0.0, N: 0.0, }, ) . Where… . sx is the sum of the index or independent variable. sy is the sum of the dependent variable. `N` is the index. `sxy` is the sum of the multiple of the independent and dependent variables. `sxx` is the sum of the multiple of the independent variables. Important Note: the reduce() function excludes any columns that aren’t in the group key in the output. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#reducing",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#reducing"
  },"90": {
    "doc": "Querying and Data Transformations",
    "title": "Manipulating Time",
    "content": "Manipulating timestamps is critical for any time series analysis tool. Timestamp manipulation in Flux includes: . | Converting timestamp formats | Calculating durations | Truncating or rounding timestamps | Shifting times | Other time manipulations | . Converting timestamp formants . So far timestamps have been represented as the following formats: . | Unix: 1567029600 | RFC3339: 2019-08-28T22:00:00Z | Relative Duration: -1h | Duration: 1h | . The range() function accepts all of those timestamps formats. However, the Annotated CSV output of a Flux query returns the timestamp data in RFC3339 by default. Users need to return the data in another timestamp format to avoid parsing strings for application development on top of InfluxDB. Convert your timestamp from RFC3339 to Unix by using the uint() or int() function. Use the map() function to convert every record in your your _time column to a Unix timestamp. data |&gt; map(fn: (r) =&gt; ({ r with _time: int(v: r._time)})) . Or . data |&gt; map(fn: (r) =&gt; ({ r with _time: uint(v: r._time)}) . Convert your timestamp from Unix to RFC3339 by using the time() function. data |&gt; map(fn: (r) =&gt; ({ r with _time: time(v: r._time)})) . Using the Air Sensor sample dataset we can manipulate the _time column from RFC339 to Unix and back into RFC339 again, storing the results in separate columns: . from(bucket: \"Air sensor sample dataset\") |&gt; range(start:0)` |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"airSensors\") |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"co\") |&gt; filter(fn: (r) =&gt; r[\"sensor_id\"] == \"TLM0100\") |&gt; map(fn: (r) =&gt; ({ r with unix_time: int(v: r._time)})) |&gt; map(fn: (r) =&gt; ({ r with rfc3339_time: time(v: r._time)})) . Important Note: the time() function requires that the unix timestamp must be in nanosecond precision. Calculating durations . Converting time from RFC3339 to Unix is especially useful when you want to find the duration between two points. To calculate the duration between two data points: . | Convert the time to Unix timestamp | Subtract the two Unix timestamps from each other | Use the duration() function to convert the Unix time difference into a duration | . Let’s calculate the duration between the current time and a few points from the Air Sensor sample dataset: . unix_now = uint(v:now()) from(bucket: \"Air sensor sample dataset\") |&gt; range(start:0) |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"airSensors\") |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"co\") |&gt; filter(fn: (r) =&gt; r[\"sensor_id\"] == \"TLM0100\") |&gt; limit(n:5) |&gt; map(fn: (r) =&gt; ({r with duration_from_now: string(duration(unix_now - uint(v: r._time)))})) . Important Note: Flux tables don’t support the duration time format. You must use the string() function to convert the duration to a string. It’s common for users who gather data from IoT devices at the edge to collect data for a while before pushing some of it to InfluxDB Cloud. They frequently want to include both the timestamp that the device recorded a metric and the timestamp when the data was actually written to InfluxDB Cloud. In this instance users should store the timestamp of the metric reading as a field as a string. Then they might want to find the duration between the time the sensor recorded the metric and the time the data was written to InfluxDB. Given the following data: . | In Group Key | In Group Key | Not In Group Key | Not In Group Key | Not In Group key | . | _measurement | _field | _value | _time | _device_time | . | measurement1 | field1 | 1.0 | 2021-09-10T07:15:12.000Z | 1631812512000 | . You can use a combination of int(), uint(), duration(), and string() functions to: . | Convert the _device_time from a string to an integer | Convert unix timestamp into nanosecond precision by multiplying by 10000 | Convert the rfc3339 timestamp of the _time column to a unix timestamp | Calculate the duration and convert it to a string | . data |&gt; map(fn: (r) =&gt; ({ r with _device_time: int(v:r._device_time) * 1000000 })) |&gt; map(fn: (r) =&gt; ({ r with duration: string(v: duration(v:uint(v:r._device_time) - uint(v: r._time)))})) . | In Group Key | In Group Key | Not In Group Key | Not In Group Key | Not In Group key | Not In Group key | . | _measurement | _field | _value | _time | _device_time | duration | . | measurement1 | field1 | 1.0 | 2021-09-10T07:15:12.000Z | 1631812512000000000 | 6d10h | . Truncating or rounding timestamps . Frequently users have data that’s irregular or recorded at different intervals. The most common reason for rounding timestamps is to either: . | Transform an irregular time series into a regular one. An irregular time series is data that isn’t collected at a regular interval. Event data is an example of irregular time series. | Align different time series collected at different intervals so that the user can perform subsequent data transformations on top of the aligned data. | . Given the following input data: . | In Group Key | In Group Key | Not In Group Key | Not In Group Key | . | _measurement | _field | _value | _time | . | measurement1 | field1 | 1.0 | 2021-07-17T12:05:21 | . | measurement1 | field1 | 2.0 | 2021-07-17T12:05:24 | . | measurement1 | field1 | 4.0 | 2021-07-17T12:05:27 | . | measurement1 | field1 | 5.0 | 2021-07-17T12:05:28 | . Use the truncateTimeColumn() function to to convert an irregular time series into a regular one: . data |&gt; truncateTimeColumn(unit: 5s) . | In Group Key | In Group Key | Not In Group Key | Not In Group Key | . | _measurement | _field | _value | _time | . | measurement1 | field1 | 1.0 | 2021-07-17T12:05:20 | . | measurement1 | field1 | 2.0 | 2021-07-17T12:05:20 | . | measurement1 | field1 | 4.0 | 2021-07-17T12:05:25 | . | measurement1 | field1 | 5.0 | 2021-07-17T12:05:25 | . Truncating timestamps is similar to the section on Windowing. The window() function groups data by start and stop times. This allows you to perform aggregations across different fields or tags that have different timestamps. Similarly you can aggregate across fields by truncating timestamps to align series with different intervals. Given the following data: . | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not In Group Key | . | table | _measurement | _field | _value | _time | . | 0 | measurement1 | field1 | 1.0 | 2021-07-17T12:05:50 | . | 0 | measurement1 | field1 | 2.0 | 2021-07-17T12:05:20 | . | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not In Group Key | . | table | _measurement | _field | _value | _time | . | 1 | measurement1 | field2 | 4.0 | 2021-07-17T12:05:27 | . | 1 | measurement1 | field2 | 5.0 | 2021-07-17T12:05:45 | . data |&gt; truncateTimeColumn(unit: 30s) |&gt; group(columns:[\"_time\"]) |&gt; sum() . | Not In Group Key | In Group Key | Not In Group Key | Not In Group Key | . | table | _measurement | _value | _time | . | 0 | measurement1 | 3.0 | 2021-07-17T12:05:00 | . | Not In Group Key | In Group Key | Not In Group Key | Not In Group Key | . | table | _measurement | _value | _time | . | 1 | measurement1 | 9.0 | 2021-07-17T12:05:30 | . Shifting time . Users frequently need to shift their timestamps to convert their data to a different timezone. Given the following data: . | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not In Group Key | . | table | _measurement | _field | _value | _time | . | 0 | measurement1 | field1 | 1.0 | 2021-07-17T08:00:00 | . | 0 | measurement1 | field1 | 2.0 | 2021-07-17T09:00:00 | . Use the timeShift() function to shift the data 2 hours ahead: . data |&gt; timeShift(duration: 2h) . | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not In Group Key | . | table | _measurement | _field | _value | _time | . | 0 | measurement1 | field1 | 1.0 | 2021-07-17T10:00:00 | . | 0 | measurement1 | field1 | 2.0 | 2021-07-17T11:00:00 | . Note: By default the timeShift() function shifts the timestamps in the _start, _stop, and _time columns. Other time manipulations . There are several other timestamp manipulation functions to be aware of in Flux. Although we won’t go into detail about how to use them all, it’s worth being aware of them: . | hourSelection(): select data between specific parts of the day. | duration(): convert a timestamp to a duration in terms of seconds, minutes, hours, etc. | events.Duration(): calculate the duration between events | now(): return the current time | system.time(): return the current time of the system | time(): convert a Unix nanosecond timestamp to an RFC3339 timestamp | uint(): convert RFC3339 timestamp to a Unix nanosecond timestamp | truncateTimeColumn(): round or truncate an entire column to a specific timestamp unit | date.truncate(): round or truncate data down to a specific timestamp unit. | Flux experimental package. This package includes a wide variety of useful functions outside of time series transformations that might be useful to you: . | experimental.addDuration(): add timestamps to each other | experimental.subDuration(): subtract timestamps from each other | experimental.alignTime(): compare data across windows; i.e., week over week or month over month. | . | Flux date package: The Flux date package provides date and time constants and functions. | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#manipulating-time",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#manipulating-time"
  },"91": {
    "doc": "Querying and Data Transformations",
    "title": "Regex",
    "content": "Using regular expressions or regex in Flux is a very powerful tool for filtering for data subsets by matching patterns. Regex is most commonly used in conjunction with functions like the filter(), map(), keep(), or drop() functions. Let’s use the Air Sensor sample dataset, to highlight how to use regex. Remember, we have the following tag and tag keys: . | 1 tag: sensor_id . | 8 sensor_id tag values: . | TML0100 | TML0101 | TML0102 | TML0103 | TML0200 | TML0201 | TML0202 | TML0203 | . | . | . If we wanted to filter for all of sensors with in the 100 range, we could uses the following query: . from(bucket: \"Air sensor sample dataset\") |&gt; range(start:0) |&gt; filter(fn: (r) =&gt; r[\"sensor_id\"] =~ /TML0[1][0][0-3]$/) . Flux uses Go’s regexp package. When constructing a regex it’s a good idea to use a regex tester to make sure that your regex is returning the correct data. You can find a wide selection of regex testers online. I enjoy regex101. To increase the performance of your Flux query it’s a good idea to make your regex as specific as possible. For example, we could use the following query with bad regex instead: . from(bucket: \"Air sensor sample dataset\") |&gt; range(start:0) |&gt; filter(fn: (r) =&gt; r[\"sensor_id\"] =~ /10/) . While it will work and only return data for the TML0100, TML0101, TML0102, and TML0103 sensors, it’s far less specific and efficient than our original regex. You can also use regex to filter for columns like so: . from(bucket: \"Air sensor sample dataset\") |&gt; range(start:0) |&gt; filter(fn: (r) =&gt; r[\"sensor_id\"] == \"TML0100\") |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"co\") |&gt; drop(fn: (column) =&gt; column !~ /^_.*/) . This query drops all columns that don’t start with an underscore. Since our dataset only has one tag, “sensor_id”, that’s the column that will be dropped. The Regexp Package . Flux also has a regexp package. This package has a variety of functions that make it easy to work with regex. You can store regex as strings in InfluxDB and use the regexp.compile() function to compile the strings into regex to filter for those strings. This is especially useful if you’re using a map() function with conditional mapping. Compiling a string into a regex outside of the map() is more efficient than compiling inside of the map(). In the example below we’re evaluating whether or not the URL field values are https or http URLS. url = regexp.compile(v: \"^https\" ) data |&gt; map(fn: (r) =&gt; ({ r with isEncrypted: if r._value =~ url then \"yes\" else \"no\" }) ) . | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not in Group Key | Not In Group Key | . | table | _measurement | _field | _value | isEncrypted | _time | . | 0 | measurement1 | URL | https://foo | yes | rfc3339time1 | . | 0 | measurement1 | URL | http://bar | no | rfc3339time2 | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#regex",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#regex"
  },"92": {
    "doc": "Querying and Data Transformations",
    "title": "The String Package",
    "content": "The Flux string package has a large selection of functions that allow you to manipulate string values. With the Flux string package you can do things like: . | Compare two strings to see if they match | See if one string contains characters in another string or contains a specified substring | Contains uppercase letters, lowercase letters, digits | Replace, split, or join strings | And much more | . For example we could replace the query in The Regexp Package section with: . import \"strings\" data |&gt; map(fn: (r) =&gt; ({ r with isEncrypted: strings.containsStr(v: r._value, substr: \"https\") }) ) . Thereby returning a similar output: . | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not in Group Key | Not In Group Key | . | table | _measurement | _field | _value | isEncrypted | _time | . | 0 | measurement1 | URL | https://foo | true | rfc3339time1 | . | 0 | measurement1 | URL | http://bar | false | rfc3339time2 | . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#the-string-package",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#the-string-package"
  },"93": {
    "doc": "Querying and Data Transformations",
    "title": "Combining Data Streams",
    "content": "A data stream is the output from a singular yield() function. A single table stream contains one or more tables. There are two primary ways that users can combine data streams together: . | Joining allows you to perform an inner join on two data streams. Performing a join expands the width of the data. | Unioning allows you to concatenate two or more streams into a single output stream. Performing a join expands the height of the data. | . Join . Joining merges two input streams into a single output stream based on columns with equal values. There are two Flux functions for joining data: . | join(): The join() function takes the two data streams as input parameters and returns a joined table stream. | experimental.join(): The experimental.join() function is a more performant version of the join() function. | . Joining your data results in a table stream output with an increased width. Math across measurements . The most common reason for joining data is to perform math across measurements. To illustrate how to perform math across measurements, imagine the following scenario: . You are an operator at a chemical plant, and you need to monitor the temperatures of a counter-current heat exchanger. You collect temperatures of the cold (TC) and hot (TH) streams from four different temperature sensors. There are two inlet (Tc2, Th1) sensors and two outlet (Tc1, Th2) sensors at positions x1 and x2 respectively. After making some assumptions, you can calculate the efficiency of heat transfer with this formula: . Where… . | ɳ is the efficiency of the heat transfer | Tc2 is the the temperature of the cold stream at position x2. | Tc1 is the temperature of the cold stream at position x1. | Th1 is the the temperature of the hot stream at position x1. | Th2 is the temperature of the hot stream at position x2. | . You collect temperature reading from each sensor at 2 different times for a total of 8 points with the following schema: . | 1 bucket: sensors | 4 measurements: Tc1, Tc2, Th1, Th2 | 1 Field: temperature | . Since the temperature readings are stored in different measurements, you need to join the data in order to calculate the efficiency. First, I want to gather the temperature readings for each sensor. I start with Th1. I need to prepare the data. I drop the “_start” and “_stop” columns because I’m not performing any group by’s or windowing. Dropping these columns is by no means necessary, it just simplifies the example. I will just be performing math across values on identical timestamps, so I keep the “_time” column. Th1 = from(bucket: \"sensors\") |&gt; range(start: -1d) |&gt; filter(fn: (r) =&gt; r._measurement == \"Th1\" and r._field == \"temperature\") |&gt; yield(name: \"Th1\") . | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not In Group Key | . | table | _measurement | _field | _value | _time | . | 0 | Th1 | temperature | 80.90 | rfc3339time1 | . | 0 | Th1 | temperature | 81.00 | rfc3339time2 | . Th2 = from(bucket: \"sensors\") |&gt; range(start: -1d) |&gt; filter(fn: (r) =&gt; r._measurement == \"Th2\" and r._field == \"temperature\") |&gt; yield(name: \"Th2\") . | Not In Group Key | In Group Key | In Group Key | Not In Group Key | Not In Group Key | . | table | _measurement | _field | _value | _time | . | 0 | Th2 | temperature | 70.2 | rfc3339time1 | . | 0 | Th2 | temperature | 71.6 | rfc3339time2 | . Next, join the two tables. TH = join(tables: {Th1: Th1, Th2: Th2}, on: [\"_time\",\"_field\"]) . | Not In Group Key | In Group Key | In Group Key | In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | . | table | _measurement_Th1 | _measurement_Th2 | _field | _value_Th1 | _value_Th2 | _time | . | 0 | Th1 | Th2 | temperature | 80.90 | 70.2 | rfc3339time1 | . | 0 | Th1 | Th2 | temperature | 81.00 | 71.6 | rfc3339time2 | . The join() function takes a key table pair as input to the tables parameter and column names to the on parameter. The join() function only executes inner joins and joins all columns with equal values. The _time and _field columns have equal values where the _value and _measuremnt columns do not. The table key is appended to the column name to trace like columns with different values back to their input table. Any columns that aren’t included in the on parameter won’t be joined. Next, apply this logic to the cold stream as well: . TC = join(tables: {Tc1: Tc1, Tc2: Tc2}, on: [\"_time\"]) . | Not In Group Key | In Group Key | In Group Key | In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | . | table | _measurement_Tc1 | _measurement_Tc2 | _field | _value_Tc1 | _value_Tc2 | _time | . | 0 | Tc1 | Tc2 | temperature | 50.50 | 60.3 | rfc3339time1 | . | 0 | Tc1 | Tc2 | temperature | 51.00 | 59.3 | rfc3339time2 | . Next, join TC with TH and calculate the efficiency. For the sake of simplicity we’ll drop the measurement columns as well. THTC = join(tables: {TH: TH, TC: TC}, on: [\"_time\"]) |&gt; drop( columns: [\"_measurement_Th1\",\"_measurement_Th2\",\"_measurement_Tc1\",\"_measurement_Tc2\"]) |&gt; yield(name: \"TCTH\") . | Not In Group Key | In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | . | table | _field | _value_Th1 | _value_Th2 | _value_Tc1 | _value_Tc2 | _time | . | 0 | temperature | 80.90 | 70.2 | 50.50 | 60.3 | rfc3339time1 | . | 0 | temperature | 81.00 | 71.6 | 51.00 | 59.3 | rfc3339time2 | . Finally, I can use the map() to calculate the efficiency across all of the measurements. This is what the code looks like all together: . TCTH |&gt; map(fn: (r) =&gt; (r with efficiency: r._value_Tc2 - r._value_Tc1)/(r._value_Th1 - r._value_Th2)*100) |&gt; yield(name: \"efficiency\") . I can see that the heat transfer efficiency has decreased over time. | Not In Group Key | In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | Not In Group Key | . | table | _field | _value_Th1 | _value_Th2 | _value_Tc1 | _value_Tc2 | efficiency | _time | . | 0 | temperature | 80.90 | 70.2 | 50.50 | 60.3 | 92 | rfc3339time1 | . | 0 | temperature | 81.00 | 71.6 | 51.00 | 59.3 | 88 | rfc3339time2 | . Union . The union() function allows you to combine one more table stream which results in a table stream output with an increased table length. Union is frequently used to: . | Merge data across measurements or tags. | Merge transformed data with the original data. | Merge data with different time ranges to make data continuous. | . For example imagine we had the following data: . | Not in Group Key | In Group Key | In Group Key | Not In Group Key | Not In Group Key | . | table | _measurement | _field | _value | _time | . | 0 | measurement1 | field1 | 1.0 | rcc3339time1 | . | 0 | measurement1 | field1 | 2.0 | rcc3339time2 | . | Not in Group Key | In Group Key | In Group Key | Not In Group Key | Not In Group Key | . | table | _measurement | _field | _value | _time | . | 0 | measurement2 | field2 | 4.0 | rcc3339time1 | . | 0 | measurement2 | field2 | 5.0 | rcc3339time2 | . | Not in Group Key | In Group Key | In Group Key | Not In Group Key | Not In Group Key | . | table | _measurement | _field | _value | _time | . | 0 | measurement3 | field3 | 3.0 | rcc3339time1 | . | 0 | measurement3 | field3 | 7.0 | rcc3339time2 | . For example we could uses array.from() to construct that example: import \"experimental\" . import \"array\" rfc3339time1 = experimental.subDuration(d: -1m, from: now()) rfc3339time2 = experimental.subDuration(d: -2m, from: now()) data1 = array.from(rows: [ {_time: rfc3339time1, _value: 1.0, _field: \"field1\", _measurement: \"measurement1\"}, {_time: rfc3339time2, _value: 2.0, _field: \"field1\", _measurement: \"measurement1\"}]) data2 = array.from(rows: [{_time: rfc3339time1, _value: 4.0, _field: \"field2\", _measurement: \"measurement2\"}, {_time: rfc3339time2, _value: 5.0, _field: \"field2\", _measurement: \"measurement2\"}]) data3 = array.from(rows: [{_time: rfc3339time1, _value: 4.0, _field: \"field3\", _measurement: \"measurement3\"}, {_time: rfc3339time2, _value: 5.0, _field: \"field3\", _measurement: \"measurement3\"}]) . Now we might use union() to combine the three table streams together and pivot on the field and measurement: . union(tables: [data1, data2, data3]) |&gt; yield(name:\"after union\") |&gt; pivot(rowKey:[\"_time\"], columnKey: [\"_field\", \"_measurement\"], valueColumn: \"_value\") |&gt; yield(name:\"after pivot\") . Where the first yield() function returns “after union”: . | Not in Group Key | In Group Key | In Group Key | Not In Group Key | Not In Group Key | . | table | _measurement | _field | _value | _time | . | 0 | measurement1 | field1 | 1.0 | rcc3339time1 | . | 0 | measurement1 | field1 | 2.0 | rcc3339time2 | . | 0 | measurement2 | field2 | 4.0 | rcc3339time1 | . | 0 | measurement2 | field2 | 5.0 | rcc3339time2 | . | 0 | measurement3 | field3 | 3.0 | rcc3339time1 | . | 0 | measurement3 | field3 | 7.0 | rcc3339time2 | . The second yield() function returns “after pivot” . | Not in Group Key | In Group Key | In Group Key | Not In Group Key | Not In Group Key | . | table | _field1_measurement1 | _field2_measurement2 | _field3_measurement3 | _time | . | 0 | 1.0 | 4.0 | 3.0 | rcc3339time1 | . | 0 | 2.0 | 5.0 | 7.0 | rcc3339time2 | . Using union() and pivot in this way allows you to achieve a result similar to using a join() function. However, unlike the join() function, the union() function allows you to combine more than two tables together. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#combining-data-streams",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#combining-data-streams"
  },"94": {
    "doc": "Querying and Data Transformations",
    "title": "Accessing External Data Sources",
    "content": "You can use Flux to bring in data from a variety of other sources including SQL databases, other InfluxDB Cloud Accounts, Annotated CSV from a URL, and JSON. The Flux SQL package . You can use the Flux SQL package to query and write to a variety of SQL data source including: . | Amazon RDS | Athena | Google BigQuery | CockroachDB | MariaDB | MySQL | Percona | PostgreSQL | SAP HANA | Snowflake | Microsoft SQL Server | SQLite | . Use the sql.from() function to query a SQL source. For example, to query a local Postgres instance use the following Flux query: import \"sql\" . sql.from( driverName: \"postgres\", dataSourceName: \"postgresql://user:password@localhost\", query:\"SELECT * FROM TestTable\" ) . Use the sql.to() function to write data to SQL database. For, example to write data to a local MySQL instance use the following Flux query: . import \"sql\" data |&gt; sql.to( driverName: \"mysql\", dataSourceName: \"username:password@tcp(localhost:3306)/dbname?param=value\", table: \"example_table\", batchSize: 10000 ) . Kee the following data requirements in mind when using the sql.to() function: . | Data in the steam must have the same column names as your SQL database. Use a combination of drop(), keep(), map(), and rename() to prepare your data before using the sql.to() function. | Remember your SQL schema rules. All data that doesn’t conform to your SQL schema rules will be dropped. Use the map() function to conform data to our SQL schema rules. | . CSV . You can use Flux to import a Raw CSV or Annotated CSV from a URL (or from a local file) with the csv.from() functions. There are two csv.from() functions: . | csv.from() from the Flux experimental CSV package which supports Annotated CSV | csv.from() from stdlib which supports Annotated or Raw CSV | . experimental csv.from() . Use the csv.from() function from the Flux experimental CSV package to retrieve an Annotated CSV from a URL. For example the NOAA water sample data pulls data from an Annotated CSV: . import \"experimental/csv\" csv.from(url: \"https://influx-testdata.s3.amazonaws.com/noaa.csv\") . Note: You can also upload Annotated CSV from a local file with the csv.from() function stdlib with the Flux REPL. You need to build the Flux REPL from source and use it to access your local file system. This version of csv.from() also returns a stream of tables from Annotated CSV stored in a Flux variable. csv.from() . Use the csv.from() function from stdlib to retrieve a Raw CSV from a URL. For example you can use the csv.from() function to parse CSV data from API and write it to InfluxDB in a task. A great example of this can be found in the Earthquake Feed Ingestion task from the Earthquake Command Center Community Template. Here is the relevant Flux from that task: onedayago = strings.trimSuffix(v: string(v: date.truncate(t: experimental.subDuration(d: 1d, from: now()), unit: 1m)), suffix: \".000000000Z\") . csv_data_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&amp;starttime=\" + onedayago + \"&amp;includedeleted=true&amp;orderby=time-asc\" csv_data = string(v: http.get(url: csv_data_url).body) states = [\"Alaska\", \"California\", \"CA\", \"Hawaii\", \"Idaho\", \"Kansas\", \"New Mexico\", \"Nevada\", \"North Carolina\", \"Oklahoma\", \"Oregon\", \"Washington\", \"Utah\"] countries_dictionary = dict.fromList(pairs: [{key: \"MX\", value: \"Mexico\"}]) csv.from(csv: csv_data, mode: \"raw\") . First the user builds their URL. Since this is a task, or a Flux script that’s executed on a schedule, the user wants to build their URL with a dynamic starttime value. They use the experimental.Subduration() function to get the timestamp from -1d. Then they truncate the timestamp with date.truncate() to round the timestamp down to the last minute or \".000000000Z\" . The string() function is used to convert the timestamp into a string and the strings.trimSuffix() function removes the subseconds to format the starttime into the required format as specified by the USGS Earthquake API. Next they use the http.get() function to submit an HTTP GET request to the USGS Earthquake API. Finally they use the csv.from() function to parse the CSV. To learn about how to install a Community Template, please look at the . JSON . Use the json.parse() function from the Flux experimental JSON package to return values from a JSON. Like the example above, you can also use json.parse() with http.get() to parse a HTTP GET JSON response and convert it to a Flux table: . import \"array\" import \"experimental/json\" import \"experimental/http\" resp = http.get(url: \"https://api.openweathermap.org/data/2.5/weather?q=London,uk&amp;APPID=0xx2\") jsonData = json.parse(data: resp.body) array.from(rows: [{_time: now(), _value: float(v:jsonData.main.temp)}]) |&gt; yield() . Which produces the following table: . | Not in Group Key | Not Iin Group Key | Not In Group Key | . | table | _value | _time | . | 0 | 285.33 | rcc3339time1 | . Where the OpenWeatherMap current weather data API yields the following HTTP GET JSON response: . {\"coord\":{\"lon\":-0.1257,\"lat\":51.5085},\"weather\":[{\"id\":801,\"main\":\"Clouds\",\"description\":\"few clouds\",\"icon\":\"02n\"}],\"base\":\"stations\",\"main\":{\"temp\":285.33,\"feels_like\":284.67,\"temp_min\":282.94,\"temp_max\":287.35,\"pressure\":1024,\"humidity\":79},\"visibility\":10000,\"wind\":{\"speed\":2.11,\"deg\":254,\"gust\":4.63},\"clouds\":{\"all\":21},\"dt\":1633546918,\"sys\":{\"type\":2,\"id\":2019646,\"country\":\"GB\",\"sunrise\":1633500560,\"sunset\":1633541256},\"timezone\":3600,\"id\":2643743,\"name\":\"London\",\"cod\":200} . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#accessing-external-data-sources",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#accessing-external-data-sources"
  },"95": {
    "doc": "Querying and Data Transformations",
    "title": "Materialized Views or Downsampling Tasks",
    "content": "Materialized views or downsampling is the process of converting high resolution data to lower resolution aggregates. Downsampling is an important practice in time series database management because it allows users to preserve disk space while retaining low precision trends of their data over long periods of time. Users typically apply an aggregate or selector function to their high resolution data to create a materialized view of a lower resolution summary: . | Flux built-in aggregate transformations like mean(), count(), sum() etc. | Flux built-in selector transformations like max(), min(), median(), etc. | . To downsample the data temperature from the Air Sensor sample dataset, you might perform the following query: from(bucket: \"airsensor\") . |&gt; range(start: -10d) |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"airSensors\") |&gt; aggregateWindow(every:1d, fn: mean, createEmpty: false) |&gt; to(bucket: \"airSensors_materializedView\"0 . Use the to() function to write the data to a destination bucket. Destination buckets usually have a longer retention policy than the source bucket to conserve on disk space. Running this query will write the materialized view to the “airSensors_materializedView” bucket once. However, users typically perform downsampling on a schedule, or a task. Using tasks to create materialized views will be covered in detail in Part 3. Next Section . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-2/querying-and-data-transformations/#materialized-views-or-downsampling-tasks",
    "relUrl": "/docs/part-2/querying-and-data-transformations/#materialized-views-or-downsampling-tasks"
  },"96": {
    "doc": "Setting Up InfluxDB",
    "title": "Introduction to InfluxDB",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/setting-up-influxdb/#introduction-to-influxdb",
    "relUrl": "/docs/part-1/setting-up-influxdb/#introduction-to-influxdb"
  },"97": {
    "doc": "Setting Up InfluxDB",
    "title": "Table of contents",
    "content": ". | Setting up InfluxDB Open Source | An introductory tour of the InfluxDB UI . | User and Account Information . | About | . | Data . | Buckets | Tokens | . | Explorer . | Query Builder | Flux Editor | . | . | Setting Up the CLI . | Find Your Host URL | Finding Your Org ID | Generating Your All-Access Token | Create and Test the CLI Config | . | Setting Up VSCode | . In general, I will assume that you are using the SaaS version of InfluxDB, InfluxDB Cloud. However, most of the content here should be applicable to the OSS standalone version, and when there are important differences between the two, I will point them out. Start by visiting the InfluxDB Cloud sign up page and fill in the sign up form: . The InfluxDB Cloud sign up page. Image 6 . After acknowledging the email verification, choose a Cloud Provider and a region. This is extremely useful if you are already running cloud-based services. InfluxDB Cloud account because InfluxDB Cloud runs in multiple regions across Google, Azure, and AWS clouds. If you don’t have a preference, just look for a region close to your locality. It’s possible to migrate data and the rest of the account later, but the process is not particularly streamlined, so if you do have a preference, keep that in mind. Note that you don’t need your own account with any of these providers to create an InfluxDB Cloud account. Next, choose a plan. A free tier plan is perfect for light workloads and getting started with your application. Converting a free tier plan to a paid plan is easy if you need more resources or if you are going into production later. A free tier plan does not require a credit card–just create a free InfluxDB Cloud account and get started. Now your account is ready to use. You’ll explore some options for uploading sample data or your real data in the next section. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/setting-up-influxdb/#table-of-contents",
    "relUrl": "/docs/part-1/setting-up-influxdb/#table-of-contents"
  },"98": {
    "doc": "Setting Up InfluxDB",
    "title": "Setting up InfluxDB Open Source",
    "content": "Running InfluxDB OSS is not much more involved than creating an InfluxDB Cloud account. Simply download the InfluxDB OSS single binary, run InfluxDB, and then access the InfluxDB UI in your browser. There are a few options for installing InfluxDB on the install page of the docs depending on your OS A few things to note: . | The Linux Arm versions are limited to 64 bit architectures, so they require a later version of Raspberry Pi, for example, if you are seeking to run it there. | InfluxDB is packaged in brew for the Mac, and many Linux distributions have native package manager support. However, some repositories still have an older 1.x version of InfluxDB in their repositories. | . Because InfluxDB is available as one simple binary, you can also download the latest release from the downloads portal. Depending on your installation method, influxdb may already be running. If not, run InfluxDB with with the influxd command: . $ influxd . And then access it at http://localhost:8086. You’ll be prompted to create an initial account to get started, and then the set up experience is pretty much identical to InfluxDB Cloud. The **Getting Started **page in the InfluxDB UI . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/setting-up-influxdb/#setting-up-influxdb-open-source",
    "relUrl": "/docs/part-1/setting-up-influxdb/#setting-up-influxdb-open-source"
  },"99": {
    "doc": "Setting Up InfluxDB",
    "title": "An introductory tour of the InfluxDB UI",
    "content": "Now that you’ve set up your InfluxDB account, take a moment to familiarize yourself with the InfluxDB UI. When you first log into your InfluxDB Account you should see the Getting Started home page. Navigate the User and Account Information icon and the Explorer page from the navigation bar to the right. The Getting Started home page highlights some of the key functionality of InfluxDB including the ability to: . | Load data | Build a dashboard | Set up alerting | . These panels will redirect you to corresponding pages available through the navigation bar to the right. In this section we’ll focus on the most critical areas of the InfluxDB UI for getting started with InfluxDB: . | User and Account Information icon | Explorer | . We’ll dive into the other areas later on. User and Account Information . The User and Account Information icon redirects you to corresponding Usage, Billing, and Organization (from the Users and Accounts options) pages: . About . The About tab in the Organization page provides Common ID’s that you’ll need for Authentication and Authorization within InfluxDB. These ID’s include your User ID and Organization ID (org ID). You’ll need these ID’s to configure the CLI, use a Client, and more. Data . The Data page provides you with multiple solutions for loading your data into InfluxDB including file upload, Client Libraries, and Telegraf. It also contains tabs for all the necessary resource management to successfully write data through those solutions. Specifically, the Data page allows you to: . | Load data from a variety of sources including file upload, client libraries, and Telegraf. | Create and manage your buckets. | Create and manage your Telegraf configurations. | Create and manage your Authentication tokens. Authentication tokens are required for a variety of InfluxDB tools. You’ll need an Authentication token to configure the CLI, use a Client, and more. | . Buckets . The Buckets tab allows you to create and delete buckets. Buckets are a named location to store data within InfluxDB. InfluxDB Data Model will cover buckets in detail. Click the + Create Bucket button to name generate a new bucket. Tokens . The Tokens tab allows you to create and delete tokens. InfluxDB Authentication tokens ensure secure interaction between users and data. You’ll need an Authentication token to configure the CLI, use a Client, and more. Important Note: The only way to create an All-Access token is through the InfluxDB UI. An All-Access token grants full access permissions to all resources in an organization. To create an All-Access token click the + Generate Token dropdown button and select All Access Token from the list. Name the token something meaningful and click Save. Click on the token name in the list to view and copy the token string out of the InfluxDB UI. It’s a good practice to save the token string somewhere safe outside of InfluxDB, like a password manager. Select **Copy to Clipboard **to copy your token. Explorer . The Data Explorer in the InfluxDB UI enables you to build, execute, and visualize your Flux Queries. The Query Builder offers you a no-code solution to building Flux queries, and the Script Editor is a Flux code editor where you can manually edit the query. These two Flux building and editing solutions enable both beginners and advanced Flux users alike. When you navigate to the Data Explorer page you’ll be placed in the Query Builder by default. Query Builder . Build a Flux query with the Query Builder by clicking on the data you want to visualize, selecting a time range, and applying an aggregation to your data. Hit Submit to run the query. Flux Editor . Click the Script Editor button to see the corresponding Flux that you generated with the Query Builder. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/setting-up-influxdb/#an-introductory-tour-of-the-influxdb-ui",
    "relUrl": "/docs/part-1/setting-up-influxdb/#an-introductory-tour-of-the-influxdb-ui"
  },"100": {
    "doc": "Setting Up InfluxDB",
    "title": "Setting Up the CLI",
    "content": "The InfluxDB CLI works equally well for both Cloud and OSS. This section will cover setting up the CLI and setting up a configuration to make the CLI easier to use. If you downloaded and installed InfluxDB OSS, you most likely already have the CLI installed. Otherwise, you can pick it up from the downloads page. Though optional, using the CLI is much easier if you set up a config. Instructions for creating a config are easily available at the command prompt. % influx config create -h The influx config create command creates a new InfluxDB connection configuration and stores it in the configs file (by default, stored at ~/.influxdbv2/configs). Examples: # create a config and set it active influx config create -a -n $CFG_NAME -u $HOST_URL -t $TOKEN -o $ORG_NAME # create a config and without setting it active influx config create -n $CFG_NAME -u $HOST_URL -t $TOKEN -o $ORG_NAME For information about the config command, see https://docs.influxdata.com/influxdb/latest/reference/cli/influx/config/ and https://docs.influxdata.com/influxdb/latest/reference/cli/influx/config/create/ Usage: influx config create [flags] Flags: -a, --active Set as active config -c, --active-config string Config name to use for command; Maps to env var $INFLUX_ACTIVE_CONFIG -n, --config-name string The config name (required) --configs-path string Path to the influx CLI configurations; Maps to env var $INFLUX_CONFIGS_PATH (default \"/Users/rickspencer/.influxdbv2/configs\") -h, --help Help for the create command --hide-headers Hide the table headers; defaults false; Maps to env var $INFLUX_HIDE_HEADERS -u, --host-url string The host url (required) --json Output data as json; defaults false; Maps to env var $INFLUX_OUTPUT_JSON -o, --org string The optional organization name -t, --token string The token for host (required) . So you will need: . | The URL pointing to your Influxdb account or host URL . | The Org ID for your account. | An All-Access token. | . Find Your Host URL . For a cloud account, the base url is easiest to find in the web browser. For the rest of the book we’ll use the following host URL: . https://eastus-1.azure.cloud2.influxdata.com . Additionally if you know the region and cloud provider you selected during account creation, you can lookup your host URL in this complete list of host URLs. If you are using Open Source, it is most likely: . http://localhost:8086 . Finding Your Org ID . You can find your org ID in the UI by navigating to the about page, as described above. For the rest of the book we’ll use the following Org ID: 0261487478164c85. Generating Your All-Access Token . You can generate an All-Access token through the InfluxDB UI, as described above. For the rest of the book we’ll use the following All-Access token: . w4NgOD1k941sMMBlw4L9KxEUsX5gC5Ix5_9u2r-Ac3Z8u6xAlIhHqT5Gu2t2XKsdxU6_tDyc4tOg_qBnpwXRyQ== . Create and Test the CLI Config . Now I you can issue the command to create the config: . % influx config create -u https://eastus-1.azure.cloud2.influxdata.com -o 0261487478164c85 -t w4NgOD1k941sMMBlw4L9KxEUsX5gC5Ix5_9u2r-Ac3Z8u6xAlIhHqT5Gu2t2XKsdxU6_tDyc4tOg_qBnpwXRyQ== --config-name cloud Active Name URL Org cloud https://eastus-1.azure.cloud2.influxdata.com 0261487478164c85 . We can make sure the CLI is working by issuing a simple influx bucket list command: . % influx bucket list ID Name Retention Shard group duration Organization ID 964221964e8fa8ab _monitoring 168h0m0s n/a 0261487478164c85 31ad8b129cd7db7a _tasks 72h0m0s n/a 0261487478164c85= . You should expect a list of your buckets with additional detail about them. Every InfluxDB instance contains two default system buckets: . | _monitoring | _tasks | . We’ll dive into more detail about these buckets later on, but you should always expect two buckets in the output from your influx bucket list command at a minimum. ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/setting-up-influxdb/#setting-up-the-cli",
    "relUrl": "/docs/part-1/setting-up-influxdb/#setting-up-the-cli"
  },"101": {
    "doc": "Setting Up InfluxDB",
    "title": "Setting Up VSCode",
    "content": "The Visual Studio Code Flux Plugin is designed to allow you to integrate InfluxDB as a backend into your existing application development process, and it is relatively easy to set up the InfluxDB VSCode Extension. The Flux extension is easy to find by searching for “Influx” in the extension manager. After installing the extension, you can see an InfluxDB window added to the bottom left. Begin setting up a connection with my Cloud account by giving focus to the InfluxDB window and clicking the + button, and complete the form. After saving, the InfluxDB window should be populated. Part 2 . ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/setting-up-influxdb/#setting-up-vscode",
    "relUrl": "/docs/part-1/setting-up-influxdb/#setting-up-vscode"
  },"102": {
    "doc": "Setting Up InfluxDB",
    "title": "Setting Up InfluxDB",
    "content": " ",
    "url": "http://localhost:4000/time-to-awesome/docs/part-1/setting-up-influxdb/",
    "relUrl": "/docs/part-1/setting-up-influxdb/"
  }
}
